<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="dns-prefetch" href="https://kkewwei.github.io/elasticsearch_learning">
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一个一直向阳的人">
<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://kkewwei.github.io/elasticsearch_learning/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="一个一直向阳的人">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
<meta name="twitter:description" content="一个一直向阳的人">
  
    <link rel="alternative" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/elasticsearch_learning/img/ico.png">
  
  <link rel="stylesheet" type="text/css" href="/elasticsearch_learning/./main.0cf68a.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  

  

</head>
</html>
<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/elasticsearch_learning/" class="profilepic">
			<img src="https://kkewwei.github.io/elasticsearch_learning/img/myself.png" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/elasticsearch_learning/">jianguo</a></h1>
		</hgroup>
		
		<p class="header-subtitle">往事随风</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/elasticsearch_learning/">主页</a></li>
	        
				<li><a href="/elasticsearch_learning/tags/随笔/">随笔</a></li>
	        
				<li><a href="/elasticsearch_learning/categories">分类</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
		        
					<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
		        
					<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
		        
					<a class="mail" target="_blank" href="/elasticsearch_learning/kkewwei@163.com" title="mail"><i class="icon-mail"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="https://kkewwei.github.io/elasticsearch_learning/img/myself.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author">jianguo</h1>
			</hgroup>
			
			<p class="header-subtitle"><i class="icon icon-quo-left"></i>往事随风<i class="icon icon-quo-right"></i></p>
			
			
			
				
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="#" title="github"><i class="icon-github"></i></a>
			        
						<a class="weibo" target="_blank" href="#" title="weibo"><i class="icon-weibo"></i></a>
			        
						<a class="rss" target="_blank" href="#" title="rss"><i class="icon-rss"></i></a>
			        
						<a class="mail" target="_blank" href="/elasticsearch_learning/kkewwei@163.com" title="mail"><i class="icon-mail"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 70%">
				
				
					<li style="width: 33.333333333333336%"><a href="/elasticsearch_learning/">主页</a></li>
		        
					<li style="width: 33.333333333333336%"><a href="/elasticsearch_learning/tags/随笔/">随笔</a></li>
		        
					<li style="width: 33.333333333333336%"><a href="/elasticsearch_learning/categories">分类</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            
  
    <article id="post-HTTP异步Client源码解析" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/elasticsearch_learning/2021/11/05/HTTP异步Client源码解析/">HTTP异步Client源码解析</a>
    </h1>
  

        
        <a href="/elasticsearch_learning/2021/11/05/HTTP异步Client源码解析/" class="archive-article-date">
  	<time datetime="2021-11-05T10:52:51.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2021-11-05</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我们知道Netty作为高性能通信框架，优点在于内部封装了管道的连接通信等操作，用户只需要调用封装好的接口，便可以很便捷的进行高并发通信。类似，在Http请求时，我们通过调用HttpClient，内部使用java NIO技术，通过引入连接池概念，来提高Http的并发能力，本文主要讲解该客户端内部是如何实现并发能力提高的原理。Http客户端分为同步和异步方式，以下示例展示了最基本的异步使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">RequestConfig.Builder requestConfigBuilder = RequestConfig.custom()</span><br><span class="line">        .setConnectTimeout(5000)</span><br><span class="line">        .setSocketTimeout(0)</span><br><span class="line">        .setConnectionRequestTimeout(3000);</span><br><span class="line">HttpAsyncClientBuilder httpClientBuilder = HttpAsyncClientBuilder.create().setDefaultRequestConfig(requestConfigBuilder.build())</span><br><span class="line">// 这些参数会用来生成PoolingNHttpClientConnectionManager，若PoolingNHttpClientConnectionManager自定义了，那么这些参数也就无效了</span><br><span class="line">        .setMaxConnPerRoute(10).setMaxConnTotal(30);</span><br><span class="line">//配置io线程</span><br><span class="line">IOReactorConfig ioReactorConfig = IOReactorConfig.custom().</span><br><span class="line">        setIoThreadCount(Runtime.getRuntime().availableProcessors())</span><br><span class="line">        .setSoKeepAlive(true)</span><br><span class="line">        .build();</span><br><span class="line">DefaultConnectingIOReactor ioReactor = new DefaultConnectingIOReactor(ioReactorConfig);</span><br><span class="line"></span><br><span class="line">ioReactor.setExceptionHandler(new IOReactorExceptionHandler() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public boolean handle(IOException e) &#123;</span><br><span class="line">        System.out.println(&quot;dsdsdsd&quot;);</span><br><span class="line">        return true;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public boolean handle(RuntimeException e) &#123;</span><br><span class="line">        System.out.println(&quot;dsssd&quot;);</span><br><span class="line">        return true;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">// 设置channel连接池并发参数</span><br><span class="line">PoolingNHttpClientConnectionManager poolingNHttpClientConnectionManager = new PoolingNHttpClientConnectionManager(ioReactor);</span><br><span class="line">poolingNHttpClientConnectionManager.setDefaultMaxPerRoute(5);</span><br><span class="line">poolingNHttpClientConnectionManager.setMaxTotal(80);</span><br><span class="line">httpClientBuilder.setConnectionManager(poolingNHttpClientConnectionManager);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 初始化Client并启动</span><br><span class="line">CloseableHttpAsyncClient client = HttpAsyncClients.custom().</span><br><span class="line">        setConnectionManager(poolingNHttpClientConnectionManager)</span><br><span class="line">        .build();</span><br><span class="line">client.start();</span><br><span class="line"></span><br><span class="line">final HttpGet request = new HttpGet(&quot;http://1.1.1.2:9200/indexName/_search&quot;);</span><br><span class="line"></span><br><span class="line">// 异步查询</span><br><span class="line">client.execute(request, new FutureCallback&lt;HttpResponse&gt;() &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void completed(HttpResponse result)&#123;</span><br><span class="line">        try &#123;</span><br><span class="line"></span><br><span class="line">            System.out.println(EntityUtils.toString(result.getEntity()));</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.fillInStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void failed(Exception ex) &#123;</span><br><span class="line">        ex.fillInStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void cancelled() &#123;</span><br><span class="line">        System.out.println(&quot;cancelled&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">Thread.sleep(10000);</span><br><span class="line">client.close();</span><br></pre></td></tr></table></figure>

<p>使用上没啥好说的，我们就直接以数据流流向为主线，看内部是如何使用连接池进行请求处理的。需要注意的是，若我们自定义了poolingNHttpClientConnectionManager对象，那么在requestConfigBuilder中设置的连接并发将不生效。</p>
<h1 id="客户端内部初始化"><a href="#客户端内部初始化" class="headerlink" title="客户端内部初始化"></a>客户端内部初始化</h1><p>pom文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;httpcore&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.4.12&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;httpclient&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.5.10&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;httpcore-nio&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.4.12&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;httpasyncclient&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.1.4&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>目前httpclient已经升级到了5.x，本文源码基于4.X</p>
<h2 id="InternalHttpAsyncClient客户端"><a href="#InternalHttpAsyncClient客户端" class="headerlink" title="InternalHttpAsyncClient客户端"></a>InternalHttpAsyncClient客户端</h2><p>我们需要关注下<code>InternalHttpAsyncClient</code>及其基类<code>CloseableHttpAsyncClientBase</code>，这里把重要的属性都罗列出来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 线程池管理者</span><br><span class="line">private final NHttpClientConnectionManager connmgr; </span><br><span class="line">//MainClientExec 请求发送接收时的处理 </span><br><span class="line">private final InternalClientExec exec;</span><br><span class="line">// 类似netty的boss线程，负责管道建立连接</span><br><span class="line">private final Thread reactorThread;</span><br></pre></td></tr></table></figure>

<p>下图是客户端初始化时创建的一些重要的对象：<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/httpasycnclient1.png" height="400" width="1300"><br>PoolingNHttpClientConnectionManager:根据名称就可以看到，是连接池管理者。<br>CPool:连接池，存放了当前连接池的连接信息，比如全局空闲连接available、每个route独自的Pool，后面会详细介绍。</p>
<h2 id="连接建立线程-请求处理线程"><a href="#连接建立线程-请求处理线程" class="headerlink" title="连接建立线程+请求处理线程"></a>连接建立线程+请求处理线程</h2><p>客户端内部会创建两类线程，类似netty的boss和worker线程，分别用来创建连接管道：AbstractMultiworkerIOReactor、以及请求发送线程:BaseIOReactor。本文中，也复用netty的称呼，分别将这两类线程称呼为boss线程和worker线程。boss线程在CloseableHttpAsyncClientBase构造函数初始化时初始化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">if (threadFactory != null &amp;&amp; handler != null) &#123;</span><br><span class="line">    this.reactorThread = threadFactory.newThread(new Runnable() &#123; </span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        public void run() &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                // 比如当线程接收到数据，就跑到IOEventDispatch里面了</span><br><span class="line">                final IOEventDispatch ioEventDispatch = new InternalIODispatch(handler);</span><br><span class="line">                // 将跑到PoolingNHttpClientConnectionManager.execute()</span><br><span class="line">                connmgr.execute(ioEventDispatch);</span><br><span class="line">            &#125; catch (final Exception ex) &#123;</span><br><span class="line">                log.error(&quot;I/O reactor terminated abnormally&quot;, ex);</span><br><span class="line">            &#125; finally &#123;</span><br><span class="line">                status.set(Status.STOPPED);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;);</span><br><span class="line">&#125; else &#123;</span><br><span class="line">    this.reactorThread = null;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>boss线程真正工作的地方是在AbstractMultiworkerIOReactor，我们需要注意的是selector选择器(会在AbstractMultiworkerIOReactor构造时产生)，每当需要构建管道时，都会向该selector上注册OP_CONNECT事件。AbstractMultiworkerIOReactor初始化代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">public void execute(//eventDispatch=InternalIODispatch</span><br><span class="line">        final IOEventDispatch eventDispatch) throws InterruptedIOException, IOReactorException &#123;</span><br><span class="line">    synchronized (this.statusLock) &#123;</span><br><span class="line">        this.status = IOReactorStatus.ACTIVE; </span><br><span class="line">        // Start I/O dispatchers</span><br><span class="line">        for (int i = 0; i &lt; this.dispatchers.length; i++) &#123;</span><br><span class="line">            final BaseIOReactor dispatcher = new BaseIOReactor(this.selectTimeout, this.interestOpsQueueing);</span><br><span class="line">            dispatcher.setExceptionHandler(exceptionHandler);</span><br><span class="line">            this.dispatchers[i] = dispatcher;</span><br><span class="line">        &#125;</span><br><span class="line">        for (int i = 0; i &lt; this.workerCount; i++) &#123;</span><br><span class="line">            final BaseIOReactor dispatcher = this.dispatchers[i];</span><br><span class="line">            this.workers[i] = new Worker(dispatcher, eventDispatch);</span><br><span class="line">            // 产生的线程名称都是&quot;I/O dispatcher 120&quot;</span><br><span class="line">            this.threads[i] = this.threadFactory.newThread(this.workers[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    try &#123;</span><br><span class="line">        // I/O dispatcher开头的线程名称</span><br><span class="line">        for (int i = 0; i &lt; this.workerCount; i++) &#123;</span><br><span class="line">            if (this.status != IOReactorStatus.ACTIVE) &#123;</span><br><span class="line">                return;</span><br><span class="line">            &#125;</span><br><span class="line">            this.threads[i].start();</span><br><span class="line">        &#125;</span><br><span class="line">        // 无线死循环了，除非管道关闭</span><br><span class="line">        for (;;) &#123; </span><br><span class="line">            final int readyCount;</span><br><span class="line">            try &#123;</span><br><span class="line">                // 默认睡眠1s</span><br><span class="line">                readyCount = this.selector.select(this.selectTimeout);</span><br><span class="line">            &#125; catch (final InterruptedIOException ex) &#123;</span><br><span class="line">                throw ex;</span><br><span class="line">            &#125; catch (final IOException ex) &#123;</span><br><span class="line">                throw new IOReactorException(&quot;Unexpected selector failure&quot;, ex);</span><br><span class="line">            &#125;</span><br><span class="line">            // 如果有需要处理的事件, 则进入processEvents流程, 实际的连接过程就在这里</span><br><span class="line">            if (this.status.compareTo(IOReactorStatus.ACTIVE) == 0) &#123;</span><br><span class="line">                // 纯粹管连接的地方</span><br><span class="line">                processEvents(readyCount);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            // Verify I/O dispatchers</span><br><span class="line">            for (int i = 0; i &lt; this.workerCount; i++) &#123;</span><br><span class="line">                final Worker worker = this.workers[i];</span><br><span class="line">                final Throwable ex = worker.getThrowable();</span><br><span class="line">                if (ex != null) &#123;</span><br><span class="line">                    throw new IOReactorException(</span><br><span class="line">                            &quot;I/O dispatch worker terminated abnormally&quot;, ex);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        doShutdown();</span><br><span class="line">        synchronized (this.statusLock) &#123;</span><br><span class="line">            this.status = IOReactorStatus.SHUT_DOWN;</span><br><span class="line">            this.statusLock.notifyAll();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>具体做了如下事情:<br>1.构建n个worker线程，线程名称是<code>I/O dispatcher n</code>开头的, n可以在<code>IOReactorConfig</code>初始化时设置，默认为cpu的个数。<br>2.启动n个worker线程，每个worker线程真正工作时会跑到<code>BaseIOReactor.execute()</code>中的。<br>3.死循环：<code>select(selectTimeout)</code>,监听管道建立事件发生，并调用<code>processEvents</code>进行管道建立的操作，随机选择一个woker线程，将管道及请求塞入对应的<code>newChannels</code>中，后面会再次介绍。每当有新管道需要创建时，会自动调用selector.wakeup()函数。</p>
<p>我们再看下worker线程内初始化时构建了哪些对象：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public AbstractIOReactor(final long selectTimeout, final boolean interestOpsQueueing) throws IOReactorException &#123;</span><br><span class="line">    super();</span><br><span class="line">    // 每个worker线程默认睡眠selectTimeout，然后从select(selectTimeout)醒来检查</span><br><span class="line">    this.selectTimeout = selectTimeout;</span><br><span class="line">    // 该worker管理的所有IOSessionImpl</span><br><span class="line">    this.sessions = Collections.synchronizedSet(new HashSet&lt;IOSession&gt;());</span><br><span class="line">    // 该worker接受的从boss线程建立好管道，而需要进行数据尕怂的请求体</span><br><span class="line">    this.newChannels = new ConcurrentLinkedQueue&lt;ChannelEntry&gt;();</span><br><span class="line">    try &#123;</span><br><span class="line">        // 每个worker都会拥有一个selector，用来监听读写请求。</span><br><span class="line">        this.selector = Selector.open();</span><br><span class="line">    &#125; catch (final IOException ex) &#123;</span><br><span class="line">        throw new IOReactorException(&quot;Failure opening selector&quot;, ex);</span><br><span class="line">    &#125;</span><br><span class="line">    this.statusMutex = new Object();</span><br><span class="line">    this.status = IOReactorStatus.INACTIVE;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们需要知道的是：<br>1.每个woker线程也拥有一个selector。<br>2.当boss新建管道后，将管道及请求随机放入worker线程newChannels中，后续工作由worker进行。<br>我们再看下worker线程一直在忙哪些操作:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">protected void execute() throws InterruptedIOException, IOReactorException &#123;</span><br><span class="line">    this.status = IOReactorStatus.ACTIVE;</span><br><span class="line">    try &#123;</span><br><span class="line">        for (;;) &#123; // 这里也是无线死循环了</span><br><span class="line">            final int readyCount;</span><br><span class="line">            try &#123;</span><br><span class="line">                readyCount = this.selector.select(this.selectTimeout); // 查询到影响，最多1s</span><br><span class="line">            &#125;</span><br><span class="line">            // Process selected I/O events</span><br><span class="line">            if (readyCount &gt; 0) &#123; // 处理IO 事件</span><br><span class="line">                processEvents(this.selector.selectedKeys());</span><br><span class="line">            &#125;</span><br><span class="line">            // Validate active channels</span><br><span class="line">            //调用AbstractIOReactor.timeoutCheck()检查这个管道对应的请求是否超时。</span><br><span class="line">            //超时了会打印milliseconds timeout on connection http-outgoing-日志</span><br><span class="line">            validate(this.selector.keys()); </span><br><span class="line">            // Process closed sessions</span><br><span class="line">            processClosedSessions();</span><br><span class="line">            // If active process new channels</span><br><span class="line">            if (this.status == IOReactorStatus.ACTIVE) &#123;</span><br><span class="line">                processNewChannels();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        hardShutdown();</span><br><span class="line">        synchronized (this.statusMutex) &#123;</span><br><span class="line">            this.statusMutex.notifyAll();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>worker主线程做了如下事情：<br>1.进行select()等待，最多等待selectTimeout。<br>2.若selector监听到事件产生后，会调用<code>processEvents()</code>进行处理，worker线程只会处理write和read事件，其余事件忽略不处理。<br>3.调用<code>validate</code>检查管道对应的请求是否超时了，超时会打印<code>milliseconds timeout on connection</code>类似的日志，相当于每个http请求增加了执行超时时间。这里的超时通过<code>setSocketTimeout</code>设置，若我们不需要设置http级别的超时时间，将该参数设置为0即可。<br>4.调用<code>processNewChannels</code>检查是否有boss线程传递过来新建立的管道，有的话，就处理，后面会介绍。</p>
<h1 id="http请求发送阶段"><a href="#http请求发送阶段" class="headerlink" title="http请求发送阶段"></a>http请求发送阶段</h1><h2 id="主线程申请请求发送"><a href="#主线程申请请求发送" class="headerlink" title="主线程申请请求发送"></a>主线程申请请求发送</h2><p>我们就直接以<code>InternalHttpAsyncClient.execute</code>代码开始，会首先构建<code>new DefaultClientExchangeHandlerImpl().start()</code>， 我们尤其需要注意<code>DefaultClientExchangeHandlerImpl</code>对象，存放着当前请求内容，当申请到管道后，会存放入管道的<code>http.nio.exchange-handler</code>属性中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public DefaultClientExchangeHandlerImpl(</span><br><span class="line">        final Log log,</span><br><span class="line">        final HttpAsyncRequestProducer requestProducer,</span><br><span class="line">        final HttpAsyncResponseConsumer&lt;T&gt; responseConsumer,//缓存Response的</span><br><span class="line">        final HttpClientContext localContext,</span><br><span class="line">        final BasicFuture&lt;T&gt; resultFuture,</span><br><span class="line">        final NHttpClientConnectionManager connmgr,</span><br><span class="line">        final ConnectionReuseStrategy connReuseStrategy,</span><br><span class="line">        final ConnectionKeepAliveStrategy keepaliveStrategy,</span><br><span class="line">        final InternalClientExec exec) &#123;   </span><br><span class="line">    // 1.基类会针对每次请求，产生一个id </span><br><span class="line">    // 2.我们需要注意localContext，可以存放请求的很多私有属性，比如</span><br><span class="line">    super(log, localContext, connmgr, connReuseStrategy, keepaliveStrategy);</span><br><span class="line">    // 请求产生者</span><br><span class="line">     this.requestProducer = requestProducer;</span><br><span class="line">    // response存储地方</span><br><span class="line">    this.responseConsumer = responseConsumer;</span><br><span class="line">    // 响应用户请求</span><br><span class="line">    this.resultFuture = resultFuture;</span><br><span class="line">    this.exec = exec;</span><br><span class="line">    // 每次查询都包含一个state,</span><br><span class="line">    this.state = new InternalState(getId(), requestProducer, responseConsumer, localContext);// 产生当前的state</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>最终会调用<code>AbstractClientExchangeHandler.requestConnection()</code> -&gt; <code>PoolingNHttpClientConnectionManager.requestConnection()</code> -&gt; <code>AbstractNIOConnPool.lease()</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public Future&lt;E&gt; lease(</span><br><span class="line">        final T route, final Object state,</span><br><span class="line">        final long connectTimeout, final long leaseTimeout, final TimeUnit timeUnit,</span><br><span class="line">        final FutureCallback&lt;E&gt; callback) &#123;</span><br><span class="line">    final BasicFuture&lt;E&gt; future = new BasicFuture&lt;E&gt;(callback);</span><br><span class="line">    final LeaseRequest&lt;T, C, E&gt; leaseRequest = new LeaseRequest&lt;T, C, E&gt;(route, state,</span><br><span class="line">            connectTimeout &gt;= 0 ? timeUnit.toMillis(connectTimeout) : -1,</span><br><span class="line">            leaseTimeout &gt; 0 ? timeUnit.toMillis(leaseTimeout) : 0,// connectionRequestTimeout，</span><br><span class="line">            future);</span><br><span class="line">    // 保证一次只能有一个获取，放在pending中占位并发</span><br><span class="line">    this.lock.lock(); </span><br><span class="line">    try &#123;</span><br><span class="line">        final boolean completed = processPendingRequest(leaseRequest); </span><br><span class="line">        if (!leaseRequest.isDone() &amp;&amp; !completed) &#123;</span><br><span class="line">            this.leasingRequests.add(leaseRequest);</span><br><span class="line">        &#125;</span><br><span class="line">        if (leaseRequest.isDone()) &#123;</span><br><span class="line">            this.completedRequests.add(leaseRequest);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        this.lock.unlock(); </span><br><span class="line">    &#125;</span><br><span class="line">    fireCallbacks();</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要目的是从是连接池中申请连接：<br>1.首先调用<code>this.lock.lock()</code>锁住线程池。这里需要说明下，在实际使用时，若并发相对较高时，发现存在严重的锁阻塞，阻塞耗时1-3s, 在httpclint5.x版本里，已经将线程池级别锁粒度细分到单个route粒度的锁，大大降低了锁互斥的等待时间。<br>2.调用<code>processPendingRequest</code>检查是否有空闲可用管道、可申请连接、还是请求需要pending。</p>
<ul>
<li>若返回为false, 且leaseRequest不为done, 说明连接池满了，将请求放入leasingRequests挂起，等待后续再次申请。</li>
<li>若返回为true, 且leaseRequest为done, 则说明申请到可复用的连接管道，请求则放入completedRequests，等待调用<code>fireCallbacks()</code>时交给worker线程。</li>
<li>若返回为true，且leaseRequest不为done, 则说明该route的连接并发未达上限，请求已经在<code>processPendingRequest</code>内放入了<code>DefaultConnectingIOReactor.requestQueue</code>,等待boss线程去创建新的管道。</li>
</ul>
<p>在继续后面的介绍前，先给大家介绍下线程池内部结构：<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/httpasycnclient2.png" height="450" width="700"></p>
<ul>
<li>leasingRequests: 存放当前route连接并发已经达到上限的请求。</li>
<li>available: 完成请求后，会将当前管道释放到入available，等待后续请求直接复用该管道。</li>
<li>pending: pending中存放的是已经获取权限，需要自己构建SocketChannel的请求。直到构建管道<code>ManagedNHttpClientConnectionImpl</code>（此时已经建立了SocketChannel），才会将请求从pending转移到leased中。</li>
<li>leased: 既包含直接从available获取到可用连接管道的请求，也包含创建ManagedNHttpClientConnectionImpl后，从pending转移过来的请求，直到请求完成后将管道释放到available中。</li>
<li>completedRequests: 直接从连接池中拿到ManagedNHttpClientConnectionImpl，等待放入worker线程池的请求。</li>
<li>service1: 远程服务器，每个service1就是一个ip, 也就是一个route。<br>内部请求转化流程如下：<img src="https://kkewwei.github.io/elasticsearch_learning/img/httpasycnclient3.png" height="350" width="1000">
这里对涉及的管道的包含关系如下：
<img src="https://kkewwei.github.io/elasticsearch_learning/img/httpasycnclient4.png" height="300" width="1200"></li>
</ul>
<p>我们具体看下<code>processPendingRequest</code>是如何从连接池中申请管道的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">private boolean processPendingRequest(final LeaseRequest&lt;T, C, E&gt; request) &#123;</span><br><span class="line">    final T route = request.getRoute();</span><br><span class="line">    final long now = System.currentTimeMillis();</span><br><span class="line">    // 检查获取锁是否已经超时了</span><br><span class="line">    if (now &gt; deadline) &#123; </span><br><span class="line">        request.failed(new TimeoutException(&quot;Connection lease request time out&quot;));</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line">    // 这个route对应的连接Pool</span><br><span class="line">    final RouteSpecificPool&lt;T, C, E&gt; pool = getPool(route);</span><br><span class="line">    E entry;</span><br><span class="line">    for (;;) &#123;</span><br><span class="line">        // 首先从free中获取</span><br><span class="line">        entry = pool.getFree(state); </span><br><span class="line">        // 没有空闲的</span><br><span class="line">        if (entry == null) &#123; </span><br><span class="line">            break;</span><br><span class="line">        &#125; // 从free中获取到了</span><br><span class="line">        if (entry.isClosed() || entry.isExpired(System.currentTimeMillis())) &#123;</span><br><span class="line">            entry.close();</span><br><span class="line">            this.available.remove(entry);</span><br><span class="line">             // 那么直接释放了 </span><br><span class="line">            pool.free(entry, false);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            break;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // 从空闲队列申请到了CPoolEntry</span><br><span class="line">    if (entry != null) &#123; </span><br><span class="line">        this.available.remove(entry);</span><br><span class="line">        // 转移到全局申请出去的列表中</span><br><span class="line">        this.leased.add(entry); </span><br><span class="line">        // 标记完成了</span><br><span class="line">        request.completed(entry); </span><br><span class="line">        // 啥都不做</span><br><span class="line">        onReuse(entry);</span><br><span class="line">        onLease(entry);</span><br><span class="line">        // 直接return了</span><br><span class="line">        return true;</span><br><span class="line">    &#125;</span><br><span class="line">    // 没有空闲可用</span><br><span class="line">    // New connection is needed</span><br><span class="line">    final int maxPerRoute = getMax(route);</span><br><span class="line">    // Shrink the pool prior to allocating a new connection</span><br><span class="line">    final int excess = Math.max(0, pool.getAllocatedCount() + 1 - maxPerRoute);</span><br><span class="line">    // 仅仅是为了检查已经生成的的队列是否超过当前route限制，若超过了，就需要主动关闭了</span><br><span class="line">    if (excess &gt; 0) &#123; </span><br><span class="line">        // 超过了就开始从空闲队列中关闭</span><br><span class="line">        for (int i = 0; i &lt; excess; i++) &#123;</span><br><span class="line">            // 从空闲列表中拿个</span><br><span class="line">            final E lastUsed = pool.getLastUsed();</span><br><span class="line">            if (lastUsed == null) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">            lastUsed.close();</span><br><span class="line">            this.available.remove(lastUsed);</span><br><span class="line">            pool.remove(lastUsed); // 从本管道中关闭</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // 该route若还没超过本routing身线程池</span><br><span class="line">    if (pool.getAllocatedCount() &lt; maxPerRoute) </span><br><span class="line">        // 总池子的使用</span><br><span class="line">        final int totalUsed = this.pending.size() + this.leased.size();</span><br><span class="line">         // 当前申请的是否已经超过了总连接池个数</span><br><span class="line">        final int freeCapacity = Math.max(this.maxTotal - totalUsed, 0);</span><br><span class="line">        // 满了</span><br><span class="line">        if (freeCapacity == 0) &#123; </span><br><span class="line">            return false;</span><br><span class="line">        &#125;</span><br><span class="line">        // 此时还没超过</span><br><span class="line">        // 查看全局空闲是否超过限制了</span><br><span class="line">        final int totalAvailable = this.available.size();</span><br><span class="line">        // 当前空闲的+1是否超过了全局可用剩余个数 </span><br><span class="line">        if (totalAvailable &gt; freeCapacity - 1) &#123;</span><br><span class="line">             // 若超过了，那么就关闭一个 </span><br><span class="line">            if (!this.available.isEmpty()) &#123;</span><br><span class="line">                final E lastUsed = this.available.removeLast();// CPoolEntry</span><br><span class="line">                lastUsed.close();</span><br><span class="line">                final RouteSpecificPool&lt;T, C, E&gt; otherpool = getPool(lastUsed.getRoute());</span><br><span class="line">                otherpool.remove(lastUsed);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        final SocketAddress localAddress;</span><br><span class="line">        final SocketAddress remoteAddress;</span><br><span class="line">        try &#123;</span><br><span class="line">            //会把域名映射出来，比如:qa1.l1c.data.hehe.com映射5个ips, admin.daxe1.l1c.data.hehe.com映射一个vip，但是只取第一个</span><br><span class="line">            remoteAddress = this.addressResolver.resolveRemoteAddress(route);</span><br><span class="line">            localAddress = this.addressResolver.resolveLocalAddress(route);</span><br><span class="line">        &#125; catch (final IOException ex) &#123;</span><br><span class="line">            request.failed(ex);</span><br><span class="line">            return false;</span><br><span class="line">        &#125;</span><br><span class="line">        // 将请求放入了请求Queue中，并唤醒了主selector。</span><br><span class="line">        final SessionRequest sessionRequest = this.ioReactor.connect(</span><br><span class="line">                remoteAddress, localAddress, route, this.sessionRequestCallback);</span><br><span class="line">        request.attachSessionRequest(sessionRequest);</span><br><span class="line">        final long connectTimeout = request.getConnectTimeout();</span><br><span class="line">        if (connectTimeout &gt;= 0) &#123;</span><br><span class="line">            sessionRequest.setConnectTimeout(connectTimeout &lt; Integer.MAX_VALUE ? (int) connectTimeout : Integer.MAX_VALUE);</span><br><span class="line">        &#125;</span><br><span class="line">        // 加入到route连接池pending集合</span><br><span class="line">        this.pending.add(sessionRequest);</span><br><span class="line">        // 已经获得了连接权，但是还没有建立连接的请求</span><br><span class="line">        pool.addPending(sessionRequest, request.getFuture());</span><br><span class="line">        return true;</span><br><span class="line">    &#125;</span><br><span class="line">    return false;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>向连接池申请连接主要做了如下事情：<br>1.检查获取lock是否超时，超时参数通过<code>setConnectionRequestTimeout(3000)</code>参数设置。<br>2.获取该route对应的连接池：RouteSpecificPool, 检测是否有空闲可用的连接，有的话就返回CPoolEntry。<br>3.检查当前route连接是否超过上限，有的话，就从availabe中取出，并关闭管道。<br>4.若连接还没达到上限，那么就调用<code>ioReactor.connect()</code>将请求放入DefaultConnectingIOReactor.requestQueue中，并唤醒主线程，等待boss线程去创建新的管道。</p>
<h2 id="boss线程创建新的连接管道"><a href="#boss线程创建新的连接管道" class="headerlink" title="boss线程创建新的连接管道"></a>boss线程创建新的连接管道</h2><p>前面也提到了，boss线程会在selector.select()中唤醒。唤醒后，会进入<code>DefaultConnectingIOReactor.processEvents</code>判断是否有需要建立连接的请求。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">protected void processEvents(final int readyCount) throws IOReactorException &#123;</span><br><span class="line">    // 创建新的管道</span><br><span class="line">    processSessionRequests();</span><br><span class="line">    if (readyCount &gt; 0) &#123;</span><br><span class="line">        final Set&lt;SelectionKey&gt; selectedKeys = this.selector.selectedKeys();</span><br><span class="line">        for (final SelectionKey key : selectedKeys) &#123; </span><br><span class="line">            // 发现有连接事件发生了</span><br><span class="line">            processEvent(key);</span><br><span class="line">        &#125;</span><br><span class="line">        selectedKeys.clear();</span><br><span class="line">    &#125;</span><br><span class="line">    // 判断select是否超时(默认1s)</span><br><span class="line">    final long currentTime = System.currentTimeMillis();</span><br><span class="line">    if ((currentTime - this.lastTimeoutCheck) &gt;= this.selectTimeout) &#123;</span><br><span class="line">        this.lastTimeoutCheck = currentTime;</span><br><span class="line">        final Set&lt;SelectionKey&gt; keys = this.selector.keys();</span><br><span class="line">        processTimeouts(keys);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>boss每次循环主要做了如下事情：<br>1.调用<code>processSessionRequests</code>创建新的管道。<br>2.调用<code>processEvent()</code>处理发生的连接事件。<br>注意<code>DefaultConnectingIOReactor.processSessionRequests</code>只负责调用接口创建管道，而不用等待管道是否创建ok；而<code>processEvent</code>是专门用来监听管道是否建立成功的。我们继续看下创建管道做了哪些事情：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">private void processSessionRequests() throws IOReactorException &#123;</span><br><span class="line">    SessionRequestImpl request;</span><br><span class="line">    // 有权产生新的管道，但是还没有管道可用</span><br><span class="line">    while ((request = this.requestQueue.poll()) != null) &#123;</span><br><span class="line">        // 检查是否完成了 </span><br><span class="line">        if (request.isCompleted()) &#123; </span><br><span class="line">            continue;</span><br><span class="line">        &#125;</span><br><span class="line">        final SocketChannel socketChannel; // SocketChannelImpl</span><br><span class="line">        try &#123; // 建立一个socket</span><br><span class="line">            socketChannel = SocketChannel.open();</span><br><span class="line">        &#125; catch (final IOException ex) &#123;</span><br><span class="line">            request.failed(ex);</span><br><span class="line">            return;</span><br><span class="line">        &#125;</span><br><span class="line">        try &#123;</span><br><span class="line">            validateAddress(request.getLocalAddress());</span><br><span class="line">            validateAddress(request.getRemoteAddress());</span><br><span class="line">             // 设置非阻塞</span><br><span class="line">            socketChannel.configureBlocking(false);</span><br><span class="line">             // 设置SocketAdaptor一些参数，比如是否复用，连接超时，写内核buffer</span><br><span class="line">            prepareSocket(socketChannel.socket());</span><br><span class="line"></span><br><span class="line">            if (request.getLocalAddress() != null) &#123; // 为null</span><br><span class="line">                final Socket sock = socketChannel.socket();</span><br><span class="line">                sock.setReuseAddress(this.config.isSoReuseAddress());</span><br><span class="line">                sock.bind(request.getLocalAddress());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            final SocketAddress targetAddress = request.getRemoteAddress();</span><br><span class="line">            // Run this under a doPrivileged to support lib users that run under a SecurityManager this allows granting connect</span><br><span class="line">            // permissions only to this library</span><br><span class="line">            // 是否已经连接上</span><br><span class="line">            final boolean connected; </span><br><span class="line">            try &#123;</span><br><span class="line">                connected = AccessController.doPrivileged(</span><br><span class="line">                        new PrivilegedExceptionAction&lt;Boolean&gt;() &#123;</span><br><span class="line">                            @Override</span><br><span class="line">                            public Boolean run() throws IOException &#123;</span><br><span class="line">                                // 连接远程目标节点</span><br><span class="line">                                return socketChannel.connect(targetAddress);</span><br><span class="line">                            &#125;;</span><br><span class="line">                        &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">             // 如果已经建立连接</span><br><span class="line">            if (connected) &#123;</span><br><span class="line">                final ChannelEntry entry = new ChannelEntry(socketChannel, request);</span><br><span class="line">                ;// 直接就分配对对应的work了，就没boss线程啥事了</span><br><span class="line">                addChannel(entry)</span><br><span class="line">                continue;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; </span><br><span class="line">        // 还未连接成功, 则注册到selector, 等待connect事件的触发, 再用processEvent来处理</span><br><span class="line">        final SessionRequestHandle requestHandle = new SessionRequestHandle(request);</span><br><span class="line">        try &#123;</span><br><span class="line">             // 向这个管道注册connect事件</span><br><span class="line">            final SelectionKey key = socketChannel.register(this.selector, SelectionKey.OP_CONNECT, requestHandle);</span><br><span class="line">            request.setKey(key);</span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>创建管道过程也相对比较清晰：<br>1.循环从DefaultConnectingIOReactor.requestQueue拿需要创建管道的请求。（前面提了，主线程会将创建管道的请求放入该queue中）<br>2.创建SocketChannelImpl后，调用bind绑定：</p>
<ul>
<li>若同步绑定成功后，将产生的ChannelEntry(socketChannel, request)顺序分配给一个worker线程（该worker的<code>newChannels</code>中）</li>
<li>若还未绑定成功，则向boss的selector添加SelectionKey.OP_CONNECT事件，等待管道连接的事件发送。（只有ServerSocketChannel才会注册SelectionKey.OP_ACCEPT事件， SocketChannel只能注册SelectionKey.OP_CONNECT事件）</li>
</ul>
<p>我们看下<code>processEvent</code>如何处理OP_CONNECT事件的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">private void processEvent(final SelectionKey key) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        // 该key是否是connect属性</span><br><span class="line">        if (key.isConnectable()) &#123;</span><br><span class="line">            final SocketChannel channel = (SocketChannel) key.channel();</span><br><span class="line">            // Get request handle</span><br><span class="line">            final SessionRequestHandle requestHandle = (SessionRequestHandle) key.attachment();</span><br><span class="line">            final SessionRequestImpl sessionRequest = requestHandle.getSessionRequest();</span><br><span class="line"></span><br><span class="line">            // Finish connection process</span><br><span class="line">            try &#123;</span><br><span class="line">               // 非阻塞模式下，确认是否连接好，若未连接好，直接返回false,方法必不可少(置位管道状态)</span><br><span class="line">                channel.finishConnect();</span><br><span class="line">            &#125; catch (final IOException ex) &#123;</span><br><span class="line">                sessionRequest.failed(ex);</span><br><span class="line">            &#125;</span><br><span class="line">            key.cancel();</span><br><span class="line">            key.attach(null);</span><br><span class="line">            if (!sessionRequest.isCompleted()) &#123;</span><br><span class="line">                addChannel(new ChannelEntry(channel, sessionRequest));</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                try &#123;</span><br><span class="line">                    channel.close();</span><br><span class="line">                &#125; catch (final IOException ignore) &#123;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>一看就比较清晰了吧，boss线程只接受连接事件，非连接事件一律丢弃。检查到连接创建完成后，构建new ChannelEntry(channel, sessionRequest)顺序分配给一个worker线程（该worker的<code>newChannels</code>中）</p>
<h2 id="worker线程发送请求"><a href="#worker线程发送请求" class="headerlink" title="worker线程发送请求"></a>worker线程发送请求</h2><p>接下来就看worker线程接到请求后如何处理了。前面worker也提到了，worker死循环会做如下三件事情(参考<code>AbstractIOReactor.execute</code>函数);<br>1.调用<code>processEvents</code>检查新的write、read事件。<br>2.调用<code>validate</code>判断是否有查询超时，超时参数通过setSocketTimeout参数设置<br>3.调用<code>processNewChannels</code>处理boss线程传递的新创建的管道及请求。<br>我们先看下<code>AbstractIOReactor.processNewChannels()</code>如何处理新创建的管道及请求的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">private void processNewChannels() throws IOReactorException &#123;</span><br><span class="line">    ChannelEntry entry;</span><br><span class="line">    // 轮循每个新产生的请求及对应的管道</span><br><span class="line">    while ((entry = this.newChannels.poll()) != null) &#123;</span><br><span class="line"></span><br><span class="line">        final SocketChannel channel;</span><br><span class="line">        final SelectionKey key;</span><br><span class="line">        try &#123;</span><br><span class="line">            channel = entry.getChannel();</span><br><span class="line">            channel.configureBlocking(false);</span><br><span class="line">            // SelectionKeyImpl，都注册read事件</span><br><span class="line">            key = channel.register(this.selector, SelectionKey.OP_READ); </span><br><span class="line">        &#125;</span><br><span class="line">        final SessionClosedCallback sessionClosedCallback = new SessionClosedCallback() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public void sessionClosed(final IOSession session) &#123;</span><br><span class="line">                queueClosedSession(session);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        InterestOpsCallback interestOpsCallback = null;</span><br><span class="line">        final IOSession session;</span><br><span class="line">        try &#123;</span><br><span class="line">            // IOSessionImpl与key是绑定的，因为key是重复利用的，所以IOSessionImpl也是重复利用的</span><br><span class="line">            session = new IOSessionImpl(key, interestOpsCallback, sessionClosedCallback);</span><br><span class="line">            int timeout = 0;</span><br><span class="line">            try &#123;</span><br><span class="line">                timeout = channel.socket().getSoTimeout();</span><br><span class="line">            &#125; catch (final IOException ex) &#123;</span><br><span class="line">                // Very unlikely to happen and is not fatal</span><br><span class="line">                // as the protocol layer is expected to overwrite</span><br><span class="line">                // this value anyways</span><br><span class="line">            &#125;</span><br><span class="line">            // 设置http.session.attachment</span><br><span class="line">            session.setAttribute(IOSession.ATTACHMENT_KEY, entry.getAttachment()); </span><br><span class="line">            // 设置超时</span><br><span class="line">            session.setSocketTimeout(timeout);</span><br><span class="line">        &#125; </span><br><span class="line">        try &#123;</span><br><span class="line">            // 一个新的上下文请求</span><br><span class="line">            this.sessions.add(session);</span><br><span class="line">            // 将这个IOSessionImpl放入SelectionKeyImpl中 </span><br><span class="line">            key.attach(session);</span><br><span class="line">            final SessionRequestImpl sessionRequest = entry.getSessionRequest();</span><br><span class="line">            if (sessionRequest != null) &#123;</span><br><span class="line">                if (!sessionRequest.isTerminated()) &#123;</span><br><span class="line">                    //1.产生了connection，2.往AbstractNIOConnPool.leased放入CPoolEntry.3.设置可写事件</span><br><span class="line">                    sessionRequest.completed(session);</span><br><span class="line">                &#125;</span><br><span class="line">                if (!sessionRequest.isTerminated()) &#123;</span><br><span class="line">                    // 进来设置write事件了</span><br><span class="line">                    sessionCreated(key, session);</span><br><span class="line">                &#125;</span><br><span class="line">                if (sessionRequest.isTerminated()) &#123;</span><br><span class="line">                    throw new CancelledKeyException();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                sessionCreated(key, session);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情：<br>1.对该管道添加<code>SelectionKey.OP_READ</code>事件<br>2.创建IOSessionImpl对象，需要注意，该对象生命周期与SocketChannelImpl绑定的。<br>3.调用<code>sessionRequest.completed</code>:</p>
<ul>
<li>产生<code>ManagedNHttpClientConnectionImpl</code>管道。</li>
<li>构建CPoolEntry。</li>
<li>向该管道对应的<code>IOSessionImpl.attributes</code>增加<code>http.nio.exchange-handler</code>，将请求内容<code>DefaultClientExchangeHandlerImpl</code>与该管道绑定。</li>
<li>并将该管道增加<code>SelectionKey.OP_WRITE</code>感兴趣的事件<blockquote>
<p>注意：</p>
</blockquote>
</li>
</ul>
<p>1.调用<code>AbstractClientExchangeHandler.connectionAllocated</code>表示ManagedNHttpClientConnectionImpl管道已经就绪，就等待worker发送请求，该函数将在两个地方调用：1.主函数从空闲列表中申请到可用管道。2.worker线程接到boss线程创建的SocketChannleImpl后创建了ManagedNHttpClientConnectionImpl管道。<br>2.每个管道在数据发送前，会通过<code>http.nio.exchange-handler</code>属性，与请求绑定。每个管道就是一个连接并发，每次只能发送一次请求，只有当上一个请求结束后，该管道才会分配给下个请求。<br>3.需要说下，为啥我们不可以直接发送请求、而再来注册SelectionKey.OP_WRITE事件呢？注册后, 系统会去检查内核写缓冲区是否写满了, 若写满了，会发送失败的情况。<br>4.此时管道已经注册了<code>SelectionKey.OP_READ</code>和<code>SelectionKey.OP_WRITE</code>事件。</p>
<p>我们再看下<code>AbstractIOReactor.processEvents</code>如何处理事件的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">private void processEvents(final Set&lt;SelectionKey&gt; selectedKeys) &#123;</span><br><span class="line">    for (final SelectionKey key : selectedKeys) &#123;</span><br><span class="line">        processEvent(key);</span><br><span class="line">    &#125;</span><br><span class="line">    selectedKeys.clear();</span><br><span class="line">&#125;</span><br><span class="line">protected void processEvent(final SelectionKey key) &#123;</span><br><span class="line">    // 直接通过IOsessionImpl获取元数据，复用时，</span><br><span class="line">    final IOSessionImpl session = (IOSessionImpl) key.attachment();</span><br><span class="line">    try &#123;</span><br><span class="line">        if (key.isAcceptable()) &#123;// accept事件</span><br><span class="line">            acceptable(key); // 啥都不干</span><br><span class="line">        &#125;</span><br><span class="line">        if (key.isConnectable()) &#123; // connect事件</span><br><span class="line">            connectable(key); // 啥都不干</span><br><span class="line">        &#125;</span><br><span class="line">        if (key.isReadable()) &#123; // 读事件</span><br><span class="line">            session.resetLastRead();</span><br><span class="line">            readable(key);</span><br><span class="line">        &#125;</span><br><span class="line">        if (key.isWritable()) &#123;// 里面注册了可写事件</span><br><span class="line">            session.resetLastWrite();</span><br><span class="line">            writable(key); // 真正写数据</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; catch (final CancelledKeyException ex) &#123;</span><br><span class="line">        queueClosedSession(session);</span><br><span class="line">        key.attach(null);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里主要关注的是<code>write</code>、<code>read</code>事件，针对<code>accept</code>、<code>connect</code>直接丢弃，<code>read</code>响应在下一章详细介绍。我们继续看下监听到<code>write</code>后发生了什么事情，发送数据时会跑到<code>DefaultNHttpClientConnection.produceOutput</code>这里：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">public void produceOutput(final NHttpClientEventHandler handler) &#123;// HttpAsyncRequestExecutor</span><br><span class="line">    try &#123;</span><br><span class="line">        if (this.status == ACTIVE) &#123;</span><br><span class="line">            if (this.contentEncoder == null &amp;&amp; !this.outbuf.hasData()) &#123;</span><br><span class="line">                handler.requestReady(this);</span><br><span class="line">            &#125;</span><br><span class="line">            // 编码请求，默认使用LengthDelimitedEncoder进行编码</span><br><span class="line">            if (this.contentEncoder != null) &#123;</span><br><span class="line">                handler.outputReady(this, this.contentEncoder);</span><br><span class="line">                if (this.contentEncoder.isCompleted()) &#123;</span><br><span class="line">                    resetOutput();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        if (this.outbuf.hasData()) &#123;</span><br><span class="line">            // 真正向管道中刷数据了</span><br><span class="line">            final int bytesWritten = this.outbuf.flush(this.session.channel());</span><br><span class="line">            if (bytesWritten &gt; 0) &#123;</span><br><span class="line">                this.outTransportMetrics.incrementBytesTransferred(bytesWritten);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        if (!this.outbuf.hasData()) &#123;// 若没有数据了</span><br><span class="line">            if (this.status == CLOSING) &#123;</span><br><span class="line">                this.session.close();</span><br><span class="line">                this.status = CLOSED;</span><br><span class="line">                resetOutput();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        // Finally set the buffered output flag</span><br><span class="line">        this.hasBufferedOutput = this.outbuf.hasData();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>主要做了两件事：<br>1.针对body使用<code>LengthDelimitedEncoder</code>进行编码。<br>2.调用<code>this.outbuf.flush()</code>将编码内容从SocketChannel真正发送出去。</p>
<h2 id="http响应阶段"><a href="#http响应阶段" class="headerlink" title="http响应阶段"></a>http响应阶段</h2><p>http响应阶段在<code>AbstractIOReactor.processEvents</code>的<code>key.isReadable()</code>处接受响应，会进入到<code>BaseIOReactor.readable()</code>中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">protected void readable(final SelectionKey key) &#123;</span><br><span class="line">    //获取这个key绑定的IOSessionImpl（在key与管道，IOSessionImpl都是绑定一起的）</span><br><span class="line">    final IOSession session = getSession(key); </span><br><span class="line">    try &#123;</span><br><span class="line">        // Try to gently feed more data to the event dispatcher</span><br><span class="line">        // if the session input buffer has not been fully exhausted</span><br><span class="line">        // (the choice of 5 iterations is purely arbitrary)</span><br><span class="line">        for (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">            // 实现类是InternalIODispatch</span><br><span class="line">            this.eventDispatch.inputReady(session);</span><br><span class="line">            if (!session.hasBufferedInput()</span><br><span class="line">                    || (session.getEventMask() &amp; SelectionKey.OP_READ) == 0) &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        if (session.hasBufferedInput()) &#123;</span><br><span class="line">            this.bufferingSessions.add(session);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从管道读取时，会循环5次（一般调用一次<code>InternalIODispatch.inputReady</code>就读取完数据了），直到读取完数据。读取数据会进入到<code>DefaultNHttpClientConnection.consumeInput</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">public void consumeInput(final NHttpClientEventHandler handler) &#123;// HttpAsyncRequestExecutor</span><br><span class="line">    try &#123;</span><br><span class="line">        if (this.response == null) &#123;</span><br><span class="line">            int bytesRead;</span><br><span class="line">            // 循环读取，直到读取完成</span><br><span class="line">            do &#123;</span><br><span class="line">                // 首先读8k</span><br><span class="line">                bytesRead = this.responseParser.fillBuffer(this.session.channel());</span><br><span class="line">                if (bytesRead &gt; 0) &#123;</span><br><span class="line">                    this.inTransportMetrics.incrementBytesTransferred(bytesRead);</span><br><span class="line">                &#125;</span><br><span class="line">                //BasicHttpResponse，解析了如何读取http的字节流</span><br><span class="line">                this.response = this.responseParser.parse();</span><br><span class="line">            &#125; while (bytesRead &gt; 0 &amp;&amp; this.response == null);</span><br><span class="line">            if (this.response != null) &#123;</span><br><span class="line">                if (this.response.getStatusLine().getStatusCode() &gt;= 200) &#123;</span><br><span class="line">                    // 这里才会产生一个createContentDecoder</span><br><span class="line">                    final HttpEntity entity = prepareDecoder(this.response);</span><br><span class="line">                    this.response.setEntity(entity);</span><br><span class="line">                    this.connMetrics.incrementResponseCount();</span><br><span class="line">                &#125;</span><br><span class="line">                this.hasBufferedInput = this.inbuf.hasData();</span><br><span class="line">                onResponseReceived(this.response);// 没用</span><br><span class="line">                handler.responseReceived(this);//从管道中读取完数据后，handler=HttpAsyncRequestExecutor</span><br><span class="line">                if (this.contentDecoder == null) &#123;</span><br><span class="line">                    resetInput();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            if (bytesRead == -1 &amp;&amp; !this.inbuf.hasData()) &#123;</span><br><span class="line">                handler.endOfInput(this);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        if (this.contentDecoder != null &amp;&amp; (this.session.getEventMask() &amp; SelectionKey.OP_READ) &gt; 0) &#123;</span><br><span class="line">            // 1.读取body,2.会存在释放管道的行为.3.响应用户</span><br><span class="line">            handler.inputReady(this, this.contentDecoder);</span><br><span class="line">            if (this.contentDecoder.isCompleted()) &#123;</span><br><span class="line">                // Response entity received</span><br><span class="line">                // Ready to receive a new response</span><br><span class="line">                resetInput();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        // Finally set buffered input flag</span><br><span class="line">        this.hasBufferedInput = this.inbuf.hasData();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>读取过程做了如下事情：<br>1.首先调用<code>responseParser.fillBuffer()</code>从管道中读取8KB的字节流出来，接着调用<code>AbstractMessageParser.parse()</code>解析http的头部数据。<br>2.调用<code>NHttpConnectionBase.prepareDecoder()</code>,从header的content-length解析出content的长度。并产生解析数据使用的<code>LengthDelimitedDecoder</code>， 此时8k字节流buffer也放入了LengthDelimitedDecoder中。<br>3.调用<code>HttpAsyncRequestExecutor.responseReceived</code>根据content-length来初始化接收响应使用的buffer[]，默认使用HeapByteBufferAllocator.INSTANCE。<br>4.调用<code>HttpAsyncRequestExecutor.inputReady()</code>来组装整个content（实际会进入<code>SimpleInputBuffer.consumeContent()从channel中读取</code>, 解析数据使用的<code>LengthDelimitedDecoder</code>；然后调用<code>HttpAsyncRequestExecutor.processResponse()</code>释放管道，响应用户。</p>
<p>我们看下<code>AbstractMessageParser.parse()</code>如何解析http头部的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">public T parse() throws IOException, HttpException &#123;</span><br><span class="line">    while (this.state != COMPLETED) &#123;</span><br><span class="line">        if (this.lineBuf == null) &#123;// 先读取最开头的&quot;HTTP/1.1 200 OK&quot;</span><br><span class="line">            this.lineBuf = new CharArrayBuffer(64);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            this.lineBuf.clear(); // 清空接着用</span><br><span class="line">        &#125;</span><br><span class="line">        // 若没有结束的话，每次读取一行,若读取的是\r\n，经过过滤，长度就变成了0，说明headers就读取完了</span><br><span class="line">        final boolean lineComplete = this.sessionBuffer.readLine(this.lineBuf, this.endOfStream);// 从sessionBuffer中读取一行</span><br><span class="line">        final int maxLineLen = this.constraints.getMaxLineLength();</span><br><span class="line">        if (maxLineLen &gt; 0 &amp;&amp;</span><br><span class="line">                (this.lineBuf.length() &gt; maxLineLen ||</span><br><span class="line">                        (!lineComplete &amp;&amp; this.sessionBuffer.length() &gt; maxLineLen))) &#123;</span><br><span class="line">            throw new MessageConstraintException(&quot;Maximum line length limit exceeded&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        if (!lineComplete) &#123;</span><br><span class="line">            break;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        switch (this.state) &#123;//more是0</span><br><span class="line">        case READ_HEAD_LINE:// read_head_line</span><br><span class="line">            try &#123;</span><br><span class="line">                // 算是解析HTTP/1.1 200 OK</span><br><span class="line">                parseHeadLine();</span><br><span class="line">            &#125; catch (final ParseException px) &#123;</span><br><span class="line">                throw new ProtocolException(px.getMessage(), px);</span><br><span class="line">            &#125;</span><br><span class="line">            this.state = READ_HEADERS;// read_headers</span><br><span class="line">            break;</span><br><span class="line">        case READ_HEADERS:// read_headers</span><br><span class="line">            if (this.lineBuf.length() &gt; 0) &#123;</span><br><span class="line">                // 若读取长度为0，就说明读取完了</span><br><span class="line">                final int maxHeaderCount = this.constraints.getMaxHeaderCount();</span><br><span class="line">                if (maxHeaderCount &gt; 0 &amp;&amp; headerBufs.size() &gt;= maxHeaderCount) &#123;</span><br><span class="line">                    throw new MessageConstraintException(&quot;Maximum header count exceeded&quot;);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                parseHeader();</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                this.state = COMPLETED;</span><br><span class="line">            &#125;</span><br><span class="line">            break;</span><br><span class="line">        &#125;</span><br><span class="line">        if (this.endOfStream &amp;&amp; !this.sessionBuffer.hasData()) &#123;</span><br><span class="line">            this.state = COMPLETED;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    if (this.state == COMPLETED) &#123;</span><br><span class="line">        for (final CharArrayBuffer buffer : this.headerBufs) &#123;</span><br><span class="line">            try &#123;</span><br><span class="line">                // 开始解析header</span><br><span class="line">                this.message.addHeader(lineParser.parseHeader(buffer));</span><br><span class="line">            &#125; catch (final ParseException ex) &#123;</span><br><span class="line">                throw new ProtocolException(ex.getMessage(), ex);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return this.message;</span><br><span class="line">    &#125;</span><br><span class="line">    return null;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里涉及到读取state的转变，转变过程如下：<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/httpasycnclient5.png" height="150" width="470"><br>字节流前缀如下：<code>HTTP/1.1 200 OK\r\nheaders\r\n\r\ncontents</code>，可以看到，header与content之间以两个<code>\r\n</code>为分隔符，<code>AbstractMessageParser.parse()</code>就是解析http content之前的内容。</p>
<p>我们再看下<code>SimpleInputBuffer.consumeContent()</code>如何组装整个content。在前面读取headers时，直接从SocketChannelImpl读取了8k字节流，此时仅仅读取了http header部分，8k中也包含了部分content内容，这里也会一起读取出来。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">public int consumeContent(final ContentDecoder decoder) throws IOException &#123;</span><br><span class="line">    // 重新读取</span><br><span class="line">    setInputMode(); </span><br><span class="line">    int totalRead = 0;</span><br><span class="line">    int bytesRead; </span><br><span class="line">    // 可以申请多大的DirectBuffer，就读取多少数据</span><br><span class="line">    while ((bytesRead = decoder.read(this.buffer)) != -1) &#123;</span><br><span class="line">        if (bytesRead == 0) &#123; </span><br><span class="line">            if (!this.buffer.hasRemaining()) &#123;</span><br><span class="line">                expand();</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                break;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            totalRead += bytesRead; // 每次只能读取185472b左右的数据，若多了，这里while也读取不完</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    if (bytesRead == -1 || decoder.isCompleted()) &#123;</span><br><span class="line">        this.endOfStream = true;</span><br><span class="line">    &#125;</span><br><span class="line">    return totalRead;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里就比较简单了，就是循环调用<code>decoder.read</code>来从管道中读取剩余的字节流了。而<code>decoder.read</code>读取http content部分如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public int read(final ByteBuffer dst) throws IOException &#123;</span><br><span class="line">    final int chunk = (int) Math.min((this.contentLength - this.len), Integer.MAX_VALUE);</span><br><span class="line">    final int bytesRead;</span><br><span class="line">    if (this.buffer.hasData()) &#123;</span><br><span class="line">        final int maxLen = Math.min(chunk, this.buffer.length());</span><br><span class="line">        bytesRead = this.buffer.read(dst, maxLen);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        // 一次读取多少文档，取决于从DirectBufferCache中申请的DirectDuffer大小</span><br><span class="line">        bytesRead = readFromChannel(dst, chunk);</span><br><span class="line">    &#125;</span><br><span class="line">    this.len += bytesRead;</span><br><span class="line">    if (this.len &gt;= this.contentLength) &#123;</span><br><span class="line">        setCompleted();</span><br><span class="line">    &#125;</span><br><span class="line">    return isCompleted() &amp;&amp; bytesRead == 0 ? -1 : bytesRead;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情：<br>1.首先将之前8k中未读取的content放入dst中<br>2.再依次从管道中读取剩余所有的content放入dst中。<br>此时整个content部分也读取完成了。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>http请求发送时存在3种可能，1.连接池无可用管道，连接也没达到上限，那么将请求交给boss线程新建管道，再交给worker线程发送请求。2.连接池有可用管道，那么直接将请求交给worker发送。3.连接池无可用管道，且连接个数已达上限，那么请求阻塞等待。每个管道一次只能发送一次请求，下个请求只能等当前请求完成、管道释放后才能进行，通过管道个数来限制连接并发，导致管道利用率不高，这里也许可以进行部分优化。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/elasticsearch_learning/categories/Java学习//" class="article-tag-list-link color2">Java学习</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/elasticsearch_learning/2021/11/05/HTTP异步Client源码解析/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-ES-cat-nodes接口无响应问题定位" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/elasticsearch_learning/2021/08/20/ES-cat-nodes接口无响应问题定位/">ES _cat/nodes接口无响应问题定位</a>
    </h1>
  

        
        <a href="/elasticsearch_learning/2021/08/20/ES-cat-nodes接口无响应问题定位/" class="archive-article-date">
  	<time datetime="2021-08-20T04:21:28.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2021-08-20</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h1><p>目前对ES集群的报警的一个重要接口是_cat&#x2F;nodes，在线上环境，该接口经常超时无响应。</p>
<h1 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h1><h2 id="根据代码入手"><a href="#根据代码入手" class="headerlink" title="根据代码入手"></a>根据代码入手</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">   final ClusterStateRequest clusterStateRequest = new ClusterStateRequest();</span><br><span class="line">    clusterStateRequest.clear().nodes(true);</span><br><span class="line">    return channel -&gt; client.admin().cluster().state(clusterStateRequest, new RestActionListener&lt;ClusterStateResponse&gt;(channel) &#123; </span><br><span class="line">        @Override </span><br><span class="line">        public void processResponse(final ClusterStateResponse clusterStateResponse) &#123; </span><br><span class="line">            ......</span><br><span class="line">            //  向每个节点发送将发送_nodes/&#123;nodeId&#125;</span><br><span class="line">            client.admin().cluster().nodesInfo(nodesInfoRequest, new RestActionListener&lt;NodesInfoResponse&gt;(channel) &#123;</span><br><span class="line">                @Override</span><br><span class="line">                public void processResponse(final NodesInfoResponse nodesInfoResponse) &#123;</span><br><span class="line">                    ...</span><br><span class="line">                    // 向每个节点发送将发送_nodes/stats</span><br><span class="line">                    client.admin().cluster().nodesStats(nodesStatsRequest, new RestResponseListener&lt;NodesStatsResponse&gt;(channel) &#123;</span><br><span class="line">                        @Override // </span><br><span class="line">                        public RestResponse buildResponse(NodesStatsResponse nodesStatsResponse) throws Exception &#123;</span><br><span class="line">                            return ...</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当client 发送_cat&#x2F;nodes请求后，协调节点依次做如下3件事情：<br>1.协调节点向master请求_cluster&#x2F;state集群，以便获取所有节点，其中超时30s。<br>2.协调节点向每个节点发送&#x2F;_nodes&#x2F;{nodeId}请求，获取所有节点的运行信息，包括jvm,os,http,process等运行信息，没有超时时间设置。<br>3.协调节点向每个节点发送&#x2F;_nodes&#x2F;{nodeId}&#x2F;stats请求，获取所有节点当前jvm,os,fs,process运行指标，没有超时时间设置。<br>结论: 首先怀疑肯定是第2、3步出问题，在获取某个指标的时候某些节点超时无响应了。</p>
<h2 id="硬件资源入手"><a href="#硬件资源入手" class="headerlink" title="硬件资源入手"></a>硬件资源入手</h2><p>节点处问题期间，master日志也报如下日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2021-08-12T11:41:16,407][WARN ][o.e.t.TransportService   ] [rz-data-hdp-dn-rtyarn0108] Received response for a request that has timed out, sent [80029ms] ago, timed out [65024ms] ago, action [cluster:monitor/nodes/stats[n]], node [&#123;data1&#125;], id [95175528]</span><br></pre></td></tr></table></figure>

<p>大致可以猜测是这个节点导致的接口超时。观察硬件指标如下：<br>内存：节点内存占用率并不高，可能才50%-70%<br>cpu：cpu也没有完全使用完，不到70%<br>io：部分节点的io使用率持续性达到了100%<br>结论: 此时基于以上指标，可以断定是因为IO持续达到100%，导致的请求无响应。</p>
<h2 id="再具体分析每步请求"><a href="#再具体分析每步请求" class="headerlink" title="再具体分析每步请求"></a>再具体分析每步请求</h2><p>1.分析第一步<br>协调节点向集群master发送请求，获取所有节点。此时master所有返回结果都是从内存中拿的，不涉及到IO，且master负载较低。肯定不是第一步出现了问题。<br>2.分析第二步<br>每个节点在响应时，会获取本地jvm,os,http,process等初始化信息，但是这些信息在ES启动时就已经完成初始化了，之后每次请求也都是从内存中获取现成的，不涉及到IO操作，看起来也不是第2步出现了问题。<br>3.分析第三步<br>每个节点在响应时，会获取本地jvm,os,fs,process等运行指标，这些指标都是实时获取的，其中fs、jvm等运行指标会与jvm,磁盘打交道，容易受节点压力大导致响应超时等现象。为了复现这些指标的获取，甚至将这些指标单独摘取出来，在IO负载高的机器上单独运行，指标都获取很快，并没有出现超时的现象。<br>结论</p>
<p>理论分析看起来完全行不通，从代码层面解释不清楚为啥接口超时的情况</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="测试1"><a href="#测试1" class="headerlink" title="测试1"></a>测试1</h3><p>虽然第3步指标获取单独跳出来，测试没啥问题，可能分析没把重要的一步给找到，那么我们就开发特定代码，将第3步中的指标获取全部取消，这样确定第3步只涉及到内存操作，理论上超时现象应该可以极大缓解。<br>经过线上压力不大的集群进行_cat&#x2F;nodes线上测试：</p>
<ul>
<li>正常接口下，平均请求耗时在3s+</li>
<li>特定代码的平均耗时在0.5s+<br>看起来的确找到了问题。</li>
</ul>
<p>经过线上压力较大的集群进行_cat&#x2F;nodes线上测试，理论上效果也良好：</p>
<ul>
<li>正常接口下，平均请求耗时在25s+</li>
<li>特定代码的平均耗时在25s+<br>特定代码看起来 并没有一丝变好。<br>结论：确定第三步中实时fs,jvm等指标获取并不耗时。</li>
</ul>
<h3 id="测试2"><a href="#测试2" class="headerlink" title="测试2"></a>测试2</h3><p>模仿协调节点依次向master发送_cluster&#x2F;state，每个数据节点发送&#x2F;_nodes&#x2F;{nodeId}、&#x2F;_nodes&#x2F;{nodeId}&#x2F;stats请求，发现协调节点在&#x2F;_nodes&#x2F;{nodeId}时，IO负载高的节点总是超时，有时1min+都无响应，确定是这步操作出问题了。我们又仔细核对了第二步响应的指标，的确都是从内存中获取的，完全与IO无关。<br>结论：第二步的确出问题了，但是与IO，内存都无关。</p>
<h3 id="测试3"><a href="#测试3" class="headerlink" title="测试3"></a>测试3</h3><p>我们尝试向问题节点发送_cat&#x2F;pending_tasks等请求，发现这些请求响应非常快。他们区别就是可能使用的线程池不一样，第一反应就是问题节点接收&#x2F;_nodes&#x2F;{nodeId}&#x2F;stats的线程池被打满了。</p>
<ul>
<li>经过确定，问题节点处理&#x2F;_nodes&#x2F;{nodeId}&#x2F;stats请求时使用的是management线程池，而_cat&#x2F;pending_tasks没有使用线程池，线程池的确是有区别</li>
<li>获取线上每个节点线程池的使用请求：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&quot;management&quot; : &#123;</span><br><span class="line">  &quot;threads&quot; : 5,</span><br><span class="line">  &quot;queue&quot; : 793,</span><br><span class="line">  &quot;active&quot; : 5,</span><br><span class="line">  &quot;rejected&quot; : 0,</span><br><span class="line">  &quot;largest&quot; : 5,</span><br><span class="line">  &quot;completed&quot; : 888436</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure></li>
</ul>
<p>发现该类线程池存在严重的堆积情况，工作线程只有5个，但是排队的线程个数达到了793个，的确验证了线程池被打满，导致请求无法响应的情况。</p>
<ul>
<li>我们看下management线程在做哪些事情呢：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&quot;elasticsearch[data1][management][T#5]&quot; #176 daemon prio=5 os_prio=0 tid=0x00007f7854003800 nid=0x7e2 runnable [0x00007f79ab8f8000]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line">        at sun.nio.ch.FileDispatcherImpl.force0(Native Method)</span><br><span class="line">        at sun.nio.ch.FileDispatcherImpl.force(FileDispatcherImpl.java:76)</span><br><span class="line">        at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:388)</span><br><span class="line">        at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:471)</span><br><span class="line">        at org.apache.lucene.store.FSDirectory.syncMetaData(FSDirectory.java:309)</span><br><span class="line">        at org.elasticsearch.gateway.MetadataStateFormat.performStateDirectoriesFsync(MetadataStateFormat.java:172)</span><br><span class="line">        at org.elasticsearch.gateway.MetadataStateFormat.write(MetadataStateFormat.java:246)</span><br><span class="line">        at org.elasticsearch.gateway.MetadataStateFormat.writeAndCleanup(MetadataStateFormat.java:185)</span><br><span class="line">        at org.elasticsearch.index.seqno.ReplicationTracker.persistRetentionLeases(ReplicationTracker.java:494)</span><br><span class="line">        - locked &lt;0x00007f7c4981d978&gt; (a java.lang.Object)</span><br><span class="line">        at org.elasticsearch.index.shard.IndexShard.persistRetentionLeases(IndexShard.java:2256)</span><br><span class="line">        at org.elasticsearch.index.seqno.RetentionLeaseBackgroundSyncAction.lambda$shardOperationOnPrimary$0(RetentionLeaseBackgroundSyncAction.java:161)</span><br><span class="line">        at org.elasticsearch.index.seqno.RetentionLeaseBackgroundSyncAction$$Lambda$5355/1679727689.get(Unknown Source)</span><br><span class="line">        at org.elasticsearch.action.ActionListener.completeWith(ActionListener.java:325)</span><br><span class="line">        at org.elasticsearch.index.seqno.RetentionLeaseBackgroundSyncAction.shardOperationOnPrimary(RetentionLeaseBackgroundSyncAction.java:157)</span><br><span class="line">        at org.elasticsearch.index.seqno.RetentionLeaseBackgroundSyncAction.shardOperationOnPrimary(RetentionLeaseBackgroundSyncAction.java:64)</span><br><span class="line">        at org.elasticsearch.action.support.replication.TransportReplicationAction$PrimaryShardReference.perform(TransportReplicationAction.java:968)</span><br><span class="line">        at org.elasticsearch.action.support.replication.ReplicationOperation.execute(ReplicationOperation.java:122)</span><br><span class="line">        at org.elasticsearch.action.support.replication.TransportReplicationAction$AsyncPrimaryAction.runWithPrimaryShardReference(TransportReplicationAction.java:429)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>发现management线程在被别的命令拿来使用IO落盘操作，因为IO压力大，导致落盘效率极低，导致请求出现严重堆积情况。问题原因彻底找到了。扫描整个代码中，management线程池在许多接口中使用。<br>目前<a href="https://github.com/elastic/elasticsearch/pull/62753#issuecomment-948551092" target="_blank" rel="noopener">向社区反馈</a>，社区还未给出解决办法。</p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>协调节点使用management线程池处理&#x2F;_nodes&#x2F;{nodeId}&#x2F;stats请求，但是management线程池与其他接口一起共用，其他接口有落盘操作，IO打满导致落盘落不下去，极大降低了management线程工作效率，进而影响到_cat&#x2F;nodes请求。根本原因还是IO压力大导致的。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">_cat/nodes接口</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/elasticsearch_learning/categories/Elasticsearch//" class="article-tag-list-link color4">Elasticsearch</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/elasticsearch_learning/2021/08/20/ES-cat-nodes接口无响应问题定位/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-Lucene8-6-2底层架构-Segment-StoredFields合并原理详解" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/elasticsearch_learning/2021/06/13/Lucene8-6-2底层架构-Segment-StoredFields合并原理详解/">Lucene8.6.2底层架构-Segment StoredFields合并原理详解</a>
    </h1>
  

        
        <a href="/elasticsearch_learning/2021/06/13/Lucene8-6-2底层架构-Segment-StoredFields合并原理详解/" class="archive-article-date">
  	<time datetime="2021-06-13T09:09:57.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2021-06-13</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在<a href="https://kkewwei.github.io/elasticsearch_learning/2019/10/17/ES%E6%AE%B5%E5%90%88%E5%B9%B6%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">ES段合并逻辑分析</a>我们详细介绍了如何进行查找一个合并的段，本文主要讲解合并线程的Merge内如何进行StoredFields合并的。StoredFields合并过程其实新的StoredFields构建过程，构建过程可参考<a href="https://kkewwei.github.io/elasticsearch_learning/2019/10/29/Lucenec%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-fdt-fdx%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B/">Lucene8.2.0底层架构-fdt&#x2F;fdx构建过程</a>，本文以Lucene8.6.2结构为基础，StoredFields底层索引结构相对Lucene8.2.0有较小改变，为了便于本文详解，这里首先贴出StoredFields底层文件的索引结构：<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene7.8.2_segment_fdt1.png" height="400" width="1000"></p>
<h1 id="构建CompressingStoredFieldsWriter对象"><a href="#构建CompressingStoredFieldsWriter对象" class="headerlink" title="构建CompressingStoredFieldsWriter对象"></a>构建CompressingStoredFieldsWriter对象</h1><p>merge的过程就是将几个Segment重新合并、写到一个segment中，其中必然需要先创建一个新的Merge，并构建新Segment storedFields写入的Writer，本文将以<code>SegmentMerger.mergeFields()</code>开始介绍：首先构建CompressingStoredFieldsWriter对象，然后进行StoredFields的写入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">private int mergeFields() throws IOException &#123;</span><br><span class="line">  try (StoredFieldsWriter fieldsWriter = codec.storedFieldsFormat().fieldsWriter(directory, mergeState.segmentInfo, context)) &#123;</span><br><span class="line">    return fieldsWriter.merge(mergeState);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="开始合并"><a href="#开始合并" class="headerlink" title="开始合并"></a>开始合并</h1><p>merge过程是整个StoredFields写入的核心代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">public int merge(MergeState mergeState) throws IOException &#123;</span><br><span class="line">  // 统计当前merge中所有segment中包含的文档数</span><br><span class="line">  int docCount = 0; </span><br><span class="line">  int numReaders = mergeState.maxDocs.length; // 多少个semgent来合并</span><br><span class="line">  MatchingReaders matching = new MatchingReaders(mergeState);</span><br><span class="line">  if (mergeState.needsIndexSort) &#123;</span><br><span class="line">    ....</span><br><span class="line">  &#125;</span><br><span class="line">  for (int readerIndex=0;readerIndex&lt;numReaders;readerIndex++) &#123;</span><br><span class="line">    ......</span><br><span class="line">    // 这个segment的文档ID</span><br><span class="line">    final int maxDoc = mergeState.maxDocs[readerIndex];</span><br><span class="line">    final Bits liveDocs = mergeState.liveDocs[readerIndex];</span><br><span class="line">     // 一般跳过if</span><br><span class="line">    // if its some other format, or an older version of this format, or safety switch:</span><br><span class="line">    if (matchingFieldsReader == null || matchingFieldsReader.getVersion() != VERSION_CURRENT </span><br><span class="line">        || BULK_MERGE_ENABLED == false) &#123;</span><br><span class="line">     ......</span><br><span class="line">    &#125; else if (matchingFieldsReader.getCompressionMode() == compressionMode &amp;&amp;</span><br><span class="line">               matchingFieldsReader.getChunkSize() == chunkSize &amp;&amp; </span><br><span class="line">               matchingFieldsReader.getPackedIntsVersion() == PackedInts.VERSION_CURRENT &amp;&amp;</span><br><span class="line">               liveDocs == null &amp;&amp;</span><br><span class="line">               !tooDirty(matchingFieldsReader)) &#123;</span><br><span class="line">          批量读取StoredFields并写入新的StoredFields中       </span><br><span class="line">    &#125; else &#123;</span><br><span class="line">          读取每个doc，将doc写入新的StoredFields中</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  finish(mergeState.mergeFieldInfos, docCount);</span><br><span class="line">  return docCount;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该方法会轮循每个segment，检查每个StoredFields：是否有删除或者、或者脏chunk个数大于1024个、或者脏chunk占比该segment大于1%。<br>1.若不成立，则进入<code>else if</code>批量读取旧StoredFields中的chunk写到新的StoredFields中<br>2.若成立：那么就会进入<code>else</code>读取每个文档，重新读取每个doc, 通过CompressingStoredFieldsWriter逐个将文档写入内存中；<br>3.最终将缓存文档刷成StoredFields.</p>
<h2 id="批量读取旧StoredFields中的chunk写到新的StoredFields中"><a href="#批量读取旧StoredFields中的chunk写到新的StoredFields中" class="headerlink" title="批量读取旧StoredFields中的chunk写到新的StoredFields中"></a>批量读取旧StoredFields中的chunk写到新的StoredFields中</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">//检验fdx的正确性</span><br><span class="line"> matchingFieldsReader.checkIntegrity(); </span><br><span class="line"> // flush any pending chunks</span><br><span class="line"> // 如果还有未写入的段</span><br><span class="line"> if (numBufferedDocs &gt; 0) &#123; </span><br><span class="line">   flush();</span><br><span class="line">   numDirtyChunks++; // incomplete: we had to force this flush</span><br><span class="line"> &#125;</span><br><span class="line"> // fdt文件</span><br><span class="line"> IndexInput rawDocs = matchingFieldsReader.getFieldsStream(); </span><br><span class="line"> // FieldsIndexReader</span><br><span class="line"> FieldsIndex index = matchingFieldsReader.getIndexReader(); </span><br><span class="line"> // 首先从第0个chunk开始找</span><br><span class="line"> rawDocs.seek(index.getStartPointer(0));</span><br><span class="line"> int docID = 0; // 这个segment目前的文档数</span><br><span class="line"> // 每循环一次，读取一个chunk</span><br><span class="line"> while (docID &lt; maxDoc) &#123;</span><br><span class="line">   // read header</span><br><span class="line">   int base = rawDocs.readVInt();</span><br><span class="line">   int code = rawDocs.readVInt();</span><br><span class="line">   // write a new index entry and new header for this chunk.</span><br><span class="line">   int bufferedDocs = code &gt;&gt;&gt; 1;</span><br><span class="line">   //  向新的dft写入</span><br><span class="line">   indexWriter.writeIndex(bufferedDocs, fieldsStream.getFilePointer());</span><br><span class="line">   fieldsStream.writeVInt(docBase); // rebase</span><br><span class="line">   fieldsStream.writeVInt(code);</span><br><span class="line">   docID += bufferedDocs;</span><br><span class="line">   docBase += bufferedDocs;</span><br><span class="line">   docCount += bufferedDocs;</span><br><span class="line">   if (docID == maxDoc) &#123;</span><br><span class="line">     end = matchingFieldsReader.getMaxPointer();</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     // 下个chunk的起始位置</span><br><span class="line">     end = index.getStartPointer(docID); </span><br><span class="line">   &#125; // 从原始fdt中批量拷贝数据，写入新的fdt中</span><br><span class="line">   fieldsStream.copyBytes(rawDocs, end - rawDocs.getFilePointer());</span><br><span class="line"> &#125; //是拷贝fdt中每个chunK的后三项（numStoredFields, Doclength BufferDoc）</span><br><span class="line"> // since we bulk merged all chunks, we inherit any dirty ones from this segment.</span><br><span class="line"> numChunks += matchingFieldsReader.getNumChunks();</span><br><span class="line"> numDirtyChunks += matchingFieldsReader.getNumDirtyChunks();</span><br></pre></td></tr></table></figure>

<p>主要做了如下步骤：<br>1.首先检查是否还有未刷新的文档，若有的话，则调用flush进行刷新。<br>2.调用<code>index.getStartPointer(0)</code>查询到第0个文档中dft的起始位置，然后调用ChecksumIndexInput.seek()跳转到文件指定位置。<br>3.轮循这个Segment每个chunk, 依次读取docBase(用来更新全局的docBase，旧的docBase将废弃)、bufferedDocs、本chunk在fdt文件中的终点。最终会copy chunk后三部分内容。</p>
<h3 id="找到docId在fdt存储的起始位置"><a href="#找到docId在fdt存储的起始位置" class="headerlink" title="找到docId在fdt存储的起始位置"></a>找到docId在fdt存储的起始位置</h3><p>首先我们详细介绍下<code>FieldsIndexReader.getStartPointer(docId)</code>如何从fdm中将文档docId所在chunk的存储地址（fdt中存储的起始位置）读取出来的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">long getStartPointer(int docID) &#123;</span><br><span class="line">    FutureObjects.checkIndex(docID, maxDoc);</span><br><span class="line">    // 找到这个docId的文档在哪个chunk上</span><br><span class="line">    long blockIndex = docs.binarySearch(0, numChunks, docID); </span><br><span class="line">    // 找一个chunk内的docId</span><br><span class="line">    if (blockIndex &lt; 0) &#123;</span><br><span class="line">      blockIndex = -2 - blockIndex;</span><br><span class="line">    &#125;</span><br><span class="line">    return startPointers.get(blockIndex); // DirectMonotonicReader</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了三个操作：<br>1.首选确定这个docId一定在这segment中存在，那么第2步骤一定可以查询到这个docId存在某个chunk上。<br>2.调用<code>DirectMonotonicReader.binarySearch</code>进行二分搜索这个docId落在了哪个chunk上, 退出该函数的条件有两个:<br>+.正好命中原始累加值, 那么就直接返回该id所在chunkId<br>+.low&#x3D;hig+1, 且id在当前范围内, 则返回 -1 - low（那么最外层-2-blockIndex&#x3D;low-1&#x3D;high), 这时，该docId一定在第low-1个chunk上<br>3.调用startPointers()，解析fdm文件的filepointer部分，获取这个chunk在fdt上存储的起始位置。<br>后面会分别详细展开第2、3步骤。<br>我们首先看下<code>DirectMonotonicReader.binarySearch</code>二叉搜索过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public long binarySearch(long fromIndex, long toIndex, long key) &#123;</span><br><span class="line">   if (fromIndex &lt; 0 || fromIndex &gt; toIndex) &#123;</span><br><span class="line">     throw new IllegalArgumentException(&quot;fromIndex=&quot; + fromIndex + &quot;,toIndex=&quot; + toIndex);</span><br><span class="line">   &#125;</span><br><span class="line">   long lo = fromIndex;</span><br><span class="line">   long hi = toIndex - 1;</span><br><span class="line">   while (lo &lt;= hi) &#123;</span><br><span class="line">     final long mid = (lo + hi) &gt;&gt;&gt; 1;</span><br><span class="line">     // Try to run as many iterations of the binary search as possible without</span><br><span class="line">     // hitting the direct readers, since they might hit a page fault.</span><br><span class="line">     final long[] bounds = getBounds(mid);</span><br><span class="line">     if (bounds[1] &lt; key) &#123;</span><br><span class="line">       lo = mid + 1;</span><br><span class="line">     &#125; else if (bounds[0] &gt; key) &#123;</span><br><span class="line">       hi = mid - 1;</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       final long midVal = get(mid);</span><br><span class="line">       if (midVal &lt; key) &#123;</span><br><span class="line">         lo = mid + 1;</span><br><span class="line">       &#125; else if (midVal &gt; key) &#123;</span><br><span class="line">         hi = mid - 1;</span><br><span class="line">       &#125; else &#123;</span><br><span class="line">         return mid;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   return -1 - lo;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>我们首先需要知道DirectMonotonicWriter的存储方式：比如我们需要存储3，2，6，2，6(每个数字表示每个chunk的文档个数), 那么在存储的时候，实际DirectMonotonicReader存储的数字是3，5，11，13，19，使之变成了递增数组。此函数就是为了找key&#x3D;docId的文档在哪个chunk中。在递增数组中可以通过二分查找来确定。<br>我们需要看下两个细节：<code>getBounds(mid)</code>确定这个chunk的文档id的范围，<code>get(mid)</code>确定这个chunk的起始位置。为了更清晰帮助用户理解，先讲解下<code>DirectMonotonicWriter.flush</code>是如何存储数据的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">// 在storedField写入时，buffer里面每个元素，都是一个chunk</span><br><span class="line">private void flush() throws IOException &#123; </span><br><span class="line">  assert bufferSize != 0;</span><br><span class="line">  // 首先计算这组数据的梯度差</span><br><span class="line">  final float avgInc = (float) ((double) (buffer[bufferSize-1] - buffer[0]) / Math.max(1, bufferSize - 1));</span><br><span class="line">  for (int i = 0; i &lt; bufferSize; ++i) &#123;</span><br><span class="line">    // 期望值从0开始</span><br><span class="line">    final long expected = (long) (avgInc * (long) i); </span><br><span class="line">    //实际值-期望值</span><br><span class="line">    buffer[i] -= expected;</span><br><span class="line">  &#125;</span><br><span class="line">  // 找出最小的值</span><br><span class="line">  long min = buffer[0];</span><br><span class="line">  for (int i = 1; i &lt; bufferSize; ++i) &#123;</span><br><span class="line">    min = Math.min(buffer[i], min);</span><br><span class="line">  &#125;</span><br><span class="line">  // 可能的最大值</span><br><span class="line">  long maxDelta = 0; </span><br><span class="line">  for (int i = 0; i &lt; bufferSize; ++i) &#123;</span><br><span class="line">    buffer[i] -= min;//比最小值大多少</span><br><span class="line">    // use | will change nothing when it comes to computing required bits</span><br><span class="line">    // but has the benefit of working fine with negative values too</span><br><span class="line">    // (in case of overflow)</span><br><span class="line">    maxDelta |= buffer[i];</span><br><span class="line">  &#125;</span><br><span class="line">  //第一步，先存储chunk个数与理论平均值差距的最小值</span><br><span class="line">  meta.writeLong(min); </span><br><span class="line">  // 第二步，存储理论平均值</span><br><span class="line">  meta.writeInt(Float.floatToIntBits(avgInc)); </span><br><span class="line">  // 在fdx中的起始位置</span><br><span class="line">  meta.writeLong(data.getFilePointer() - baseDataPointer); </span><br><span class="line">  if (maxDelta == 0) &#123;</span><br><span class="line">    meta.writeByte((byte) 0);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // 需要几位</span><br><span class="line">    final int bitsRequired = DirectWriter.unsignedBitsRequired(maxDelta);</span><br><span class="line">    DirectWriter writer = DirectWriter.getInstance(data, bufferSize, bitsRequired);</span><br><span class="line">    for (int i = 0; i &lt; bufferSize; ++i) &#123;</span><br><span class="line">      // 每个chunk与理论平均值之间的差距</span><br><span class="line">      writer.add(buffer[i]); </span><br><span class="line">    &#125;</span><br><span class="line">    writer.finish();</span><br><span class="line">    // 写入需要的byte个数</span><br><span class="line">    meta.writeByte((byte) bitsRequired); </span><br><span class="line">  &#125;</span><br><span class="line">  bufferSize = 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>仍然以上面的数组示例来说明存储了数据到底是啥：<br>原始数：  3     2     6         2      6<br>写入前：  3     5    11        13     19  &#x2F;&#x2F;原始数据累加值，-&gt;avgInc：(19-3)&#x2F;4 &#x3D; 4<br>梯度值：  0     4     8        12     16<br>变换1：   3     1     3        1      3   &#x2F;&#x2F;写入前-梯度值, -&gt;最小值min:  1<br>变换2：   2     0     2        0      2   &#x2F;&#x2F;变换1-min，-&gt;maxDelta &#x3D;max(变换2)<br>bound：(1,4) (5,8) (9,12)  (13,16) (17,20)<br>1.若还原第1个(下标从0开始)元素写入前的原始日志：1+4x1+0&#x3D;5，也就是 <code>min+avg*index+变换2</code>，这个就是DirectMonotonicReader.get()的实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//index是下标（一个元素就是一个chunk个数）,返回的是这个chunk的文档个数</span><br><span class="line">public long get(long index) &#123; </span><br><span class="line">  // 在第几个block上</span><br><span class="line">  final int block = (int) (index &gt;&gt;&gt; blockShift); </span><br><span class="line">  // 这个block内第几个chunk</span><br><span class="line">  final long blockIndex = index &amp; ((1 &lt;&lt; blockShift) - 1);</span><br><span class="line">  // 从fdx中读偏移量 </span><br><span class="line">  final long delta = readers[block].get(blockIndex); </span><br><span class="line">  // 返回这chunk对应的数字</span><br><span class="line">  return mins[block] + (long) (avgs[block] * blockIndex) + delta; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2.若我们想估算第i个元素(一个元素就是一个chunk的个数)的取值范围：<br>第i个元素&#x3D;mins[block] + (long) (avgs[block] * blockIndex) + delta,那么下列等式将成立：<br>lowerBound &#x3D; mins[block] + (long) (avgs[block] * blockIndex)<br>upperBound &#x3D; lowerBound + maxDelta &#x3D; lowerBound + (1 &lt;&lt; bitsRequired -1)<br>这个也就是<code>DirectMonotonicReader.getBounds(mid)</code>的定义:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> // index=第i个chunk</span><br><span class="line">private long[] getBounds(long index) &#123;</span><br><span class="line">  // 这个chunk在第几个block上上</span><br><span class="line">  final int block = Math.toIntExact(index &gt;&gt;&gt; blockShift); </span><br><span class="line">  // 落到某个block内的chunk数</span><br><span class="line">  final long blockIndex = index &amp; ((1 &lt;&lt; blockShift) - 1); </span><br><span class="line">  // 这个chunk的下限</span><br><span class="line">  final long lowerBound = mins[block] + (long) (avgs[block] * blockIndex);</span><br><span class="line">  // 这个chunk的上限 </span><br><span class="line">  final long upperBound = lowerBound + (1L &lt;&lt; bpvs[block]) - 1;</span><br><span class="line">  if (bpvs[block] == 64 || upperBound &lt; lowerBound) &#123;</span><br><span class="line">    return new long[] &#123; Long.MIN_VALUE, Long.MAX_VALUE &#125;;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    return new long[] &#123; lowerBound, upperBound &#125;;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="从fdt中循环读取每个chunk"><a href="#从fdt中循环读取每个chunk" class="headerlink" title="从fdt中循环读取每个chunk"></a>从fdt中循环读取每个chunk</h3><p>当前已知chunkId编号，则直接从fdm中该chunk的filePointer部分，根据前面提到的<code>DirectMonotonicReader.get()</code>取出该doc所在chunk在fdt中的存储位置。然后进入BlockState.reset(int docID)加载具体的chunk内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">void reset(int docID) throws IOException &#123;</span><br><span class="line">  boolean success = false;</span><br><span class="line">  try &#123;</span><br><span class="line">  //读取这个docID所在chunk的所有原始数据</span><br><span class="line">    doReset(docID); </span><br><span class="line">    success = true;</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    if (success == false) &#123;</span><br><span class="line">      // if the read failed, set chunkDocs to 0 so that it does not</span><br><span class="line">      // contain any docs anymore and is not reused. This should help</span><br><span class="line">      // get consistent exceptions when trying to get several</span><br><span class="line">      // documents which are in the same corrupted block since it will</span><br><span class="line">      // force the header to be decoded again</span><br><span class="line">      chunkDocs = 0;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">private void doReset(int docID) throws IOException &#123;</span><br><span class="line">  // 获取这个docID所在的chnk在fdt中的起始位置</span><br><span class="line">  docBase = fieldsStream.readVInt(); </span><br><span class="line">  final int token = fieldsStream.readVInt();</span><br><span class="line">  chunkDocs = token &gt;&gt;&gt; 1;</span><br><span class="line">   // 是否压缩了</span><br><span class="line">  sliced = (token &amp; 1) != 0;</span><br><span class="line">  offsets = ArrayUtil.grow(offsets, chunkDocs + 1);</span><br><span class="line">  numStoredFields = ArrayUtil.grow(numStoredFields, chunkDocs);</span><br><span class="line">  if (chunkDocs == 1) &#123;</span><br><span class="line">    numStoredFields[0] = fieldsStream.readVInt();</span><br><span class="line">    offsets[1] = fieldsStream.readVInt();</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // Number of stored fields per document</span><br><span class="line">    final int bitsPerStoredFields = fieldsStream.readVInt();</span><br><span class="line">    // 首先读取numDoc个numStoredFields</span><br><span class="line">    if (bitsPerStoredFields == 0) &#123; </span><br><span class="line">      Arrays.fill(numStoredFields, 0, chunkDocs, fieldsStream.readVInt());</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields, 1);</span><br><span class="line">      for (int i = 0; i &lt; chunkDocs; ++i) &#123;</span><br><span class="line">        numStoredFields[i] = (int) it.next();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // The stream encodes the length of each document and we decode</span><br><span class="line">    // it into a list of monotonically increasing offsets  其次读取numDoc个docLength</span><br><span class="line">    final int bitsPerLength = fieldsStream.readVInt();</span><br><span class="line">    // 长度一致</span><br><span class="line">    if (bitsPerLength == 0) &#123; </span><br><span class="line">      final int length = fieldsStream.readVInt();</span><br><span class="line">      for (int i = 0; i &lt; chunkDocs; ++i) &#123;</span><br><span class="line">        offsets[1 + i] = (1 + i) * length; //</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">       // 一般都跑到这里，计算出这个doc在fdt中的偏移量。</span><br><span class="line">      final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);</span><br><span class="line">      for (int i = 0; i &lt; chunkDocs; ++i) &#123;</span><br><span class="line">        offsets[i + 1] = (int) it.next();</span><br><span class="line">      &#125;</span><br><span class="line">      for (int i = 0; i &lt; chunkDocs; ++i) &#123;</span><br><span class="line">        // 在fdt中每个doc的偏移量</span><br><span class="line">        offsets[i + 1] += offsets[i]; </span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // Additional validation: only the empty document has a serialized length of 0</span><br><span class="line">    for (int i = 0; i &lt; chunkDocs; ++i) &#123;</span><br><span class="line">      final int len = offsets[i + 1] - offsets[i];</span><br><span class="line">      final int storedFields = numStoredFields[i];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 读取当前读到的地方</span><br><span class="line">  startPointer = fieldsStream.getFilePointer();</span><br><span class="line">  // 读取真正的doc 原始压缩数据</span><br><span class="line">  if (merging) &#123;</span><br><span class="line">    final int totalLength = offsets[chunkDocs];</span><br><span class="line">    // decompress eagerly</span><br><span class="line">    if (sliced) &#123;</span><br><span class="line">      bytes.offset = bytes.length = 0;</span><br><span class="line">      for (int decompressed = 0; decompressed &lt; totalLength; ) &#123;</span><br><span class="line">        final int toDecompress = Math.min(totalLength - decompressed, chunkSize);</span><br><span class="line">        decompressor.decompress(fieldsStream, toDecompress, 0, toDecompress, spare);</span><br><span class="line">        bytes.bytes = ArrayUtil.grow(bytes.bytes, bytes.length + spare.length);</span><br><span class="line">        System.arraycopy(spare.bytes, spare.offset, bytes.bytes, bytes.length, spare.length);</span><br><span class="line">        bytes.length += spare.length;</span><br><span class="line">        decompressed += toDecompress;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      decompressor.decompress(fieldsStream, totalLength, 0, totalLength, bytes);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>读取每个chunk具体的过程，和写chunk过程是一一对应的，要是感兴趣的话，可以看下<code>CompressingStoredFieldsWriter.flush</code>过程，这里就不再赘述。</p>
<h2 id="有删除或者脏chunk占比过大"><a href="#有删除或者脏chunk占比过大" class="headerlink" title="有删除或者脏chunk占比过大"></a>有删除或者脏chunk占比过大</h2><p>针对以下两种情况：对有文档删除的segment，我们需要在新的Segment中踢掉已经处于删除状态的文档；脏chunk占比过大，会影响压缩效率，我们也需要重建Segment, 基于此只能读取旧StoredFields中的每个文档，重新逐条写入新的的Segments中。具体操作如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">for (int docID = 0; docID &lt; maxDoc; docID++) &#123;</span><br><span class="line">   // 这个节点需要被删除，那么直接忽略</span><br><span class="line">   if (liveDocs != null &amp;&amp; liveDocs.get(docID) == false) &#123; </span><br><span class="line">     continue;</span><br><span class="line">   &#125;</span><br><span class="line">   SerializedDocument doc = matchingFieldsReader.document(docID);</span><br><span class="line">   startDocument();</span><br><span class="line">   // 读取出来</span><br><span class="line">   bufferedDocs.copyBytes(doc.in, doc.length);</span><br><span class="line">   numStoredFieldsInDoc = doc.numStoredFields;</span><br><span class="line">   finishDocument();</span><br><span class="line">   ++docCount;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>由于需要重新组织新的chunk结构，那么就需要每个live文档从旧StoredFields读取出来，写入新的chunk中。会轮循每个文档，每个文档都做了两个事情：<br>1.检查该文档所在的chunk是否已经加载到内存中，若没有加载到的话，那么调用<code>BlockState.reset(int docID)</code>加载chunk，然后放入内存中。<br>2.从内存中读取这个文档的value,写入即将创建的chunk中，此时新写入缓存的文档，将在最后的finish统一刷入文件形成一个新的chunk。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>StoredFields合并过程主要是循环每个segment，针对每个segment都从文档中读取出来，写入新的chunk中。读取每个segment过程分为两种；一种是批量copy整个chunk的值，写入新的chunk，然后再copy下个chunk，一种方式是读取该segment中每个文档，然后写入新的chunk，当然批量copy整个chunk的效率更高。选取哪种方式读取，取决于这个sement是否存在删除，或者存在dirtyChunk过多的情况。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/elasticsearch_learning/categories/Lucene//" class="article-tag-list-link color2">Lucene</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/elasticsearch_learning/2021/06/13/Lucene8-6-2底层架构-Segment-StoredFields合并原理详解/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-Lucene8-6-2底层架构-Point查询过程" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/elasticsearch_learning/2021/01/03/Lucene8-6-2底层架构-Point查询过程/">Lucene8.6.2底层架构-Point查询过程</a>
    </h1>
  

        
        <a href="/elasticsearch_learning/2021/01/03/Lucene8-6-2底层架构-Point查询过程/" class="archive-article-date">
  	<time datetime="2021-01-03T11:56:55.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2021-01-03</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/elasticsearch_learning/categories/Lucene//" class="article-tag-list-link color2">Lucene</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/elasticsearch_learning/2021/01/03/Lucene8-6-2底层架构-Point查询过程/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-Lucene8-6-2底层架构-BKW树构建过程" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/elasticsearch_learning/2020/11/01/Lucene8-6-2底层架构-BKW树构建过程/">Lucene8.6.2底层架构-BKW树构建过程</a>
    </h1>
  

        
        <a href="/elasticsearch_learning/2020/11/01/Lucene8-6-2底层架构-BKW树构建过程/" class="archive-article-date">
  	<time datetime="2020-11-01T08:22:41.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2020-11-01</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>针对数值型的倒排索引，Lecene从6.X引入了BKD树结构，BKD全称：Block K-Dimension Balanced Tree。在此之前，数值型查找和String结构一样，使用<a href="https://kkewwei.github.io/elasticsearch_learning/2020/02/25/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-%E8%AF%8D%E5%85%B8fst%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/">FST结构</a>）建立索引，FST结构针对精确匹配存在较大的优势，但是数值型很大部分使用场景为范围查找, BKD树就是解决这类使用场景的。若我们将多维简化为一维时，结构就是bst(二叉查找树)。</p>
<h1 id="数据放入内存中"><a href="#数据放入内存中" class="headerlink" title="数据放入内存中"></a>数据放入内存中</h1><p>BKD树支持多维范围，数值型包括int,float,point等， 这里就以int类型写入作为示例，将<code>age</code>建构为三维：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Document document = new Document();</span><br><span class="line">document.add(new IntPoint(&quot;age&quot;, i, i*i, i%20));</span><br><span class="line">indexWriter.addDocument(document);</span><br></pre></td></tr></table></figure>

<p>IntPoint内部会将多维转变为一维数组，转变过程比较简单，比如int，将转变为长度为3*4&#x3D;12的byte数组。真正开始在内存中建立索引结构是在<code>DefaultIndexingChain.indexPoint()</code>处:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">private void indexPoint(int docID, PerField fp, IndexableField field) &#123;</span><br><span class="line">  fp.pointValuesWriter.addPackedValue(docID, field.binaryValue());</span><br><span class="line">&#125;</span><br><span class="line">// Point有多个值的话，都会堆砌到一个BytesRef中</span><br><span class="line"> public void PointValuesWriter.addPackedValue(int docID, BytesRef value) &#123; </span><br><span class="line">  bytes.append(value);</span><br><span class="line">  docIDs[numPoints] = docID; </span><br><span class="line">  if (docID != lastDocID) &#123;</span><br><span class="line">    numDocs++;</span><br><span class="line">    lastDocID = docID;</span><br><span class="line">  &#125;</span><br><span class="line">  numPoints++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中，使用ByteBlockPool弹性扩容的功能存储byte数组的value， docIDs记录的是第几个point对应的文档号。numPoints记录的是point的个数（一个point可以由多个域构成），因为存在一个字段，会存储多个point的情况。</p>
<h1 id="将内存中的结构flush到磁盘"><a href="#将内存中的结构flush到磁盘" class="headerlink" title="将内存中的结构flush到磁盘"></a>将内存中的结构flush到磁盘</h1><h2 id="构建kdd文件"><a href="#构建kdd文件" class="headerlink" title="构建kdd文件"></a>构建kdd文件</h2><p>flush到文件中指的是形成一个segment，触发条件有两个（同<a href="https://kkewwei.github.io/elasticsearch_learning/2019/10/29/Lucenec%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-fdt-fdx%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B/#%E5%88%B7%E5%88%B0fdx%E6%96%87%E4%BB%B6">fdx</a>，<a href="https://kkewwei.github.io/elasticsearch_learning/2019/11/15/Lucene%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-dvm-dvm%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B/#%E5%88%B7%E6%96%B0%E5%88%B0%E6%96%87%E4%BB%B6">dvm</a>、<a href="https://kkewwei.github.io/elasticsearch_learning/2020/02/28/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-tim-tip%E8%AF%8D%E5%85%B8%E7%BB%93%E6%9E%84%E5%8E%9F%E7%90%86%E7%A0%94%E7%A9%B6/#flush%E5%88%B0%E6%96%87%E4%BB%B6%E4%B8%AD">词典建立</a>一样）：<br>1.lucene建立的索引结构占用内存或者缓存文档数超过阈值。该check会在每次索引完一个文档后(详见<code>flushControl.doAfterDocument</code>)。<br>2.用户主动调用indexWriter.flush()触发。<br>刷新建立BKD树时，我们首先进入<code>PointValuesWriter.flush()</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">public void flush(SegmentWriteState state, Sorter.DocMap sortMap, PointsWriter writer) throws IOException &#123;</span><br><span class="line">  // 封装了读取point&amp;文档的方法</span><br><span class="line">  PointValues points = new MutablePointValues() &#123;</span><br><span class="line">    // 给每个point都编了号。若之后对该域每个point进行排序，也仅仅是对这个号排序。</span><br><span class="line">    final int[] ords = new int[numPoints]; </span><br><span class="line">    &#123;</span><br><span class="line">      for (int i = 0; i &lt; numPoints; ++i) &#123;</span><br><span class="line">        ords[i] = i; </span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // ords存放的是排序后的point编号，通过docIDs来查找真正的docId</span><br><span class="line">    @Override</span><br><span class="line">    public int getDocID(int i) &#123;  return docIDs[ords[i]]; &#125;</span><br><span class="line">   // 读取第i个point的值</span><br><span class="line">    @Override</span><br><span class="line">    public void getValue(int i, BytesRef packedValue) &#123;</span><br><span class="line">      // 这个数据的偏移量</span><br><span class="line">      final long offset = (long) packedBytesLength * ords[i]; </span><br><span class="line">      packedValue.length = packedBytesLength;</span><br><span class="line">      bytes.setRawBytesRef(packedValue, offset);</span><br><span class="line">    &#125;</span><br><span class="line">   // 第i个point的第k位</span><br><span class="line">    @Override</span><br><span class="line">    public byte getByteAt(int i, int k) &#123;</span><br><span class="line">      final long offset = (long) packedBytesLength * ords[i] + k;</span><br><span class="line">       // 从BytePool中读取offset</span><br><span class="line">      return bytes.readByte(offset);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;;</span><br><span class="line">  final PointValues values = points; </span><br><span class="line">  writer.writeField(fieldInfo, reader); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里用MutablePointValues封装了读取每个point的方法，需要进入Lucene86PointsWriter.writeField看下。我们需要知道，Lucene86PointsWriter定义了BKD每个叶子存放的point不能超过512个（BKDWriter.DEFAULT_MAX_POINTS_IN_LEAF_NODE），大小不能超过16MB（BKDWriter.DEFAULT_MAX_MB_SORT_IN_HEAP）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public void writeField(FieldInfo fieldInfo, PointsReader reader) throws IOException &#123;</span><br><span class="line">  PointValues values = reader.getValues(fieldInfo.name);</span><br><span class="line">  try (BKDWriter writer = new BKDWriter(writeState.segmentInfo.maxDoc(),</span><br><span class="line">                                        writeState.directory,</span><br><span class="line">                                        writeState.segmentInfo.name,</span><br><span class="line">                                        fieldInfo.getPointDimensionCount(),</span><br><span class="line">                                        fieldInfo.getPointIndexDimensionCount(),</span><br><span class="line">                                        fieldInfo.getPointNumBytes(),</span><br><span class="line">                                        maxPointsInLeafNode,</span><br><span class="line">                                        maxMBSortInHeap,</span><br><span class="line">                                        values.size())) &#123;</span><br><span class="line"></span><br><span class="line">    if (values instanceof MutablePointValues) &#123;</span><br><span class="line">      Runnable finalizer = writer.writeField(metaOut, indexOut, dataOut, fieldInfo.name, (MutablePointValues) values);</span><br><span class="line">      if (finalizer != null) &#123;</span><br><span class="line">        metaOut.writeInt(fieldInfo.number);</span><br><span class="line">        finalizer.run();</span><br><span class="line">      &#125;</span><br><span class="line">      return;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>BKDWriter函数就是构建BKD数的核心类， 需要继续进入BKDWriter.writeField-&gt;writeFieldNDims看如何构建的，我们以point为2+维进行介绍，一维的是其简化版：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">  private Runnable writeFieldNDims(IndexOutput metaOut, IndexOutput indexOut, IndexOutput dataOut, String fieldName, MutablePointValues values) throws IOException &#123;</span><br><span class="line">    finished = true;</span><br><span class="line"></span><br><span class="line">    pointCount = values.size();</span><br><span class="line">    // 统计多少个叶子节点，一个叶子节点存放512个point</span><br><span class="line">    final int numLeaves = Math.toIntExact((pointCount + maxPointsInLeafNode - 1) / maxPointsInLeafNode);</span><br><span class="line">    final int numSplits = numLeaves - 1;</span><br><span class="line"></span><br><span class="line">    // 第1位放置当前级的Dim，第2部分防止分割时候的当前值。记录了每个NodeId都是从哪离开始切割的。完全二叉树，1，2，3</span><br><span class="line">    final byte[] splitPackedValues = new byte[numSplits * bytesPerDim];</span><br><span class="line">    final byte[] splitDimensionValues = new byte[numSplits];</span><br><span class="line">    // 每个叶子在kdd中开始存放的位置</span><br><span class="line">    final long[] leafBlockFPs = new long[numLeaves]; </span><br><span class="line">    // 获取每个维度的最大值与最小值</span><br><span class="line">    // compute the min/max for this slice</span><br><span class="line">    computePackedValueBounds(values, 0, Math.toIntExact(pointCount), minPackedValue, maxPackedValue, scratchBytesRef1);</span><br><span class="line">    for (int i = 0; i &lt; Math.toIntExact(pointCount); ++i) &#123;</span><br><span class="line">      docsSeen.set(values.getDocID(i));</span><br><span class="line">    &#125;</span><br><span class="line">    // 开始构造BKD树</span><br><span class="line">    final long dataStartFP = dataOut.getFilePointer();</span><br><span class="line">    // 将统计每个维度拆分的次数，若存在某个维度切分次数不足最大的一半，那么本次将选择这个维度切分，以便尽量避免每个维度拆分次数差距过大，而导致查询毛刺</span><br><span class="line">    final int[] parentSplits = new int[numIndexDims]; </span><br><span class="line">    build(0, numLeaves, values, 0, Math.toIntExact(pointCount), dataOut,</span><br><span class="line">            minPackedValue.clone(), maxPackedValue.clone(), parentSplits,</span><br><span class="line">            splitPackedValues, splitDimensionValues, leafBlockFPs,</span><br><span class="line">            new int[maxPointsInLeafNode]);</span><br><span class="line"></span><br><span class="line">    scratchBytesRef1.length = bytesPerDim;</span><br><span class="line">    scratchBytesRef1.bytes = splitPackedValues;</span><br><span class="line">    BKDTreeLeafNodes leafNodes  = new BKDTreeLeafNodes() &#123;</span><br><span class="line">      @Override</span><br><span class="line">      public long getLeafLP(int index) &#123;</span><br><span class="line">        return leafBlockFPs[index];</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      @Override</span><br><span class="line">      public BytesRef getSplitValue(int index) &#123;</span><br><span class="line">        scratchBytesRef1.offset = index * bytesPerDim;</span><br><span class="line">        return scratchBytesRef1;</span><br><span class="line">      &#125;</span><br><span class="line">      @Override</span><br><span class="line">      public int getSplitDimension(int index) &#123;</span><br><span class="line">        return splitDimensionValues[index] &amp; 0xff;</span><br><span class="line">      &#125;</span><br><span class="line">      @Override</span><br><span class="line">      public int numLeaves() &#123;</span><br><span class="line">        return leafBlockFPs.length;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">    return () -&gt; &#123;</span><br><span class="line">      try &#123; // metaOut:kdm文件   indexOut:kdi文件</span><br><span class="line">        writeIndex(metaOut, indexOut, maxPointsInLeafNode, leafNodes, dataStartFP); // 写dim文件</span><br><span class="line">      &#125; catch (IOException e) &#123;</span><br><span class="line">        throw new UncheckedIOException(e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情：<br>1.计算了该BDK的叶子数<br>2.首先统计每个维度的最大值&amp;最小值， 以便决定是从哪个维度开始切分排序<br>3.进入<code>BKDWriter.build()</code>函数开始递归构建每个维度。原则上，对于当前所有point，首先按照512个point为1个节点，放入完全二叉树的叶子中。按照从跟节点从上向下切分所有的叶子节点，切分前查找每个维度的最大最小差值，以这个维度将切分的左右子树保持局部有序。<br>4.将构建好的BKD树元数据存放在kdi和kdm文件中.</p>
<p><code>BKDWriter.build</code>将切分过程&amp;局部有序分成两个阶段：<br>1.切分到叶子节点后也保证叶子内某个维度有序。<br>2.当没切分到叶子节点时，保证左右子树局部有序。</p>
<h3 id="当不是叶子节点时，需要split，保证某一维度有序"><a href="#当不是叶子节点时，需要split，保证某一维度有序" class="headerlink" title="当不是叶子节点时，需要split，保证某一维度有序"></a>当不是叶子节点时，需要split，保证某一维度有序</h3><p>这里看下如何还不是叶子节点时的切分过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">  final int splitDim;</span><br><span class="line">  // compute the split dimension and partition around it</span><br><span class="line">  // 只有一个维度</span><br><span class="line">  if (numIndexDims == 1) &#123; </span><br><span class="line">    splitDim = 0;</span><br><span class="line">    // 至少两个维度的数据</span><br><span class="line">  &#125; else &#123; </span><br><span class="line">    // for dimensions &gt; 2 we recompute the bounds for the current inner node to help the algorithm choose best</span><br><span class="line">    // split dimensions. Because it is an expensive operation, the frequency we recompute the bounds is given</span><br><span class="line">    // by SPLITS_BEFORE_EXACT_BOUNDS.</span><br><span class="line">     // 大于2个维度的话，为了最合适的拆分，需要重新找下最大值和最小值</span><br><span class="line">    if (numLeaves != leafBlockFPs.length &amp;&amp; numIndexDims &gt; 2 &amp;&amp; Arrays.stream(parentSplits).sum() % SPLITS_BEFORE_EXACT_BOUNDS == 0) &#123;</span><br><span class="line">      computePackedValueBounds(reader, from, to, minPackedValue, maxPackedValue, scratchBytesRef1);</span><br><span class="line">    &#125;</span><br><span class="line">    // 查找每个维度最大值和最小值差值的最大的那个维度，决定以这个维度开始拆分</span><br><span class="line">    splitDim = split(minPackedValue, maxPackedValue, parentSplits);</span><br><span class="line">  &#125;</span><br><span class="line">  // 左子树叶子节点</span><br><span class="line">  // How many leaves will be in the left tree:</span><br><span class="line">  int numLeftLeafNodes = getNumLeftLeafNodes(numLeaves);</span><br><span class="line">  // How many points will be in the left tree:</span><br><span class="line">  final int mid = from + numLeftLeafNodes * maxPointsInLeafNode; // 那么重点节点的index编号</span><br><span class="line">  // 确定最大值和最小值相同的前缀长度</span><br><span class="line">  int commonPrefixLen = FutureArrays.mismatch(minPackedValue, splitDim * bytesPerDim,</span><br><span class="line">          splitDim * bytesPerDim + bytesPerDim, maxPackedValue, splitDim * bytesPerDim,</span><br><span class="line">          splitDim * bytesPerDim + bytesPerDim);</span><br><span class="line">  if (commonPrefixLen == -1) &#123;</span><br><span class="line">    commonPrefixLen = bytesPerDim;</span><br><span class="line">  &#125;</span><br><span class="line">  // 通过基数排序+快排实现了排序，保证中间数左右有序</span><br><span class="line">  MutablePointsReaderUtils.partition(numDataDims, numIndexDims, maxDoc, splitDim, bytesPerDim, commonPrefixLen,</span><br><span class="line">          reader, from, to, mid, scratchBytesRef1, scratchBytesRef2);</span><br><span class="line"></span><br><span class="line">  final int rightOffset = leavesOffset + numLeftLeafNodes;</span><br><span class="line">   // 拆分时那个节点的偏移量</span><br><span class="line">  final int splitOffset = rightOffset - 1;</span><br><span class="line">  // set the split value</span><br><span class="line">  final int address = splitOffset * bytesPerDim;</span><br><span class="line">  // 以哪个节点哪个维度开始切分</span><br><span class="line">  splitDimensionValues[splitOffset] = (byte) splitDim;</span><br><span class="line"> </span><br><span class="line">  reader.getValue(mid, scratchBytesRef1);</span><br><span class="line">  System.arraycopy(scratchBytesRef1.bytes, scratchBytesRef1.offset + splitDim * bytesPerDim, splitPackedValues, address, bytesPerDim);</span><br><span class="line"></span><br><span class="line">  byte[] minSplitPackedValue = ArrayUtil.copyOfSubArray(minPackedValue, 0, packedIndexBytesLength); //从minPackedValue中copy一份最小值</span><br><span class="line">  byte[] maxSplitPackedValue = ArrayUtil.copyOfSubArray(maxPackedValue, 0, packedIndexBytesLength); //从maxPackedValue中copy一份最大值</span><br><span class="line">   //重新定义左子树的最小值</span><br><span class="line">  System.arraycopy(scratchBytesRef1.bytes, scratchBytesRef1.offset + splitDim * bytesPerDim,</span><br><span class="line">          minSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);</span><br><span class="line">  System.arraycopy(scratchBytesRef1.bytes, scratchBytesRef1.offset + splitDim * bytesPerDim,</span><br><span class="line">          maxSplitPackedValue, splitDim * bytesPerDim, bytesPerDim);</span><br><span class="line"></span><br><span class="line">  // recurse</span><br><span class="line">  // 统计哪个维度被切分了</span><br><span class="line">  parentSplits[splitDim]++; </span><br><span class="line">  // 左中</span><br><span class="line">  build(leavesOffset, numLeftLeafNodes, reader, from, mid, out,</span><br><span class="line">          minPackedValue, maxSplitPackedValue, parentSplits,</span><br><span class="line">          splitPackedValues, splitDimensionValues, leafBlockFPs, spareDocIds);</span><br><span class="line">  // 中又</span><br><span class="line">  build(rightOffset, numLeaves - numLeftLeafNodes, reader, mid, to, out,</span><br><span class="line">          minSplitPackedValue, maxPackedValue, parentSplits,</span><br><span class="line">          splitPackedValues, splitDimensionValues, leafBlockFPs, spareDocIds);</span><br><span class="line">  parentSplits[splitDim]--;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>根据当前数据进行排序切分，主要做了如下事情：<br>1.判断是否需要重新(精确)统计from-&gt;to所有维度最大值、最小值，仅当父类节点在当前维度每切分4次（SPLITS_BEFORE_EXACT_BOUNDS）才统计，因为精确统计所有point的边界是一个非常昂贵的操作。<br>2.在<code>BKDWriter.split()</code>根据最大值最小值边界差距最大的那个维度确定以哪个维度排序。同时为了考虑每个维度被选中切分排序的次数不能差距太大，规定了，每个维度切分排序次数不能相差2倍，若切分次数太少了，会强制选择切分次数最小的那个维度。<br>3.通过构建完全二叉树来计算左子树、右子树分别有多少个叶子节点。<br>4.确定在被切分的维度上，最大值和最小值有多少个相同的前缀，以便压缩存储。<br>5.使用<code>MutablePointsReaderUtils.partition</code>来进行排序。保证当前维度下，左子树的值小于右子树。<br>6.重新恢复左子树的所有维度的最大值：<code>maxSplitPackedValue</code>和右子树的所有维度的最小值：<code>minSplitPackedValue</code><br>7.分别递归左子树和右子树，再继续查找分隔维度并在此维度上进行排序。最终形成了每个子树只在一个维度保证了左右有序。切分过程如下：<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/bkd1.png" height="350" width="350"></p>
<p>我们再看下<code>MutablePointsReaderUtils.partition</code>如何在splitDim维度上保证左边的数值小于等于mid对应的值、右边的数大于等于mid对应的值。这里其实使用了堆排&amp;快排的思想完成的排序。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">public static void partition(int numDataDim, int numIndexDim, int maxDoc, int splitDim, int bytesPerDim, int commonPrefixLen,</span><br><span class="line">                             MutablePointValues reader, int from, int to, int mid,</span><br><span class="line">                             BytesRef scratch1, BytesRef scratch2) &#123;</span><br><span class="line">  // 这个point内的偏移量：该维度</span><br><span class="line">  final int dimOffset = splitDim * bytesPerDim + commonPrefixLen; </span><br><span class="line">  // 需要比较的位数</span><br><span class="line">  final int dimCmpBytes = bytesPerDim - commonPrefixLen; </span><br><span class="line">  // 整个point的数据结尾位置</span><br><span class="line">  final int dataOffset = numIndexDim * bytesPerDim; </span><br><span class="line">  // 该point该point不相同的个数</span><br><span class="line">  final int dataCmpBytes = (numDataDim - numIndexDim) * bytesPerDim + dimCmpBytes; </span><br><span class="line">  final int bitsPerDocId = PackedInts.bitsRequired(maxDoc - 1);</span><br><span class="line">   //  这里位数为两类，可以从byteAt()看出，读取每一类的方式也不一样</span><br><span class="line">  new RadixSelector(dataCmpBytes + (bitsPerDocId + 7) / 8) &#123;</span><br><span class="line">  // 第一类就是普通的dimCmpBytes，读取的是不相同的字符；第二类是 (bitsPerDocId + 7) / 8， 读取的是文档Id，就是说把文档Id作为了桶的一部分</span><br><span class="line">    @Override</span><br><span class="line">    // 使用快排进行排序</span><br><span class="line">    protected Selector getFallbackSelector(int k) &#123; </span><br><span class="line">      final int dataStart = (k &lt; dimCmpBytes) ? dataOffset : dataOffset + k - dimCmpBytes;</span><br><span class="line">      final int dataEnd = numDataDim * bytesPerDim;</span><br><span class="line">      return new IntroSelector() &#123;</span><br><span class="line"></span><br><span class="line">        final BytesRef pivot = scratch1;</span><br><span class="line">        int pivotDoc;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">        protected void swap(int i, int j) &#123;</span><br><span class="line">          reader.swap(i, j);</span><br><span class="line">        &#125;</span><br><span class="line">        @Override</span><br><span class="line">        protected void setPivot(int i) &#123;</span><br><span class="line">          reader.getValue(i, pivot);</span><br><span class="line">          pivotDoc = reader.getDocID(i);</span><br><span class="line">        &#125;</span><br><span class="line">        @Override</span><br><span class="line">        protected int comparePivot(int j) &#123;</span><br><span class="line">          if (k &lt; dimCmpBytes) &#123;</span><br><span class="line">            reader.getValue(j, scratch2);</span><br><span class="line">            int cmp = FutureArrays.compareUnsigned(pivot.bytes, pivot.offset + dimOffset + k, pivot.offset + dimOffset + dimCmpBytes,</span><br><span class="line">                scratch2.bytes, scratch2.offset + dimOffset + k, scratch2.offset + dimOffset + dimCmpBytes);</span><br><span class="line">            if (cmp != 0) &#123;</span><br><span class="line">              return cmp;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          if (k &lt; dataCmpBytes) &#123;</span><br><span class="line">            reader.getValue(j, scratch2);</span><br><span class="line">            int cmp = FutureArrays.compareUnsigned(pivot.bytes, pivot.offset + dataStart, pivot.offset + dataEnd,</span><br><span class="line">                scratch2.bytes, scratch2.offset + dataStart, scratch2.offset + dataEnd);</span><br><span class="line">            if (cmp != 0) &#123;</span><br><span class="line">              return cmp;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          // 通过文档大小相比较</span><br><span class="line">          return pivotDoc - reader.getDocID(j); </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    protected void swap(int i, int j) &#123;</span><br><span class="line">      reader.swap(i, j);</span><br><span class="line">    &#125;</span><br><span class="line">    // 可以从maxLength=dataCmpBytes + (bitsPerDocId + 7) / 8可以看出，属于不同的读法，</span><br><span class="line">    @Override</span><br><span class="line">     // 第i个point，第k个byte</span><br><span class="line">    protected int byteAt(int i, int k) &#123;</span><br><span class="line">    // 读取的是dataCmpBytes中的数据</span><br><span class="line">      if (k &lt; dimCmpBytes) &#123; </span><br><span class="line">        return Byte.toUnsignedInt(reader.getByteAt(i, dimOffset + k));</span><br><span class="line">      &#125; else if (k &lt; dataCmpBytes) &#123;</span><br><span class="line">        return Byte.toUnsignedInt(reader.getByteAt(i, dataOffset + k - dimCmpBytes));</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">       // 读取的是docId的高位，通过(k - dataCmpBytes)去掉原本影响</span><br><span class="line">        final int shift = bitsPerDocId - ((k - dataCmpBytes + 1) &lt;&lt; 3);</span><br><span class="line">        // 应该仅仅是为了hash，从docId中取值 </span><br><span class="line">        return (reader.getDocID(i) &gt;&gt;&gt; Math.max(0, shift)) &amp; 0xff; </span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;.select(from, to, mid);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>排序逻辑我们需要注意两个地方：<br>1.每个point当前切分维度splitDim的逻辑位数：dataCmpBytes + (bitsPerDocId + 7) &#x2F; 8。我们当使用基数排序时，会确定每个point的位数，然后可以从高位-&gt;低位的排序。同样，这里对每个point的第splitDim维度产生虚拟位数。针对from-to的point的第splitDim维度，首先使用从不相同的byte进行排序，其次基于docId进行排序。<br>2.byteAt中定义了逻辑位数的获取方法，k表示逻辑元素的位数，当k小于该元素该维度的不同起始位时，获取具体的byte；当k超过不同位数dimCmpBytes时，则比较文档Id大小。<br>我们看下RadixSelector里最核心的排序算法部分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">private void radixSelect(int from, int to, int k, int d, int l) &#123;</span><br><span class="line">   //统计的是每一字符多少个数据</span><br><span class="line">  final int[] histogram = this.histogram;</span><br><span class="line">  // 每次使用都得清空</span><br><span class="line">  Arrays.fill(histogram, 0); </span><br><span class="line"></span><br><span class="line">  final int commonPrefixLength = computeCommonPrefixLengthAndBuildHistogram(from, to, d, histogram);</span><br><span class="line">  // 所有point中有相同的逻辑位数前缀</span><br><span class="line">  if (commonPrefixLength &gt; 0) &#123; </span><br><span class="line">    // if there are no more chars to compare or if all entries fell into the</span><br><span class="line">    // first bucket (which means strings are shorter than d) then we are done</span><br><span class="line">    // otherwise recurse</span><br><span class="line">    if (d + commonPrefixLength &lt; maxLength</span><br><span class="line">        &amp;&amp; histogram[0] &lt; to - from) &#123;</span><br><span class="line">      // 忽略过相同的前缀部分</span><br><span class="line">      radixSelect(from, to, k, d + commonPrefixLength, l); </span><br><span class="line">    &#125;</span><br><span class="line">    return;</span><br><span class="line">  &#125;</span><br><span class="line">  assert assertHistogram(commonPrefixLength, histogram);</span><br><span class="line">  // 所有point逻辑位数没有一个相同前缀</span><br><span class="line">  int bucketFrom = from;</span><br><span class="line">  // 遍历每一个桶</span><br><span class="line">  for (int bucket = 0; bucket &lt; HISTOGRAM_SIZE; ++bucket) &#123; </span><br><span class="line">    // 获取桶里面数据的范围</span><br><span class="line">    final int bucketTo = bucketFrom + histogram[bucket]; </span><br><span class="line">     // 若读取的数据个数超过中位数，</span><br><span class="line">    if (bucketTo &gt; k) &#123;</span><br><span class="line">     // 通过快排的思想，完成左边小于桶，右边大于桶</span><br><span class="line">      partition(from, to, bucket, bucketFrom, bucketTo, d);</span><br><span class="line">      // 最中间的那个桶继续排序</span><br><span class="line">      if (bucket != 0 &amp;&amp; d + 1 &lt; maxLength) &#123;</span><br><span class="line">        // all elements in bucket 0 are equal so we only need to recurse if bucket != 0</span><br><span class="line">        select(bucketFrom, bucketTo, k, d + 1, l + 1);</span><br><span class="line">      &#125;</span><br><span class="line">      return;</span><br><span class="line">    &#125;</span><br><span class="line">    bucketFrom = bucketTo;</span><br><span class="line">  &#125;</span><br><span class="line">  throw new AssertionError(&quot;Unreachable code&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情：<br>1.通过computeCommonPrefixLengthAndBuildHistogram统计第splitDim维度逻辑位数第d位相同的前缀commonPrefixLength，并且统计每个字母出现的次数histogram。<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/bkd2.png" height="300" width="570"><br>2.若commonPrefixLength大于0，代表该维度逻辑位数为d位有相同的前缀。同时检测到还有逻辑位数没有排序完，那么直接跳到逻辑位数不同的位数继续进行排序。<br>3.否则，该维度逻辑位数为d位没有相同的前缀，那么就统计下，第k（初始化时为mid）个数逻辑位数d的值在哪个histogram维度内，然后调用<code>partition()</code>按照快排的思想将找出bucketTo-bucketFrom的值放在中间，这样，左边的值都小于中间的那组值，右边的值都大于中间的那组值。<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/bkd3.png" height="350" width="450"><br>4.继续递归，完成第k个所在那档的point所在splitDim维度逻辑为数第d+1位有序，直到这档splitDim维度逻辑完全有序。<br>此时，所有point在splitDim维度维度，形成了相对排序：以k为分隔， 第k个point左边的所有point均小于等于k，第k个pint右边的所有point均大于等于k。</p>
<h3 id="当递归到叶子节点时，需要split"><a href="#当递归到叶子节点时，需要split" class="headerlink" title="当递归到叶子节点时，需要split"></a>当递归到叶子节点时，需要split</h3><p>此时叶子节点内的point并没有按照某一个维度有序。每个叶子的处理顺序比较有序，是从第一个叶子、第二个、第三个、、、、最后一个叶子的顺序进行的。这部分主要是将叶子节所有point点如何高效的存储起来。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">// 叶子节点的起始位置</span><br><span class="line">final int count = to - from; </span><br><span class="line">//计算这批数据里面每个域相同的前缀</span><br><span class="line">// Compute common prefixes</span><br><span class="line">Arrays.fill(commonPrefixLengths, bytesPerDim);</span><br><span class="line">// 读取第from个point所有维度的值</span><br><span class="line">reader.getValue(from, scratchBytesRef1);</span><br><span class="line">//从下个开始，找相同的前缀 </span><br><span class="line">for (int i = from + 1; i &lt; to; ++i) &#123; </span><br><span class="line">  reader.getValue(i, scratchBytesRef2);</span><br><span class="line">  // 比较每个维度从from-&gt;to中相同的前缀</span><br><span class="line">  for (int dim=0;dim&lt;numDataDims;dim++) &#123; </span><br><span class="line">    final int offset = dim * bytesPerDim;</span><br><span class="line">    int dimensionPrefixLength = commonPrefixLengths[dim];</span><br><span class="line">    commonPrefixLengths[dim] = FutureArrays.mismatch(scratchBytesRef1.bytes, scratchBytesRef1.offset + offset, // 不同数据的起点</span><br><span class="line">            scratchBytesRef1.offset + offset + dimensionPrefixLength,</span><br><span class="line">            scratchBytesRef2.bytes, scratchBytesRef2.offset + offset,</span><br><span class="line">            scratchBytesRef2.offset + offset + dimensionPrefixLength);</span><br><span class="line">    if (commonPrefixLengths[dim] == -1) &#123;</span><br><span class="line">      // 两个字符串一模一样，那么不修改相同前缀长度</span><br><span class="line">      commonPrefixLengths[dim] = dimensionPrefixLength;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// Find the dimension that has the least number of unique bytes at commonPrefixLengths[dim]</span><br><span class="line">FixedBitSet[] usedBytes = new FixedBitSet[numDataDims];</span><br><span class="line">for (int dim = 0; dim &lt; numDataDims; ++dim) &#123;</span><br><span class="line">   // 所有字符不一样长</span><br><span class="line">  if (commonPrefixLengths[dim] &lt; bytesPerDim) &#123;</span><br><span class="line">   //因为最多有128个字符，这里用256位就满足了.只有不一样的才会被赋值</span><br><span class="line">    usedBytes[dim] = new FixedBitSet(256); </span><br><span class="line">  &#125;</span><br><span class="line">&#125; // 统计不一样的那个维度，去重之后可以分为多少个字符</span><br><span class="line">for (int i = from + 1; i &lt; to; ++i) &#123;</span><br><span class="line">  for (int dim=0;dim&lt;numDataDims;dim++) &#123;</span><br><span class="line">    if (usedBytes[dim] != null) &#123; // 该维度值不一样</span><br><span class="line">      byte b = reader.getByteAt(i, dim * bytesPerDim + commonPrefixLengths[dim]);</span><br><span class="line">      usedBytes[dim].set(Byte.toUnsignedInt(b));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">// 统计两个维度中distinct字母最少的那个维度</span><br><span class="line">int sortedDim = 0; </span><br><span class="line"> // distinct后的值个数</span><br><span class="line">int sortedDimCardinality = Integer.MAX_VALUE;</span><br><span class="line">for (int dim = 0; dim &lt; numDataDims; ++dim) &#123;</span><br><span class="line">  if (usedBytes[dim] != null) &#123;</span><br><span class="line">    final int cardinality = usedBytes[dim].cardinality();</span><br><span class="line">    if (cardinality &lt; sortedDimCardinality) &#123;</span><br><span class="line">      sortedDim = dim;</span><br><span class="line">      sortedDimCardinality = cardinality;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125; // 将数据以最集中的那个维度排序</span><br><span class="line">// 每个维度都有一系列数据，这系列数据在某位开始不同，统计开始不同这位有多少个distinct个数据，找到这几个维度中，distinct最小的那个维度，进行排序</span><br><span class="line">// sort by sortedDim</span><br><span class="line">MutablePointsReaderUtils.sortByDim(numDataDims, numIndexDims, sortedDim, bytesPerDim, commonPrefixLengths,</span><br><span class="line">        reader, from, to, scratchBytesRef1, scratchBytesRef2);</span><br><span class="line"></span><br><span class="line">BytesRef comparator = scratchBytesRef1;</span><br><span class="line">BytesRef collector = scratchBytesRef2;</span><br><span class="line"> // 读取排序后的第一个point，被比较的数</span><br><span class="line">reader.getValue(from, comparator);</span><br><span class="line">// 获取的是point（全维度）与后面一个point不相同的个数</span><br><span class="line">int leafCardinality = 1; </span><br><span class="line">for (int i = from + 1; i &lt; to; ++i) &#123;</span><br><span class="line">   // 读取下一个point, collector是最新的数</span><br><span class="line">  reader.getValue(i, collector); </span><br><span class="line">  // 几个维度，只有前面一个和后面有一个不相同，就leafCardinality+1</span><br><span class="line">  for (int dim =0; dim &lt; numDataDims; dim++) &#123; </span><br><span class="line">    // 从不同之处开始比较 </span><br><span class="line">    final int start = dim * bytesPerDim + commonPrefixLengths[dim];</span><br><span class="line">    final int end = dim * bytesPerDim + bytesPerDim;</span><br><span class="line">    // 如果不是完全一样</span><br><span class="line">    if (FutureArrays.mismatch(collector.bytes, collector.offset + start, collector.offset + end,</span><br><span class="line">            comparator.bytes, comparator.offset + start, comparator.offset + end) != -1) &#123;</span><br><span class="line">      // 每个value都不同</span><br><span class="line">      leafCardinality++; </span><br><span class="line">      // 在交换collector和comparator的值，是想前后比较是否一致</span><br><span class="line">      BytesRef scratch = collector;</span><br><span class="line">      collector = comparator;</span><br><span class="line">      comparator = scratch;</span><br><span class="line">      // 直接退出了, 所以交换没啥用</span><br><span class="line">      break; </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">// Save the block file pointer:</span><br><span class="line">leafBlockFPs[leavesOffset] = out.getFilePointer(); // kdd</span><br><span class="line">// Write doc IDs</span><br><span class="line">int[] docIDs = spareDocIds;</span><br><span class="line">for (int i = from; i &lt; to; ++i) &#123;</span><br><span class="line">  // 获取from-&gt;to之间的文档Id</span><br><span class="line">  docIDs[i - from] = reader.getDocID(i); </span><br><span class="line">&#125;</span><br><span class="line"> // 把文档Id给存储起来了</span><br><span class="line">writeLeafBlockDocs(scratchOut, docIDs, 0, count); </span><br><span class="line">// 存储相同的前缀</span><br><span class="line">// Write the common prefixes:</span><br><span class="line"> // copy第一个词</span><br><span class="line">reader.getValue(from, scratchBytesRef1);</span><br><span class="line">System.arraycopy(scratchBytesRef1.bytes, scratchBytesRef1.offset, scratch1, 0, packedBytesLength);</span><br><span class="line">// 存储前缀</span><br><span class="line">writeCommonPrefixes(scratchOut, commonPrefixLengths, scratch1); </span><br><span class="line">// Write the full values:</span><br><span class="line">IntFunction&lt;BytesRef&gt; packedValues = new IntFunction&lt;BytesRef&gt;() &#123;</span><br><span class="line">  @Override</span><br><span class="line">  public BytesRef apply(int i) &#123;</span><br><span class="line">    reader.getValue(from + i, scratchBytesRef1);</span><br><span class="line">    return scratchBytesRef1;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line">// 再写入叶子剩余数据</span><br><span class="line">writeLeafBlockPackedValues(scratchOut, commonPrefixLengths, count, sortedDim, packedValues, leafCardinality);</span><br><span class="line">// // 写入kdd文件</span><br><span class="line">out.writeBytes(scratchOut.getBytes(), 0, scratchOut.getPosition()); </span><br><span class="line">scratchOut.reset();</span><br></pre></td></tr></table></figure>

<p>具体做了如下事情：<br>1.遍历from-to所有point，依次比较每个point同一个维度相同的前缀长度，存放在commonPrefixLengths。<br>2.统计每个维度不相同前缀point的cardinaliy，统计时使用长度为256的FixedBitSet，代表着256个字符。<br>3.遍历每个维度，找出cardinaliy值最小的那个维度sortedDim。<br>4.基于sortedDim维度，调用<code>MutablePointsReaderUtils.sortByDim</code>使用快排原理保证叶子内所有元树在sortedDim维度有序。<br>5.统计from-to个Point的cardinaliy，这里每个维度都要对比。<br>6.存储from-to个排序后Point的docId<br>7.存储每个维度相同的前缀。<br>8.使用<code>BKDWriter.writeLeafBlockPackedValues()</code>存储from-to个具体的point。<br>kdd文件结构如下：<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/bkd5.png" height="100" width="900"></p>
<h2 id="构建kdm和kdi文件"><a href="#构建kdm和kdi文件" class="headerlink" title="构建kdm和kdi文件"></a>构建kdm和kdi文件</h2><p>当对所有point进行排序后，开始存储BKD树的每个子节点和叶子节点，会进入到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">private void writeIndex(IndexOutput metaOut, IndexOutput indexOut, int countPerLeaf, BKDTreeLeafNodes leafNodes, long dataStartFP) throws IOException &#123;</span><br><span class="line">   byte[] packedIndex = packIndex(leafNodes);</span><br><span class="line">   writeIndex(metaOut, indexOut, countPerLeaf, leafNodes.numLeaves(), packedIndex, dataStartFP);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了两件事：<br>1.在<code>packIndex</code>中压缩存储BKD树子节点和叶子节点。<br>2.在<code>writeIndex</code>中存储压缩后的数据，及BKD元数据。</p>
<h3 id="压缩转存BKD树"><a href="#压缩转存BKD树" class="headerlink" title="压缩转存BKD树"></a>压缩转存BKD树</h3><p>压缩BKD转存的核心函数是<code>recursePackIndex</code>，采用递归的方式转存，以中序遍历的方式对BKD树进行处理，首先先存储中间飞叶子的信息，然后再分别对左右叶子节点进行处理:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"> // 到了叶子节点</span><br><span class="line">if (numLeaves == 1) &#123; </span><br><span class="line">     if (isLeft) &#123;</span><br><span class="line">       return 0;</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       long delta = leafNodes.getLeafLP(leavesOffset) - minBlockFP;</span><br><span class="line">       writeBuffer.writeVLong(delta);</span><br><span class="line">       return appendBlock(writeBuffer, blocks);</span><br><span class="line">     &#125;</span><br><span class="line">     // 不是叶子节点</span><br><span class="line">   &#125; else &#123; </span><br><span class="line">     long leftBlockFP;</span><br><span class="line">     if (isLeft) &#123;</span><br><span class="line">       // 若是左子树，leftBlockFP就是父节点的minBlockFP</span><br><span class="line">       leftBlockFP = minBlockFP;</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       // 右子树的最小leftBlockFP，就是当前右子树包含的的所有叶子节点的第一个，也就是leavesOffset对应的叶子节点</span><br><span class="line">       leftBlockFP = leafNodes.getLeafLP(leavesOffset); </span><br><span class="line">       long delta = leftBlockFP - minBlockFP;</span><br><span class="line">       assert leafNodes.numLeaves() == numLeaves || delta &gt; 0 : &quot;expected delta &gt; 0; got numLeaves =&quot; + numLeaves + &quot; and delta=&quot; + delta;</span><br><span class="line">       writeBuffer.writeVLong(delta);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     int numLeftLeafNodes = getNumLeftLeafNodes(numLeaves);</span><br><span class="line">     final int rightOffset = leavesOffset + numLeftLeafNodes;</span><br><span class="line">     final int splitOffset = rightOffset - 1;</span><br><span class="line">     // 和构建时一样，是以哪个维度切分的，然后address指向下一个位置（value值）</span><br><span class="line">     int splitDim = leafNodes.getSplitDimension(splitOffset);</span><br><span class="line">     BytesRef splitValue = leafNodes.getSplitValue(splitOffset);// 这个维度切分时的值</span><br><span class="line">     int address = splitValue.offset;</span><br><span class="line"></span><br><span class="line">      // 查找切分的那个值和之前切分之间的之间相同的前缀</span><br><span class="line">     // find common prefix with last split value in this dim:</span><br><span class="line">     int prefix = FutureArrays.mismatch(splitValue.bytes, address, address + bytesPerDim, lastSplitValues,</span><br><span class="line">             splitDim * bytesPerDim, splitDim * bytesPerDim + bytesPerDim);</span><br><span class="line">     if (prefix == -1) &#123;</span><br><span class="line">       prefix = bytesPerDim;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     int firstDiffByteDelta;</span><br><span class="line">     if (prefix &lt; bytesPerDim) &#123; // 两次切分的值是不同的</span><br><span class="line">       firstDiffByteDelta = (splitValue.bytes[address+prefix]&amp;0xFF) - (lastSplitValues[splitDim * bytesPerDim + prefix]&amp;0xFF);</span><br><span class="line">       // 第二次作为切分阶度，那么就开始获取diff</span><br><span class="line">       if (negativeDeltas[splitDim]) &#123;</span><br><span class="line">          // 取相反数</span><br><span class="line">         firstDiffByteDelta = -firstDiffByteDelta; </span><br><span class="line">       &#125;</span><br><span class="line">       assert firstDiffByteDelta &gt; 0; </span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       firstDiffByteDelta = 0;</span><br><span class="line">     &#125;</span><br><span class="line">     // 将prefix、splitDim和firstDiffByteDelta打包编码到同一个vint中，也很容易解码出来。见BKDReader.readNodeData()中287行编码</span><br><span class="line">     // pack the prefix, splitDim and delta first diff byte into a single vInt:</span><br><span class="line">     int code = (firstDiffByteDelta * (1+bytesPerDim) + prefix) * numIndexDims + splitDim;</span><br><span class="line">     writeBuffer.writeVInt(code);</span><br><span class="line"></span><br><span class="line">     // write the split value, prefix coded vs. our parent&apos;s split value:</span><br><span class="line">     int suffix = bytesPerDim - prefix;</span><br><span class="line">     byte[] savSplitValue = new byte[suffix];</span><br><span class="line">     if (suffix &gt; 1) &#123;// 不完全一样</span><br><span class="line">       // 把这个split分词的那个词后半段存储起来</span><br><span class="line">       writeBuffer.writeBytes(splitValue.bytes, address+prefix+1, suffix-1);</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     byte[] cmp = lastSplitValues.clone(); // 不再是同一个对象</span><br><span class="line">     // 把lastSplitValues中的不相同的后缀全部copy到savSplitValue中</span><br><span class="line">     System.arraycopy(lastSplitValues, splitDim * bytesPerDim + prefix, savSplitValue, 0, suffix); // 临时保存</span><br><span class="line">     // 将splitPackedValues中该不相同的后缀copy，放到lastSplitValues对应的位置</span><br><span class="line">     // copy our split value into lastSplitValues for our children to prefix-code against</span><br><span class="line">     System.arraycopy(splitValue.bytes, address+prefix, lastSplitValues, splitDim * bytesPerDim + prefix, suffix);</span><br><span class="line">     // 将writeBuffer存储的值，以[]byte方式放入blocks中，放的code，词的后半缀</span><br><span class="line">     int numBytes = appendBlock(writeBuffer, blocks); </span><br><span class="line"></span><br><span class="line">     // placeholder for left-tree numBytes; we need this so that at search time if we only need to recurse into the right sub-tree we can</span><br><span class="line">     // quickly seek to its starting point</span><br><span class="line">     int idxSav = blocks.size();</span><br><span class="line">     // 占位符，后面会赋值，会记录左子树的长度</span><br><span class="line">     blocks.add(null); </span><br><span class="line">     // 若我们以splitDim维度进行了切分，那么之后再次在维度</span><br><span class="line">     boolean savNegativeDelta = negativeDeltas[splitDim];</span><br><span class="line">     negativeDeltas[splitDim] = true;</span><br><span class="line">     int leftNumBytes = recursePackIndex(writeBuffer, leafNodes, leftBlockFP, blocks, lastSplitValues, negativeDeltas, true,</span><br><span class="line">             leavesOffset, numLeftLeafNodes);</span><br><span class="line">     if (numLeftLeafNodes != 1) &#123;</span><br><span class="line">       writeBuffer.writeVInt(leftNumBytes);</span><br><span class="line">     &#125; else &#123; // 最左边的那个叶子节点</span><br><span class="line">       assert leftNumBytes == 0: &quot;leftNumBytes=&quot; + leftNumBytes;</span><br><span class="line">     &#125;</span><br><span class="line">      // 存储的leftNumBytes的长度</span><br><span class="line">     int numBytes2 = Math.toIntExact(writeBuffer.getFilePointer());</span><br><span class="line">     byte[] bytes2 = new byte[numBytes2];</span><br><span class="line">     writeBuffer.writeTo(bytes2, 0);</span><br><span class="line">     writeBuffer.reset();</span><br><span class="line">     // replace our placeholder:</span><br><span class="line">     blocks.set(idxSav, bytes2);</span><br><span class="line"></span><br><span class="line">     negativeDeltas[splitDim] = false;// 置位</span><br><span class="line">     int rightNumBytes = recursePackIndex(writeBuffer,  leafNodes, leftBlockFP, blocks, lastSplitValues, negativeDeltas, false,</span><br><span class="line">             rightOffset, numLeaves - numLeftLeafNodes);</span><br><span class="line">     // 这里会复位</span><br><span class="line">     negativeDeltas[splitDim] = savNegativeDelta; </span><br><span class="line">     // restore lastSplitValues to what caller originally passed us:</span><br><span class="line">     // 再放回去</span><br><span class="line">     System.arraycopy(savSplitValue, 0, lastSplitValues, splitDim * bytesPerDim + prefix, suffix); </span><br><span class="line">     // 当前非叶子节点存储使用的空间，分中+左右占用</span><br><span class="line">     return numBytes + bytes2.length + leftNumBytes + rightNumBytes;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>按照中序遍历的方式存储，比如处理node1节点:<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/bkd4.png" height="550" width="570"><br>其中：<br>deltaFP：leftBlockFP - minBlockFP，minBlockFP是父节点最左边的子节点，leftBlockFP是该节点的子节点。<br>code: (firstDiffByteDelta * (1+bytesPerDim) + prefix) * numIndexDims + splitDim，firstDiffByteDelta是当前节点切分维度的value-该相同维度上一个父节点切分的value。这里采用了编码，使之存储三个数值。</p>
<p>最终，BKD树存储在了数组blocks中。</p>
<h3 id="存储bkm和bki文件"><a href="#存储bkm和bki文件" class="headerlink" title="存储bkm和bki文件"></a>存储bkm和bki文件</h3><p>在<code>BKDWriter.writeIndex</code>文件中，bki文件存储了bkd树转存后的blocks的二进制数，而bkm文件存储了BKD树的元数据信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">private void writeIndex(IndexOutput metaOut, IndexOutput indexOut, int countPerLeaf, int numLeaves, byte[] packedIndex, long dataStartFP) throws IOException &#123;</span><br><span class="line">    // metaOut: kdm文件写入</span><br><span class="line">   CodecUtil.writeHeader(metaOut, CODEC_NAME, VERSION_CURRENT);</span><br><span class="line">   metaOut.writeVInt(numDataDims);</span><br><span class="line">   metaOut.writeVInt(numIndexDims);</span><br><span class="line">    // 每个页节点的point个数</span><br><span class="line">   metaOut.writeVInt(countPerLeaf);</span><br><span class="line">   metaOut.writeVInt(bytesPerDim);</span><br><span class="line"></span><br><span class="line">   // 统计该树每个维度最大最小值</span><br><span class="line">   metaOut.writeVInt(numLeaves);</span><br><span class="line">   metaOut.writeBytes(minPackedValue, 0, packedIndexBytesLength);</span><br><span class="line">   metaOut.writeBytes(maxPackedValue, 0, packedIndexBytesLength);</span><br><span class="line"></span><br><span class="line">   metaOut.writeVLong(pointCount);</span><br><span class="line">   metaOut.writeVInt(docsSeen.cardinality());</span><br><span class="line">   // 把非叶子节点在文件中存储位置给读取出来</span><br><span class="line">   metaOut.writeVInt(packedIndex.length);</span><br><span class="line">   metaOut.writeLong(dataStartFP);</span><br><span class="line">   // If metaOut and indexOut are the same file, we account for the fact that</span><br><span class="line">   // writing a long makes the index start 8 bytes later.</span><br><span class="line">   metaOut.writeLong(indexOut.getFilePointer() + (metaOut == indexOut ? Long.BYTES : 0));</span><br><span class="line">  // kdi文件</span><br><span class="line">   indexOut.writeBytes(packedIndex, 0, packedIndex.length);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>BKD树主要运用在范围多维查找，在空间上，按照完全二叉树结构，将数据分为左右两部分，找到所有point每个维度[min,max]差距最大的维度，在该维度按照照左子树完全小于中间值，右子树完全大于间的值。通过范围查找，能够快速定位出docId。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color3">Lucene、BKW树、Point</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/elasticsearch_learning/categories/Lucene//" class="article-tag-list-link color2">Lucene</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/elasticsearch_learning/2020/11/01/Lucene8-6-2底层架构-BKW树构建过程/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-ES7-9-1-publish原理详解" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/elasticsearch_learning/2020/08/04/ES7-9-1-publish原理详解/">ES7.9.1 publish原理详解</a>
    </h1>
  

        
        <a href="/elasticsearch_learning/2020/08/04/ES7-9-1-publish原理详解/" class="archive-article-date">
  	<time datetime="2020-08-04T09:10:06.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2020-08-04</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>ES集群中，master负责实施维护集群元数据的更新，然后再分发给data节点，分发的过程就是publish，也就是本文的重点。本文将以索引创建的流程来讲解这个过程。</p>
<h1 id="master首先创建新的集群元数据"><a href="#master首先创建新的集群元数据" class="headerlink" title="master首先创建新的集群元数据"></a>master首先创建新的集群元数据</h1><p>当master接收到创建索引的请求后，首先进入如下MetadataCreateIndexService.onlyCreateIndex()函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">private void onlyCreateIndex(final CreateIndexClusterStateUpdateRequest request,</span><br><span class="line">                                final ActionListener&lt;ClusterStateUpdateResponse&gt; listener) &#123;</span><br><span class="line">       clusterService.submitStateUpdateTask(</span><br><span class="line">           &quot;create-index [&quot; + request.index() + &quot;], cause [&quot; + request.cause() + &quot;]&quot;,</span><br><span class="line">           // 每个create使用这惟一的key</span><br><span class="line">           new AckedClusterStateUpdateTask&lt;ClusterStateUpdateResponse&gt;(Priority.URGENT, request, listener) &#123; </span><br><span class="line">               protected ClusterStateUpdateResponse newResponse(boolean acknowledged) &#123;</span><br><span class="line">                   return new ClusterStateUpdateResponse(acknowledged);</span><br><span class="line">               &#125;</span><br><span class="line">               @Override</span><br><span class="line">               public ClusterState execute(ClusterState currentState) throws Exception &#123;</span><br><span class="line">                   // 产生新的ClusterState，还没成为本地的ClusterState</span><br><span class="line">                   return applyCreateIndexRequest(currentState, request, false); </span><br><span class="line">               &#125;</span><br><span class="line">           &#125;);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>

<p>master进行任何更新State的操作时，都会调用submitStateUpdateTask() -&gt; ClusterService.submitStateUpdateTasks() -&gt; MasterService.submitStateUpdateTasks() -&gt; TaskBatcher.submitTasks()，在submitTasks()中会对tasks分类合并，有些task可以合并执行以加快速度:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public void submitTasks(List&lt;? extends BatchedTask&gt; tasks, @Nullable TimeValue timeout) throws EsRejectedExecutionException &#123;</span><br><span class="line">    final BatchedTask firstTask = tasks.get(0);</span><br><span class="line">    final Map&lt;Object, BatchedTask&gt; tasksIdentity = tasks.stream().collect(Collectors.toMap( </span><br><span class="line">        BatchedTask::getTask,</span><br><span class="line">        Function.identity(),</span><br><span class="line">        (a, b) -&gt; &#123; throw new IllegalStateException(&quot;cannot add duplicate task: &quot; + a); &#125;,</span><br><span class="line">        IdentityHashMap::new));</span><br><span class="line">    //比如多个shartedShard过后来，都会先锁着。</span><br><span class="line">    synchronized (tasksPerBatchingKey) &#123; </span><br><span class="line">        LinkedHashSet&lt;BatchedTask&gt; existingTasks = tasksPerBatchingKey.computeIfAbsent(firstTask.batchingKey,</span><br><span class="line">            k -&gt; new LinkedHashSet&lt;&gt;(tasks.size()));</span><br><span class="line">        // 对应存量的</span><br><span class="line">        existingTasks.addAll(tasks);</span><br><span class="line">    &#125;</span><br><span class="line">    if (timeout != null) &#123;</span><br><span class="line">        threadExecutor.execute(firstTask, timeout, () -&gt; onTimeoutInternal(tasks, timeout)); // 这个类就是[node][masterService#updateTask][T#1]</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        threadExecutor.execute(firstTask);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，该函数会去检查task的batchingKey是否一致，若一致的话，放在相同的batchingKey下，最常见的startShard&#x2F;failShard元数据更新就是可以合并执行，batchingKey相同的前提是调用ClusterService.submitStateUpdateTask时，使用了相同的ClusterStateTaskExecutor，而startShard使用了全局唯一的ShardStartedClusterStateTaskExecutor作为key。在create中，我们明显可以看到每个index创建都会产生新的AckedClusterStateUpdateTask作为batchingKey，索引创建流程只能逐个全局同步。这里会从线程池中产生我们熟悉的[node][masterService#updateTask][T#1]线程进行构造新的ClusterStae。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">private void runTasks(TaskInputs taskInputs) &#123;</span><br><span class="line">    final ClusterState previousClusterState = state();</span><br><span class="line">    final long computationStartTime = threadPool.relativeTimeInMillis();</span><br><span class="line">     // 去执行每个task如何产生新的ClusterState</span><br><span class="line">    final TaskOutputs taskOutputs = calculateTaskOutputs(taskInputs, previousClusterState);</span><br><span class="line">    taskOutputs.notifyFailedTasks();</span><br><span class="line">    final TimeValue computationTime = getTimeSince(computationStartTime);</span><br><span class="line">    logExecutionTime(computationTime, &quot;compute cluster state update&quot;, summary);</span><br><span class="line">    if (taskOutputs.clusterStateUnchanged()) &#123;</span><br><span class="line">        ......</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        final ClusterState newClusterState = taskOutputs.newClusterState;</span><br><span class="line">        final long publicationStartTime = threadPool.relativeTimeInMillis();</span><br><span class="line">        try &#123;</span><br><span class="line">            ClusterChangedEvent clusterChangedEvent = new ClusterChangedEvent(summary, newClusterState, previousClusterState);</span><br><span class="line">            // 这里回去进行真正广播</span><br><span class="line">            publish(clusterChangedEvent, taskOutputs, publicationStartTime);  </span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            handleException(summary, publicationStartTime, newClusterState, e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个函数主要做了如下事情：<br>1.调用calculateTaskOutputs以产生新的ClusterState。产生过程可以参考前面自定义类的AckedClusterStateUpdateTask.execute调用applyCreateIndexRequest产生新的集群状态，这里将不是本文重点。<br>2.调用publish()进行全局广播，全局广播包括主master本身。</p>
<h1 id="master全局广播"><a href="#master全局广播" class="headerlink" title="master全局广播"></a>master全局广播</h1><h2 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h2><p>进行广播前，master还会做如下预处理：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">public void publish(ClusterChangedEvent clusterChangedEvent, ActionListener&lt;Void&gt; publishListener, AckListener ackListener) &#123;</span><br><span class="line">    final PublishRequest publishRequest = coordinationState.get().handleClientValue(clusterState);</span><br><span class="line">    final CoordinatorPublication publication = new CoordinatorPublication(publishRequest, publicationContext,</span><br><span class="line">           new ListenableFuture&lt;&gt;(), ackListener, publishListener);</span><br><span class="line">    currentPublication = Optional.of(publication);</span><br><span class="line">    // 从最新的集群状态中的所有的节点</span><br><span class="line">    final DiscoveryNodes publishNodes = publishRequest.getAcceptedState().nodes();</span><br><span class="line">    // master仅仅更新下数据节点列表。当master接收到data的心跳时会校验</span><br><span class="line">    leaderChecker.setCurrentNodes(publishNodes); </span><br><span class="line">    // master会更新本地维持的、对数据节点的心跳连接</span><br><span class="line">    followersChecker.setCurrentNodes(publishNodes);</span><br><span class="line">    // master更新本地需要维护的、数据节点响应的ClusterVersion版本号，便于剔除对版本超时未同步的节点。 </span><br><span class="line">    lagDetector.setTrackedNodes(publishNodes);</span><br><span class="line">    // 真正开始publish，需要注意的是`followersChecker.getFaultyNodes()`记录的是心跳超时重试未成功的节点，此时同步时直接置位这些节点PublicationTargetState状态为failed</span><br><span class="line">    publication.start(followersChecker.getFaultyNodes()); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在CoordinatorPublication初始化时，会针对整个广播设置cancel超时时间(cluster.publish.timeout publish，默认30s)+info超时(cluster.publish.info_timeout, 默认10s)，info超时日志如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[INFO ] after [10s] publication of cluster state version [407258] is still waiting for &#123;node2&#125;[SENT_APPLY_COMMIT], &#123;node1&#125;[SENT_APPLY_COMMIT]</span><br></pre></td></tr></table></figure>

<p>info超时会打印master还没接收到的commit响应的所有节点。cancel超时后，会将整个publish过程置为 cancelled+isCompleted。对于还未完成第二次响应的节点，直接置为失败，master会直接进入整个publish收尾阶段。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public void cancel(String reason) &#123;</span><br><span class="line">    if (isCompleted) &#123;</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    assert cancelled == false;</span><br><span class="line">    cancelled = true; // 那么会先去取消</span><br><span class="line">    // 若第一轮响应的master节点还没超过一半(那么直接置位失败)</span><br><span class="line">    if (applyCommitRequest.isPresent() == false) &#123; </span><br><span class="line">        final Exception e = new ElasticsearchException(&quot;publication cancelled before committing: &quot; + reason);</span><br><span class="line">        // 标记还未成的PublicationTarget为失败</span><br><span class="line">        publicationTargets.stream().filter(PublicationTarget::isActive).forEach(pt -&gt; pt.setFailed(e));</span><br><span class="line">    &#125;</span><br><span class="line">    onPossibleCompletion(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>onPossibleCompletion()会在后面介绍。</p>
<h2 id="向每个节点广播请求"><a href="#向每个节点广播请求" class="headerlink" title="向每个节点广播请求"></a>向每个节点广播请求</h2><p>开始对每个数据节点发送元数据请求：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">void sendPublishRequest() &#123;</span><br><span class="line">    if (isFailed()) &#123;</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line">    state = PublicationTargetState.SENT_PUBLISH_REQUEST;</span><br><span class="line">    Publication.this.sendPublishRequest(discoveryNode, publishRequest, new PublishResponseHandler());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>master在对每个目标节点(包含master节点本身)发送publish前，会分别对每个目标节点构建PublicationTarget对象，来跟踪publish state，表明当前节点的publish进行到了哪个阶段：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">enum PublicationTargetState &#123;</span><br><span class="line">    NOT_STARTED,  // 对象初始化时的状态</span><br><span class="line">    FAILED,  //该目标节点的publish失败了，比如超时，或者节点异常等</span><br><span class="line">    SENT_PUBLISH_REQUEST,  // master已经向数据节点发送了第一次的publish_request</span><br><span class="line">    WAITING_FOR_QUORUM,  // 数据节点已经响应了master,但是masater在等待第二次commit的条件</span><br><span class="line">    SENT_APPLY_COMMIT, // master已经向数据节点发送了commit请求</span><br><span class="line">    APPLIED_COMMIT,  //  master收到了数据节点对commit请求的响应</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>当对目标节点置位SENT_PUBLISH_REQUEST后，进入PublicationContext.sendPublishRequest()进行clusterState的发送。同时定义了PublishResponseHandler作为master响应目前节点第一次response的处理类。在sendPublishRequest时，ES会判断是否向目标节点发送全量ClusterState还是仅仅发送diff的ClusterState。最常见的全量发布ClusterState的情况就是有新的节点加入到集群。</p>
<h1 id="目标节点接收到maser发送的publish请求"><a href="#目标节点接收到maser发送的publish请求" class="headerlink" title="目标节点接收到maser发送的publish请求"></a>目标节点接收到maser发送的publish请求</h1><p>目标节点的PublicationTransportHandler.handleIncomingPublishRequest首先接收到master发送的请求，做了以下三件事情：<br>1.然后第一步就是解析出最新的ClusterState。<br>2.其次进入acceptState()-&gt;CoordinationState.handlePublishRequest()构建响应master的response。<br>3.再次调用becomeFollower()变身Follow（非master本身）。<br>我们看下第二步构建Response时做了哪些事情：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">public PublishResponse handlePublishRequest(PublishRequest publishRequest) &#123;</span><br><span class="line">    final ClusterState clusterState = publishRequest.getAcceptedState();</span><br><span class="line">    persistedState.setLastAcceptedState(clusterState);</span><br><span class="line">    return new PublishResponse(clusterState.term(), clusterState.version());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情：将接受到的集群元数据进行落盘。针对不同角色的节点，persistedState也不同：<br>1.目标节点为masters属性的角色时，persistedState&#x3D;LucenePersistedState。<br>2.目标节点为仅仅为data属性的角色时，persistedState&#x3D;AsyncLucenePersistedState。<br>这里还有lastSeenClusterState元数据，它的作用仅仅值是为了第二次接受到commit请求时做版本等校验用的。并不会作为接收到的临时元数据使用。<br>这里字面上可以知道：对master节点，对于ClusterState落盘时同步操作，若IO压力大的话，对落盘相当耗时，会拖累整个集群publish耗时；对数据节点，采用异步落盘的方式，避免阻塞整个落盘导致的响应超时。我们看下如何落盘，以及落盘落了哪些信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">void writeIncrementalStateAndCommit(long currentTerm, ClusterState previousClusterState,</span><br><span class="line">                                    ClusterState clusterState) throws IOException &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">        final long startTimeMillis = relativeTimeMillisSupplier.getAsLong();</span><br><span class="line">         // 进行lucene.flush()构建segment</span><br><span class="line">        final WriterStats stats = updateMetadata(previousClusterState.metadata(), clusterState.metadata());</span><br><span class="line">        // 进行lucene.flush()刷盘</span><br><span class="line">        commit(currentTerm, clusterState.version()); </span><br><span class="line">        final long durationMillis = relativeTimeMillisSupplier.getAsLong() - startTimeMillis;</span><br><span class="line">        final TimeValue finalSlowWriteLoggingThreshold = slowWriteLoggingThresholdSupplier.get();</span><br><span class="line">        // check是否超时</span><br><span class="line">        if (durationMillis &gt;= finalSlowWriteLoggingThreshold.getMillis()) &#123;</span><br><span class="line">            logger.warn(&quot;writing cluster state took [&#123;&#125;ms] which is above the warn threshold of [&#123;&#125;]; &quot; +</span><br><span class="line">                    &quot;wrote global metadata [&#123;&#125;] and metadata for [&#123;&#125;] indices and skipped [&#123;&#125;] unchanged indices&quot;,</span><br><span class="line">                durationMillis, finalSlowWriteLoggingThreshold, stats.globalMetaUpdated, stats.numIndicesUpdated,</span><br><span class="line">                stats.numIndicesUnchanged);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">        ......</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">        closeIfAnyIndexWriterHasTragedyOrIsClosed();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">private WriterStats updateMetadata(Metadata previouslyWrittenMetadata, Metadata metadata) throws IOException &#123;</span><br><span class="line">    // globalMeta是否有发生变化</span><br><span class="line">    final boolean updateGlobalMeta = Metadata.isGlobalStateEquals(previouslyWrittenMetadata, metadata) == false;</span><br><span class="line">    // 若变化了，那么全部覆盖掉</span><br><span class="line">    if (updateGlobalMeta) &#123; </span><br><span class="line">         // 重新产生gloabal Metadata</span><br><span class="line">        try (ReleasableDocument globalMetadataDocument = makeGlobalMetadataDocument(metadata)) &#123;</span><br><span class="line">             // 配置的数据盘，每个盘都会写一份</span><br><span class="line">            for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) &#123;</span><br><span class="line">                metadataIndexWriter.updateGlobalMetadata(globalMetadataDocument.getDocument());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // 获取旧的索引version</span><br><span class="line">    final Map&lt;String, Long&gt; indexMetadataVersionByUUID = new HashMap&lt;&gt;(previouslyWrittenMetadata.indices().size());</span><br><span class="line">    for (ObjectCursor&lt;IndexMetadata&gt; cursor : previouslyWrittenMetadata.indices().values()) &#123;</span><br><span class="line">        final IndexMetadata indexMetadata = cursor.value;</span><br><span class="line">        final Long previousValue = indexMetadataVersionByUUID.putIfAbsent(indexMetadata.getIndexUUID(), indexMetadata.getVersion());</span><br><span class="line">    &#125;</span><br><span class="line">    int numIndicesUpdated = 0;</span><br><span class="line">    int numIndicesUnchanged = 0;</span><br><span class="line">     // 遍历新的IndexMetadata</span><br><span class="line">    for (ObjectCursor&lt;IndexMetadata&gt; cursor : metadata.indices().values()) &#123;</span><br><span class="line">        final IndexMetadata indexMetadata = cursor.value;</span><br><span class="line">        final Long previousVersion = indexMetadataVersionByUUID.get(indexMetadata.getIndexUUID());</span><br><span class="line">        // IndexMetadata新创建的，或者发生了改变</span><br><span class="line">        if (previousVersion == null || indexMetadata.getVersion() != previousVersion) &#123; </span><br><span class="line">            logger.trace(&quot;updating metadata for [&#123;&#125;], changing version from [&#123;&#125;] to [&#123;&#125;]&quot;,</span><br><span class="line">                indexMetadata.getIndex(), previousVersion, indexMetadata.getVersion());</span><br><span class="line">            numIndicesUpdated++;</span><br><span class="line">            try (ReleasableDocument indexMetadataDocument = makeIndexMetadataDocument(indexMetadata)) &#123;</span><br><span class="line">                for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) &#123;</span><br><span class="line">                    metadataIndexWriter.updateIndexMetadataDocument(indexMetadataDocument.getDocument(), indexMetadata.getIndex());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else &#123; </span><br><span class="line">            numIndicesUnchanged++;</span><br><span class="line">        &#125;</span><br><span class="line">        indexMetadataVersionByUUID.remove(indexMetadata.getIndexUUID());</span><br><span class="line">    &#125;</span><br><span class="line">    // 存在旧的，但是没有在新的中存在了，那么就是被删除了。</span><br><span class="line">    for (String removedIndexUUID : indexMetadataVersionByUUID.keySet()) &#123;</span><br><span class="line">        for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) &#123;</span><br><span class="line">            metadataIndexWriter.deleteIndexMetadata(removedIndexUUID);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // 去flush一次</span><br><span class="line">    // Flush, to try and expose a failure (e.g. out of disk space) before committing, because we can handle a failure here more</span><br><span class="line">    // gracefully than one that occurs during the commit process.</span><br><span class="line">    for (MetadataIndexWriter metadataIndexWriter : metadataIndexWriters) &#123;</span><br><span class="line">        metadataIndexWriter.flush();</span><br><span class="line">    &#125;</span><br><span class="line">    return new WriterStats(updateGlobalMeta, numIndicesUpdated, numIndicesUnchanged);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>需要知道的是：<br>1.在global Metadate中，并没有存储所有indexMetadata，而是作为单独一项存储的。<br>2.存储的时候回遍历每个data.path分别都存储一份。所以在线上环境，我们需要严格将data和master节点区分开，以免data角色将磁盘IO占用过多，而影响元数据的同步落盘操作。<br>注意：数据节点仅仅是将集群元数据保存在了本地，并更新了，但是还没有真正合并到data节点当前使用的ClusterState中。真正将新的ClusterState当成本地元数据，是在接收到master发送的commit请求后。</p>
<h1 id="master接收到目标节点发送的publish响应"><a href="#master接收到目标节点发送的publish响应" class="headerlink" title="master接收到目标节点发送的publish响应"></a>master接收到目标节点发送的publish响应</h1><p>master接收到data响应的响应是在Publication$PublicationTarget$PublishResponseHandler.onResponse()，首先将对该节点publish请求状态置为WAITING_FOR_QUORUM，然后进入PublicationTarget.handlePublishResponse()</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">void handlePublishResponse(PublishResponse publishResponse) &#123;</span><br><span class="line">    // master属性节点响应过半后，才会赋值。接着直接第二次commit</span><br><span class="line">    if (applyCommitRequest.isPresent()) &#123; </span><br><span class="line">        sendApplyCommit();</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        // master检查是否有资格发送commit请求</span><br><span class="line">        try &#123;</span><br><span class="line">             // 响应节点过半的，继续执行</span><br><span class="line">            Publication.this.handlePublishResponse(discoveryNode, publishResponse).ifPresent(applyCommit -&gt; &#123; </span><br><span class="line">                assert applyCommitRequest.isPresent() == false;</span><br><span class="line">                applyCommitRequest = Optional.of(applyCommit);</span><br><span class="line">                ackListener.onCommit(TimeValue.timeValueMillis(currentTimeSupplier.getAsLong() - startTime)); </span><br><span class="line">                 // master对第一次响应的节点（状态为WAITING_FOR_QUORUM）开始进行第二次commit</span><br><span class="line">                publicationTargets.stream().filter(PublicationTarget::isWaitingForQuorum)</span><br><span class="line">                    .forEach(PublicationTarget::sendApplyCommit);</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            setFailed(e);</span><br><span class="line">            onPossibleCommitFailure();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情：<br>1.首先检查是否已经对某些节点发送了applyCommitRequest请求。master可以对数据节点发送applyCommitRequest是有条件的：必须有一半的master属性的节点已经响应了。（raft协议的特性）。<br>2.若master还没有发送过applyCommitRequest请求，那么会检查是否有资格可以对data节点发送第二次commit请求了。若有资格发送了，那么对所有状态为WAITING_FOR_QUORUM的节发送commit请求。</p>
<h1 id="master向目标节点发送二次commit请求"><a href="#master向目标节点发送二次commit请求" class="headerlink" title="master向目标节点发送二次commit请求"></a>master向目标节点发送二次commit请求</h1><p>master收到过半master属性的第一次response请求后，开始对WAITING_FOR_QUORUM状态的节点发送commit请求：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">void sendApplyCommit() &#123;</span><br><span class="line">    //对目标节点发送状态置为SENT_APPLY_COMMIT</span><br><span class="line">    state = PublicationTargetState.SENT_APPLY_COMMIT; </span><br><span class="line">    Publication.this.sendApplyCommit(discoveryNode, applyCommitRequest.get(), new ApplyCommitResponseHandler());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="目标节点接收到master发送的二次commit请求"><a href="#目标节点接收到master发送的二次commit请求" class="headerlink" title="目标节点接收到master发送的二次commit请求"></a>目标节点接收到master发送的二次commit请求</h1><p>目前目标节点收到master发送的commit请求后，首先进入了Coordinator.handleApplyCommit()</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">private void handleApplyCommit(ApplyCommitRequest applyCommitRequest, ActionListener&lt;Void&gt; applyListener) &#123;</span><br><span class="line">    synchronized (mutex) &#123;</span><br><span class="line">         // master节点收到本节点的commit响应</span><br><span class="line">        if (applyCommitRequest.getSourceNode().equals(getLocalNode())) &#123; </span><br><span class="line">            // master合并元数据到全局将在收到所有数据节commit响应后(具体见CoordinatorPublication.onCompletion())，将跑到transportCommitCallback</span><br><span class="line">            applyListener.onResponse(null); </span><br><span class="line">        &#125; else &#123; // 数据节点收到master发送的commit请求</span><br><span class="line">            clusterApplier.onNewClusterState(applyCommitRequest.toString(), () -&gt; applierState,</span><br><span class="line">                new ClusterApplyListener() &#123;</span><br><span class="line"></span><br><span class="line">                    @Override</span><br><span class="line">                    public void onFailure(String source, Exception e) &#123;</span><br><span class="line">                        applyListener.onFailure(e); // 将跑到PublicationTransportHandler.transportCommitCallback</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    @Override</span><br><span class="line">                    public void onSuccess(String source) &#123;</span><br><span class="line">                        applyListener.onResponse(null);// 将跑到PublicationTransportHandler.transportCommitCallback</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>针对不同角色，目标节点做了不同的反应：<br>1.若本节点就是主master节点，那么调用PublicationTransportHandler.transportCommitCallback()，作用仅仅是响应回去。 master只有在整个publish()完成后，才会将新元数据作为本地的全局元数据(后面会讲)。<br>2.若本目标节点是非主master节点，则调用ClusterApplierService.onNewClusterState将新ClusterState节点作为本节点维持的最新全局ClusterState。<br>我们看下本目标节点在替换元数据的时候哪些事情，实际进入的是ClusterApplierService.runTask()</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">private void runTask(UpdateTask task) &#123;</span><br><span class="line">    final ClusterState previousClusterState = state.get();</span><br><span class="line">    long startTimeMS = currentTimeInMillis();</span><br><span class="line">    final StopWatch stopWatch = new StopWatch();</span><br><span class="line">    final ClusterState newClusterState;</span><br><span class="line">    try &#123;</span><br><span class="line">        try (Releasable ignored = stopWatch.timing(&quot;running task [&quot; + task.source + &apos;]&apos;)) &#123;</span><br><span class="line">            // 直接获取的是最新ClusterState</span><br><span class="line">            newClusterState = task.apply(previousClusterState); </span><br><span class="line">        &#125;</span><br><span class="line">    &#125; catch (Exception e) &#123;</span><br><span class="line">        ......</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (previousClusterState == newClusterState) &#123;</span><br><span class="line">        TimeValue executionTime = TimeValue.timeValueMillis(Math.max(0, currentTimeInMillis() - startTimeMS));</span><br><span class="line">        warnAboutSlowTaskIfNeeded(executionTime, task.source, stopWatch);</span><br><span class="line">        task.listener.onSuccess(task.source);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        try &#123;// 超级重要，当集群元数据修改后，会去做一系列检查，比如创建索引等，将分配给本节点的分片状态置位started等</span><br><span class="line">            applyChanges(task, previousClusterState, newClusterState, stopWatch);</span><br><span class="line">            // 会去调用PublicationTransportHandler.transportCommitCallback()，直接响应主master</span><br><span class="line">            task.listener.onSuccess(task.source); </span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            ......</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>非主master节点主要做了如下事情：<br>1.获取的是最新ClusterState。<br>2.检查新旧ClusterState是否一致，若一致，则不做任何操作。<br>3.若旧ClusterState有变化，则调用applyChanges()，根据最新ClusterState适配本地。</p>
<p>我们再看下applyChanges()如何适配本地的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">private void applyChanges(UpdateTask task, ClusterState previousClusterState, ClusterState newClusterState, StopWatch stopWatch) &#123;</span><br><span class="line">    ClusterChangedEvent clusterChangedEvent = new ClusterChangedEvent(task.source, newClusterState, previousClusterState);</span><br><span class="line">    final DiscoveryNodes.Delta nodesDelta = clusterChangedEvent.nodesDelta();</span><br><span class="line">     // 比如节点个数发生了变化，那么就跑到这里</span><br><span class="line">    if (nodesDelta.hasChanges() &amp;&amp; logger.isInfoEnabled()) &#123;</span><br><span class="line">        String summary = nodesDelta.shortSummary();</span><br><span class="line">        if (summary.length() &gt; 0) &#123;</span><br><span class="line">            logger.info(&quot;&#123;&#125;, term: &#123;&#125;, version: &#123;&#125;, reason: &#123;&#125;&quot;,</span><br><span class="line">                summary, newClusterState.term(), newClusterState.version(), task.source);</span><br><span class="line">        &#125; // 会打印 removed &#123;&#123;、added &#123;&#123;日志</span><br><span class="line">    &#125; // 若added,那么已经认同加入集群了</span><br><span class="line"></span><br><span class="line">    try (Releasable ignored = stopWatch.timing(&quot;connecting to new nodes&quot;)) &#123;</span><br><span class="line">        connectToNodesAndWait(newClusterState);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    callClusterStateAppliers(clusterChangedEvent, stopWatch);</span><br><span class="line">    </span><br><span class="line">    nodeConnectionsService.disconnectFromNodesExcept(newClusterState.nodes());</span><br><span class="line"></span><br><span class="line">    state.set(newClusterState);</span><br><span class="line">    // 这里也比较重要，会去等待新的集群状态，然后触发某些操作（比如请求集群状态，但是此时没有maser,可见TransportMasterNodeAction$AsyncSingleAction.retry()）</span><br><span class="line">    callClusterStateListeners(clusterChangedEvent, stopWatch);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>非主master节点主要做了如下事情：<br>1.检查是否有节点掉线&amp;新增，对于新增节点主动进行connect，同时打印如下日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-10T12:12:22,781][INFO ][o.e.c.s.ClusterApplierService] [node1] added &#123;&#123;node2&#125;&#125;, term: 28, version: 483578, reason: ApplyCommitRequest&#123;term=28, version=483578, sourceNode=&#123;master&#125;&#125;</span><br></pre></td></tr></table></figure>

<p>2.本地调用callClusterStateAppliers()根据最新的ClusterState做一些操作，比如创建IndexService，删除索引数据，分配分片等操作。我们看下其中重要实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">private void callClusterStateAppliers(ClusterChangedEvent clusterChangedEvent, StopWatch stopWatch) &#123;</span><br><span class="line">        clusterStateAppliers.forEach(applier -&gt; &#123;</span><br><span class="line">            try (Releasable ignored = stopWatch.timing(&quot;running applier [&quot; + applier + &quot;]&quot;)) &#123;</span><br><span class="line">                applier.applyClusterState(clusterChangedEvent);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>clusterStateAppliers&#x3D;{highPriorityStateAppliers, normalPriorityStateAppliers, lowPriorityStateAppliers}，我们需要着重强调下highPriorityStateAppliers中的IndicesClusterStateService.applyClusterState()：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public synchronized void applyClusterState(final ClusterChangedEvent event) &#123;</span><br><span class="line">    final ClusterState state = event.state();</span><br><span class="line">    updateFailedShardsCache(state);</span><br><span class="line">    deleteIndices(event); // also deletes shards of deleted indices</span><br><span class="line">    removeIndices(event); // also removes shards of removed indices</span><br><span class="line">    failMissingShards(state);</span><br><span class="line">    // 删除被删掉的索引的shard</span><br><span class="line">    removeShards(state);   // removes any local shards that doesn&apos;t match what the master expects</span><br><span class="line">    // 本地更新本地索引元数据</span><br><span class="line">    updateIndices(event); // can also fail shards, but these are then guaranteed to be in failedShardsCache</span><br><span class="line">    //  在本地创建索引元数据</span><br><span class="line">    createIndices(state);</span><br><span class="line">    // 恢复或者创建分片</span><br><span class="line">    createOrUpdateShards(state);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>非master节点的本地维护元数据将在可以根据全局ClusterState进行及时调整。<br>3.关闭掉线节点的连接。<br>4.设置本地维护的最新全局ClusteState,存放在ClusterApplierService.state对象中。<br>5.调用callClusterStateListeners()来进行回调响应，比如数据节点需要请求master时，发现本地找不到master，那么就会创建一个listener，等待本地维护的ClusteState发生变化时，再去retry；又比如本节点是master,更新元数据后不是master，会做一些收尾处理等。</p>
<h1 id="master接收到目标节点发送二次commit响应"><a href="#master接收到目标节点发送二次commit响应" class="headerlink" title="master接收到目标节点发送二次commit响应"></a>master接收到目标节点发送二次commit响应</h1><p>master收到目标节点的二次响应后，最先进入ApplyCommitResponseHandler.onResponse()函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public void onResponse(TransportResponse.Empty ignored) &#123;</span><br><span class="line">    if (isFailed()) &#123;</span><br><span class="line">          return;</span><br><span class="line">    &#125;</span><br><span class="line">    // 修改这个确定的二次确认为已完成,</span><br><span class="line">    setAppliedCommit(); </span><br><span class="line">    // 同时检查是不是所有节点都二次响应</span><br><span class="line">    onPossibleCompletion(); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>master主要做了如下操作:<br>1.确认针对目标节点的二次响应完成：修改目标节点的publish状态为APPLIED_COMMIT；进入CoordinatorPublication构造函数的lister中更新master维护的每个数据节点最新ClusterState version。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public void onNodeAck(DiscoveryNode node, Exception e) &#123;</span><br><span class="line">    // acking and cluster state application for local node is handled specially</span><br><span class="line">    //本节点是master即为响应节点</span><br><span class="line">    if (node.equals(getLocalNode())) &#123; </span><br><span class="line">          synchronized (mutex) &#123;</span><br><span class="line">              if (e == null) &#123;</span><br><span class="line">                  // master本身第二次确认完成, 仅仅设置localNodeAckEvent为done</span><br><span class="line">                  localNodeAckEvent.onResponse(null);</span><br><span class="line">              &#125; else &#123;</span><br><span class="line">                  localNodeAckEvent.onFailure(e);</span><br><span class="line">               &#125;</span><br><span class="line">          &#125;</span><br><span class="line">    &#125; else &#123;// 响应节点为非master节点 </span><br><span class="line">          // 会跑到 AckCountDownListener.onNodeAck()里面检查是否全部全部节点ack。数据节点不会finish</span><br><span class="line">          ackListener.onNodeAck(node, e);</span><br><span class="line">          // 这里比较重要,会去更新本节点维护的数据节点的version，若version落后超时，会有惩罚机制</span><br><span class="line">          if (e == null) &#123; </span><br><span class="line">               lagDetector.setAppliedVersion(node, publishRequest.getAcceptedState().version());</span><br><span class="line">          &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对每个目标节点二次commit响应做了如下操作：<br>1.1 若目标节点就是本主master节点，那么仅标记localNodeAckEvent状态为done(后面会用)<br>1.2 若目标节点是非主master节点，则更新本主master维护的其他节点的ClusterState version（若落后严重，会主动被master剔除集群，后面会介绍)。</p>
<p>2.调用onPossibleCompletion()检查整个publish是否完成了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">private void onPossibleCompletion() &#123;</span><br><span class="line">    // 若超时30s(cluster.publish.timeout),就cancelled=true,置为失败</span><br><span class="line">    if (cancelled == false) &#123; </span><br><span class="line">        for (final PublicationTarget target : publicationTargets) &#123;  // 遍历每一个target</span><br><span class="line">             // 只要还有一个没有第二次确认完成，就退出</span><br><span class="line">            if (target.isActive()) &#123; </span><br><span class="line">                return;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; </span><br><span class="line">    // 此时1.要是cancelled=true；2.要么cancelled=false, 但是所有目标节点publish状态已经done-&gt;applyCommitRequest已经发送请求。</span><br><span class="line">    if (applyCommitRequest.isPresent() == false) &#123;</span><br><span class="line">        //还没有任何节点进行第二次commit：超时导致的失败</span><br><span class="line">        logger.debug(&quot;onPossibleCompletion: [&#123;&#125;] commit failed&quot;, this);</span><br><span class="line">        assert isCompleted == false;</span><br><span class="line">        isCompleted = true;</span><br><span class="line">        onCompletion(false)</span><br><span class="line">        return;</span><br><span class="line">    &#125;       </span><br><span class="line">    isCompleted = true;</span><br><span class="line">    //全部完成了才会去调用 这里还有大作用，会去调用CoordinatorPublication.applyClusterState()</span><br><span class="line">    onCompletion(true); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>主要做了如下检查：<br>2.1 检查cancelled是否置为失败，若未失败，且还有至少一个目标节点未完成二次commit，那么就退出等待。<br>2.2 若applyCommitRequest为空，说明是超时导致的失败，代表整个publish已经失败的完成了，会进入onCompletion()。<br>2.3 此时所有节点已经完成二次commit响应，进入onCompletion()</p>
<p>再继续看下主master调用onCompletion()做了哪些事情：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">//master节点上，所有任务已经完成（isCompleted=true），可能任务全部失败了（超时30会设置），也可能任务全部成功了</span><br><span class="line">protected void onCompletion(boolean committed) &#123; </span><br><span class="line">    // master本身完成二次确认</span><br><span class="line">    localNodeAckEvent.addListener(new ActionListener&lt;Void&gt;() &#123; </span><br><span class="line">        @Override</span><br><span class="line">        public void onResponse(Void ignore) &#123;</span><br><span class="line">            receivedJoinsProcessed = true;</span><br><span class="line">            // 也是比较重要的, master合并元数据进本身的ClusterState。数据节点合并是在收到commit请求后就合并(详见Coordinator.handleApplyCommit()函数)</span><br><span class="line">            clusterApplier.onNewClusterState(Coordinator.CoordinatorPublication.this.toString(), () -&gt; applierState, // 进去会去调用</span><br><span class="line">                    new ClusterApplier.ClusterApplyListener() &#123;</span><br><span class="line">                        @Override</span><br><span class="line">                        public void onSuccess(String source) &#123; // 本地master更新后</span><br><span class="line">                            synchronized (mutex) &#123;</span><br><span class="line">                                currentPublication = Optional.empty();</span><br><span class="line">                                // trigger term bump if new term was found during publication</span><br><span class="line">                                updateMaxTermSeen(getCurrentTerm());</span><br><span class="line"></span><br><span class="line">                                if (mode == Coordinator.Mode.LEADER) &#123;</span><br><span class="line">                                    .......</span><br><span class="line">                                &#125;</span><br><span class="line">                                // 开始对滞后的节点进行处理</span><br><span class="line">                                lagDetector.startLagDetector(publishRequest.getAcceptedState().version()); </span><br><span class="line">                                logIncompleteNodes(Level.WARN); // 超时30s的节点报警</span><br><span class="line">                            &#125;</span><br><span class="line">                            cancelTimeoutHandlers(); // 取消超时</span><br><span class="line">                            ackListener.onNodeAck(getLocalNode(), null); // 本节点也完成了</span><br><span class="line">                            publishListener.onResponse(null);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;, EsExecutors.newDirectExecutorService(), transportService.getThreadPool().getThreadContext());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>主要做了如下事情：若主master收到本节点的二次commti响应（设置localNodeAckEvent为done），那么<br>1.调用ClusterApplierService.onNewClusterState将新的ClusterState融合到本地节点中（参考<a href="https://kkewwei.github.io/elasticsearch_learning/2020/08/04/ES7-9-1-publish%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/#%E7%9B%AE%E6%A0%87%E8%8A%82%E7%82%B9%E6%8E%A5%E6%94%B6%E5%88%B0master%E5%8F%91%E9%80%81%E7%9A%84%E4%BA%8C%E6%AC%A1commit%E8%AF%B7%E6%B1%82">data融合新的全局元数据</a>）<br>2.开始针对本地维护的数据节点ClusterState version，若再超时时间外仍然低于当前同步的version，则将数据节点从集群中剔除，超时时间90s(由cluster.follower_lag.timeout参数决定)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">void checkForLag(final long version) &#123;</span><br><span class="line">    if (appliedStateTrackersByNode.get(discoveryNode) != this) &#123;</span><br><span class="line">        logger.trace(&quot;&#123;&#125; no longer active when checking version &#123;&#125;&quot;, this, version);</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    long appliedVersion = this.appliedVersion.get();</span><br><span class="line">    // 落后</span><br><span class="line">    logger.warn(</span><br><span class="line">            &quot;node [&#123;&#125;] is lagging at cluster state version [&#123;&#125;], although publication of cluster state version [&#123;&#125;] completed [&#123;&#125;] ago&quot;,</span><br><span class="line">            discoveryNode, appliedVersion, version, clusterStateApplicationTimeout);</span><br><span class="line">    onLagDetected.accept(discoveryNode); // 在 Coordinator 构造函数中。惩罚将节点脱离集群，实际调用removeNode()函数</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>会打印如下日志:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-10T14:12:24,781][WARN ][o.e.c.c.LagDetector] [master] node [node1] is lagging at cluster state version [483037], although publication of cluster state version [483038] completed [1.5m] ago</span><br></pre></td></tr></table></figure>

<p>然后直接调用Coordinator.removeNode()再次广播全局元数据。<br>3.打印publish超时未完成日志。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2020-08-10T12:12:24,781][WARN ][o.e.c.c.C.CoordinatorPublication] [master1] after [30.1s] publication of cluster state version [483038] is still waiting for &#123;node1&#125;[SENT_APPLY_COMMIT], &#123;node2&#125; [SENT_APPLY_COMMIT]</span><br></pre></td></tr></table></figure>

<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>master广播全过程分为第一次广播+第二次commit请求，只有过半master节点响应才能继续第二次广播。在30s超时时间后，主动设置publish状态为true, 在规定时间内元数据更新较慢的节点，master会主动将其剔除集群。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/elasticsearch_learning/categories/Elasticsearch//" class="article-tag-list-link color4">Elasticsearch</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/elasticsearch_learning/2020/08/04/ES7-9-1-publish原理详解/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-Lucene8-2-0底层架构-tim-tip词典结构原理研究" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/elasticsearch_learning/2020/02/28/Lucene8-2-0底层架构-tim-tip词典结构原理研究/">Lucene8.2.0底层架构-tim/tip词典结构原理研究</a>
    </h1>
  

        
        <a href="/elasticsearch_learning/2020/02/28/Lucene8-2-0底层架构-tim-tip词典结构原理研究/" class="archive-article-date">
  	<time datetime="2020-02-28T05:30:35.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2020-02-28</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Lucene中主要有两类倒排索引结构， 一种是词典结构， 涉及tim、tip、doc、pos。另外一种就是词典向量,统计的是单个词在文档内的，涉及tvd,tvm（参考<a href="https://kkewwei.github.io/elasticsearch_learning/2020/03/02/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-tvd-tvm%E8%AF%8D%E5%85%B8%E5%90%91%E9%87%8F%E7%BB%93%E6%9E%84%E7%A0%94%E7%A9%B6/">Lucene8.2.0底层架构-tvd&#x2F;tvm词典向量结构研究</a>）, 这者统计的信息大体相同，这两个类都继承自<code>TermsHashPerField</code>，都是对segment内相同域名所有文档共享这两个类。<br>前者由<code>FreqProxTermsWriterPerField</code>构建, 后者由<code>TermVectorsConsumerPerField</code>构建。两者之间结构相似，前者统计单个词在segment内所有文档当前域的词频等，后者统计单个词在当前文档当前域的词频等。先放一张图让大家对这两个对象有个大致的了解：<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_tim1.png" height="500" width="400"><br>可以看到，将两者连接起来的是bytePool（结构可参考<a href="https://kkewwei.github.io/elasticsearch_learning/2019/10/06/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-ByteBlockPool%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90/">Lucene8.2.0底层架构-ByteBlockPool结构分析</a>）,该对象就是存放的term内容，在词典和TermVector构建之间共享以节约内存使用，不过前者产生的termId在当前segment相同域内唯一的，而后者仅仅在文档该域内唯一。<br>Lucene查询中使用最多的就是词典结构, 根据term查询在哪些文档中存在, 也被称为倒排索引， 倒排索引结构如下:<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_tim2.png" height="350" width="400"><br>由图可知，只要知道termId，我们就可以很好地知道该term在每个document每个域的词频，位置，offset等信息，本文就以词典构建过程来进行深入研究。</p>
<h1 id="词典在内存中构建"><a href="#词典在内存中构建" class="headerlink" title="词典在内存中构建"></a>词典在内存中构建</h1><p>在对字典字段设置时, 可以进行如下设置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">FieldType fieldType = new FieldType();</span><br><span class="line">// 对term建立倒排索引存储的数据</span><br><span class="line">fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);</span><br><span class="line">fieldType.setTokenized(true);//分词</span><br><span class="line">fieldType.setStoreTermVectors(true);//分词</span><br><span class="line">fieldType.setOmitNorms(true);//分词</span><br><span class="line">fieldType.setStoreTermVectorOffsets(true);//分词</span><br><span class="line">fieldType.setStoreTermVectorPayloads(true);//分词</span><br><span class="line">fieldType.setStoreTermVectorPositions(true);//分词</span><br></pre></td></tr></table></figure>

<p>1.setIndexOptions的参数含义:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">// 不建立词的倒排索引</span><br><span class="line">NONE,</span><br><span class="line">// 仅仅对词的建立索引结构</span><br><span class="line">DOCS,</span><br><span class="line">// 在termVector中仅存储词频</span><br><span class="line">DOCS_AND_FREQS,</span><br><span class="line">// 在termVector中存储词频和词position</span><br><span class="line">DOCS_AND_FREQS_AND_POSITIONS,</span><br><span class="line">// 在termVector中存储词频和词position和offset</span><br><span class="line">DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS,</span><br></pre></td></tr></table></figure>

<p>这些参数将作用于词典中, doc仅仅是建立termId-&gt;doc的映射; freq还统每个域中每个单词的频率, postion统计了每个单词在每个域中的位置, offset统计了每个单词在每个域中的偏移量。<br>2.setStoreTermVector…()主要作用在TermVector, 只有当setIndexOptions设置为非NONE, 设置这些参数才有效。<br>这两类参数含义很像, 前一个作用于词典的信息统计, 词典是全局型的; 而后一个设置作用于termVector, 统计的是单个文档单个域内的。</p>
<p>我们从<code>DefaultIndexingChain.processField()</code>开始讲解， 首先检查该字段设置:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">if (fieldType.indexOptions() != IndexOptions.NONE) &#123;</span><br><span class="line">  // 每个字段只会保存一个 PerField 对象。</span><br><span class="line">  fp = getOrAddField(fieldName, fieldType, true);</span><br><span class="line">  // 这个文档中这个域不是重复写入</span><br><span class="line">  boolean first = fp.fieldGen != fieldGen;</span><br><span class="line">  // 创建倒排索引</span><br><span class="line">  fp.invert(field, first);</span><br><span class="line">  // 域是第一次写入</span><br><span class="line">  if (first) &#123;</span><br><span class="line">    // 这里才是真正存放，和fieldHash存放的是一个对象。真正统计的是当前文档所有的域。</span><br><span class="line">    fields[fieldCount++] = fp;</span><br><span class="line">    fp.fieldGen = fieldGen;</span><br><span class="line">  &#125;</span><br><span class="line">&#125; else &#123;</span><br><span class="line">  // 若我们fieldType=NONE, 那么关于存储termVector设置都是不合理的。</span><br><span class="line">  verifyUnIndexedFieldType(fieldName, fieldType);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了两件事:<br>1.检查该字段是否已经写入过。<br>2.调用<code>fp.inver</code>对该字段进行分词。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">  public void invert(IndexableField field, boolean first) throws IOException &#123; // PerFieldl里面开始</span><br><span class="line">    if (first) &#123; // 第一次该字段被写入</span><br><span class="line">      invertState.reset(); // 每次写入一个新的文档，这里都会被清空</span><br><span class="line">    &#125;</span><br><span class="line">    IndexableFieldType fieldType = field.fieldType();</span><br><span class="line"></span><br><span class="line">    final boolean analyzed = fieldType.tokenized() &amp;&amp; docState.analyzer != null; // 是否分词</span><br><span class="line">     // 对域的值进行了分词</span><br><span class="line">    try (TokenStream stream = tokenStream = field.tokenStream(docState.analyzer, tokenStream)) &#123;</span><br><span class="line">      // 针对TermVectorsConsumerPerField里面termvector参数进行设置</span><br><span class="line">      termsHashPerField.start(field, first);</span><br><span class="line">      while (stream.incrementToken()) &#123;</span><br><span class="line">        // 每个词增量为1</span><br><span class="line">        // 对position和offset进行统计。</span><br><span class="line">        invertState.position += posIncr; // 每个词新增1</span><br><span class="line">        invertState.lastPosition = invertState.position;</span><br><span class="line">        // 词的起始位置</span><br><span class="line">        int startOffset = invertState.offset + invertState.offsetAttribute.startOffset();</span><br><span class="line">        // 这个词的末尾</span><br><span class="line">        int endOffset = invertState.offset + invertState.offsetAttribute.endOffset();</span><br><span class="line">        // 该文档该域上一个词的截止为止</span><br><span class="line">        invertState.lastStartOffset = startOffset;</span><br><span class="line">        // 真正对词进行</span><br><span class="line">        termsHashPerField.add();</span><br><span class="line">      &#125;</span><br><span class="line">      stream.end();</span><br><span class="line">    &#125;</span><br><span class="line">    if (analyzed) &#123; // 若分词的话，</span><br><span class="line">      invertState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);</span><br><span class="line">      invertState.offset += docState.analyzer.getOffsetGap(fieldInfo.name);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情:<br>1.置空invertState统计, 每个文档每个域将分别统计。<br>2.统计该词的相关信息, 放入invertState<br>3.调用<code>termsHashPerField.add()</code>开始对一个词读取相应索引结构。<br>4.若分词的话, 需要对每个域值增加position和offset统计。对于域multiValue字段使用，用于设置多值数据间的间隔。<br>我们需要重点关注第三步:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">void add() throws IOException &#123;</span><br><span class="line">   // 获取当前term的termId, 此时获取的termId在semgment内该域都是唯一的</span><br><span class="line">  int termID = bytesHash.add(termAtt.getBytesRef());  </span><br><span class="line">  // 该term第一次写入</span><br><span class="line">  if (termID &gt;= 0) &#123;</span><br><span class="line">    //  intPool的当前buffer不够用就申请新的byte[]</span><br><span class="line">    if (numPostingInt + intPool.intUpto &gt; IntBlockPool.INT_BLOCK_SIZE) &#123;</span><br><span class="line">      intPool.nextBuffer();</span><br><span class="line">    &#125;</span><br><span class="line">    //  intPool的当前buffer不够用就申请新的byte[]</span><br><span class="line">    if (ByteBlockPool.BYTE_BLOCK_SIZE - bytePool.byteUpto &lt; numPostingInt*ByteBlockPool.FIRST_LEVEL_SIZE) &#123;</span><br><span class="line">      bytePool.nextBuffer();</span><br><span class="line">    &#125;</span><br><span class="line">    //此处 streamCount 为 2,表明在intPool中,一个词将占用2位,一个是指向该词的bytePool中docId&amp;freq存储位置,一个指向该词的bytePool中position&amp;offset存储位置。</span><br><span class="line">    intUptos = intPool.buffer;</span><br><span class="line">    // int当前buffer内的可分配位置</span><br><span class="line">    intUptoStart = intPool.intUpto;  </span><br><span class="line">    // 先在intPool中申请2个位置</span><br><span class="line">    intPool.intUpto += streamCount;  </span><br><span class="line">    // 第i个词在intintPool中（为了快速找到该词的int位置绝对起始起始位置</span><br><span class="line">    postingsArray.intStarts[termID] = intUptoStart + intPool.intOffset; </span><br><span class="line">   //在 bytePool 中分配两个空间,一个放 freq 信息,一个放 prox 信息的。</span><br><span class="line">    for(int i=0;i&lt;streamCount;i++) &#123;</span><br><span class="line">      // 返回的是该slice的相对起始位置，存放docId&amp;freq</span><br><span class="line">      final int upto = bytePool.newSlice(ByteBlockPool.FIRST_LEVEL_SIZE);</span><br><span class="line">       // 存放position&amp;offset</span><br><span class="line">      intUptos[intUptoStart+i] = upto + bytePool.byteOffset; </span><br><span class="line">    &#125;</span><br><span class="line">    // 把该termId的使用byteBlockPool起始位置给记录下来</span><br><span class="line">    postingsArray.byteStarts[termID] = intUptos[intUptoStart]; </span><br><span class="line">    // 开始向两个slice中存放该词的docId&amp;freq和position&amp;offset。</span><br><span class="line">    newTerm(termID);</span><br><span class="line">  &#125; else &#123; </span><br><span class="line">    // 这说明该词已经出现过一次， 返回该词的termId， 使用小技巧，将返回termId编码为负值</span><br><span class="line">    termID = (-termID)-1; </span><br><span class="line">    // 返回该词在在intPool中的起始位置</span><br><span class="line">    int intStart = postingsArray.intStarts[termID];</span><br><span class="line">    // intPool中当前使用的buffer的相对起始位置</span><br><span class="line">    intUptos = intPool.buffers[intStart &gt;&gt; IntBlockPool.INT_BLOCK_SHIFT];</span><br><span class="line">    intUptoStart = intStart &amp; IntBlockPool.INT_BLOCK_MASK;</span><br><span class="line">    // 向两个slice中追加该词的docId&amp;freq和position&amp;offset。</span><br><span class="line">    addTerm(termID);</span><br><span class="line">  &#125;</span><br><span class="line">  // 开始统计termVector需要的bytePool中docId&amp;freq和bytePool中position&amp;offset</span><br><span class="line">  if (doNextCall) &#123; </span><br><span class="line">    nextPerField.add(postingsArray.textStarts[termID]); </span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数对当前termId, 做了如下事情：<br>1.通过<code>bytesHash.add</code>确定该词的termId, termId在Segment内全局唯一。<br>2.若该term是第一次写入该segment，那么在intPool中申请2个byte，在bytePool中申请两个slice。int的两个byte作为指针，指向申请的两个slice。然后调用<code>addTerm</code>， 统计该termId的docId&amp;freq和position&amp;offset， 分别放入这两个slice中，结构将在后图展示。<br>3.若该term的termId已经存在， 那么调用<code>newTerm</code>统计信息，分别放入以上两个slice中。<br>4.调用<code>nextPerField.add</code>构建termVector的索引结构， 将在<a href="https://kkewwei.github.io/elasticsearch_learning/2020/03/02/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-tvd-tvm%E8%AF%8D%E5%85%B8%E5%90%91%E9%87%8F%E7%BB%93%E6%9E%84%E7%A0%94%E7%A9%B6/">Lucene8.2.0底层架构-tvd&#x2F;tvm词典向量结构研究</a>中重点介绍。</p>
<p>我们首先看下<code>bytesHash.add</code>是如何做到存储termValue的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">  final int length = bytes.length;</span><br><span class="line">  // 使用探针法， 直到找到当前要存储的bytes所在的有效槽位</span><br><span class="line">  final int hashPos = findHash(bytes);</span><br><span class="line">  int e = ids[hashPos]; </span><br><span class="line">  //如果为-1，则是新的term</span><br><span class="line">  if (e == -1) &#123;</span><br><span class="line">    final byte[] buffer = pool.buffer;</span><br><span class="line">    final int bufferUpto = pool.byteUpto;// 获取内存池的起始可用位置</span><br><span class="line">    count++;</span><br><span class="line">    // 记录对应termId在bytePool中的内容。freqProxPostingsArray.textStarts和bytesStart是同一个对象</span><br><span class="line">    bytesStart[e] = bufferUpto + pool.byteOffset;</span><br><span class="line">    // 在pool首先存储len(bytes),在存储值</span><br><span class="line">    if (length &lt; 128) &#123;</span><br><span class="line">      buffer[bufferUpto] = (byte) length;</span><br><span class="line">      pool.byteUpto += length + 1;</span><br><span class="line">      assert length &gt;= 0: &quot;Length must be positive: &quot; + length;</span><br><span class="line">      System.arraycopy(bytes.bytes, bytes.offset, buffer, bufferUpto + 1,</span><br><span class="line">          length);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      buffer[bufferUpto] = (byte) (0x80 | (length &amp; 0x7f));</span><br><span class="line">      buffer[bufferUpto + 1] = (byte) ((length &gt;&gt; 7) &amp; 0xff);</span><br><span class="line">      pool.byteUpto += length + 2;</span><br><span class="line">      System.arraycopy(bytes.bytes, bytes.offset, buffer, bufferUpto + 2,</span><br><span class="line">          length);</span><br><span class="line">    &#125;</span><br><span class="line">    ids[hashPos] = e; </span><br><span class="line">    return e;</span><br><span class="line">  &#125;</span><br><span class="line">  // 该term已经存在，直接返回termId</span><br><span class="line">  return -(e + 1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>本函数主要是将term存储到pool中， 存储时通过hash快速term存放的位置。</p>
<p>我们再看下<code>newTerm</code>怎么统计新产生的term：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">void newTerm(final int termID) &#123;</span><br><span class="line">  final FreqProxPostingsArray postings = freqProxPostingsArray; //</span><br><span class="line"></span><br><span class="line">  postings.lastDocIDs[termID] = docState.docID;</span><br><span class="line">  //DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS递进</span><br><span class="line">  if (!hasFreq) &#123; </span><br><span class="line">    ......</span><br><span class="line">  &#125; else &#123; </span><br><span class="line">    // 统计词频</span><br><span class="line">    postings.lastDocCodes[termID] = docState.docID &lt;&lt; 1;</span><br><span class="line">    postings.termFreqs[termID] = getTermFreq();</span><br><span class="line">    // 存储offset, position到第第二个slice中</span><br><span class="line">    if (hasProx) &#123; </span><br><span class="line">      // 向stream1中存储了proxCode</span><br><span class="line">      writeProx(termID, fieldState.position); </span><br><span class="line">      if (hasOffsets) &#123;</span><br><span class="line">        // 向stream1中存储了offserCode,</span><br><span class="line">        writeOffsets(termID, fieldState.offset);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    fieldState.maxTermFrequency = Math.max(postings.termFreqs[termID], fieldState.maxTermFrequency);</span><br><span class="line">  &#125;</span><br><span class="line">  fieldState.uniqueTermCount++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要统计了该词如下信息：<br>1.统计position和offset。<br>2.这里并没有立即统计该词的词频， 词频必须等该文档该域写完后才能统计。（只有当相同词的docId发生变化了才开始统计）</p>
<p>我们再看下如何对一个已经存在的term统计freq、position及offset：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">void addTerm(final int termID) &#123;</span><br><span class="line">  final FreqProxPostingsArray postings = freqProxPostingsArray;</span><br><span class="line">  if (!hasFreq) &#123;//不需要词频</span><br><span class="line">    ......</span><br><span class="line">  &#125; else if (docState.docID != postings.lastDocIDs[termID]) &#123; </span><br><span class="line">    // 上一个文档的该域所有term已经处理完了</span><br><span class="line">    if (1 == postings.termFreqs[termID]) &#123;  // 词频为1</span><br><span class="line">      writeVInt(0, postings.lastDocCodes[termID]|1); </span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      writeVInt(0, postings.lastDocCodes[termID]);</span><br><span class="line">      writeVInt(0, postings.termFreqs[termID]);</span><br><span class="line">    &#125;</span><br><span class="line">    //初始化当前文档的词频</span><br><span class="line">    postings.termFreqs[termID] = getTermFreq(); </span><br><span class="line">    fieldState.maxTermFrequency = Math.max(postings.termFreqs[termID], fieldState.maxTermFrequency);</span><br><span class="line">    //  这次出现文档-上次出现文档</span><br><span class="line">    postings.lastDocCodes[termID] = (docState.docID - postings.lastDocIDs[termID]) &lt;&lt; 1;</span><br><span class="line">    postings.lastDocIDs[termID] = docState.docID;</span><br><span class="line">    if (hasProx) &#123;</span><br><span class="line">      // 保存termId</span><br><span class="line">      writeProx(termID, fieldState.position);</span><br><span class="line">      if (hasOffsets) &#123;</span><br><span class="line">        // 保存offset</span><br><span class="line">        postings.lastOffsets[termID] = 0;</span><br><span class="line">        writeOffsets(termID, fieldState.offset);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    fieldState.uniqueTermCount++;</span><br><span class="line">  &#125; else &#123; </span><br><span class="line">    // 当前文档当前域的term还没有处理完</span><br><span class="line">    postings.termFreqs[termID] = Math.addExact(postings.termFreqs[termID], getTermFreq()); </span><br><span class="line">    // 增加词频</span><br><span class="line">    fieldState.maxTermFrequency = Math.max(fieldState.maxTermFrequency, postings.termFreqs[termID]);</span><br><span class="line">    if (hasProx) &#123; </span><br><span class="line">      // 继续统计post</span><br><span class="line">      writeProx(termID, fieldState.position-postings.lastPositions[termID]);</span><br><span class="line">      if (hasOffsets) &#123;</span><br><span class="line">        // 统计offser</span><br><span class="line">        writeOffsets(termID, fieldState.offset);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>此时bytePool中已经存在termId：<br>1.这里会检查该词最后一次出现的DocId是否和本文档一致 若不一直，则说明上一个文档该term已经处理完了，需要将文档DocId，词频保存带第一个slice中。可能这里有疑问，为啥该词若是第一次出现时不需要检文档Id是否发生了变化，因为该词第一次出现时，上一个文档更不存在。<br>2.将词position&amp;offset保存到第二个slice中。</p>
<p>最终建立的内存结构如下所示：<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_tim3.png" height="400" width="400"><br>freqProxPostingsArray里面数组下标就是termId, 上图展示的是termId&#x3D;1的term 倒排索引存放情况。</p>
<h1 id="flush到文件中"><a href="#flush到文件中" class="headerlink" title="flush到文件中"></a>flush到文件中</h1><p>flush到文件中指的是形成一个segment，触发条件有两个（同<a href="https://kkewwei.github.io/elasticsearch_learning/2019/10/29/Lucenec%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-fdt-fdx%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B/#%E5%88%B7%E5%88%B0fdx%E6%96%87%E4%BB%B6">fdx</a>，<a href="https://kkewwei.github.io/elasticsearch_learning/2019/11/15/Lucene%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-dvm-dvm%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B/#%E5%88%B7%E6%96%B0%E5%88%B0%E6%96%87%E4%BB%B6">dvm</a>一样）：<br>1.lucene建立的索引结构占用内存或者缓存文档书超过阈值。该check会在每次索引完一个文档后。<br>2.用户主动调用indexWriter.flush()触发。</p>
<p>两种情况最终都会跑到<code>BlockTreeTermsWriter.write</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">public void write(Fields fields, NormsProducer norms) throws IOException &#123;</span><br><span class="line">  String lastField = null;</span><br><span class="line">  // 遍历该segment内的每个域</span><br><span class="line">  for(String field : fields) &#123;</span><br><span class="line">    lastField = field;</span><br><span class="line">    Terms terms = fields.terms(field); </span><br><span class="line">    if (terms == null) &#123;</span><br><span class="line">      continue;</span><br><span class="line">    &#125;</span><br><span class="line">     // 遍历FreqProxTermsWriterPerField里面每个termId使用的，读取term的顺序前按照字符串排好序了</span><br><span class="line">    TermsEnum termsEnum = terms.iterator();</span><br><span class="line">    // 一个域单独产生一个</span><br><span class="line">    TermsWriter termsWriter = new TermsWriter(fieldInfos.fieldInfo(field)); </span><br><span class="line">    while (true) &#123;</span><br><span class="line">      // 这里会从FreqProxPostingsArray.textStarts循环遍历每一个termId。</span><br><span class="line">      BytesRef term = termsEnum.next(); </span><br><span class="line">      if (term == null) &#123;</span><br><span class="line">        break;</span><br><span class="line">      &#125;</span><br><span class="line">      // 将该term加入词典及建立字典索引结构。</span><br><span class="line">      termsWriter.write(term, termsEnum, norms); </span><br><span class="line">    &#125;</span><br><span class="line">    // 完成field 的构建。每个单词一个finish</span><br><span class="line">    termsWriter.finish();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数首先遍历该segment中每个域里所有的词，将所有词的倒排结构存放在doc文件，然后调用<code>termsWriter.write</code>建立词典结构，最后调用<code>termsWriter.finish()</code>将词典索引结构FST放入tip文件中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public void write(BytesRef text, TermsEnum termsEnum, NormsProducer norms) throws IOException &#123;</span><br><span class="line">  // 将该term的倒排索引读取出来并建立索引结构</span><br><span class="line">  BlockTermState state = postingsWriter.writeTerm(text, termsEnum, docsSeen, norms); // 针对的是一个词</span><br><span class="line">  if (state != null) &#123;</span><br><span class="line">    // 将当前词加入词典中</span><br><span class="line">    pushTerm(text); </span><br><span class="line">    PendingTerm term = new PendingTerm(text, state);</span><br><span class="line">    //当前term加入待索引列表</span><br><span class="line">    pending.add(term);</span><br><span class="line">    // 该词在多少文档中出现过</span><br><span class="line">    sumDocFreq += state.docFreq;</span><br><span class="line">    //该词总的出现频次</span><br><span class="line">    sumTotalTermFreq += state.totalTermFreq; </span><br><span class="line">    numTerms++;  //</span><br><span class="line">    if (firstPendingTerm == null) &#123;</span><br><span class="line">      // 写入的第一个词</span><br><span class="line">      firstPendingTerm = term;</span><br><span class="line">    &#125;</span><br><span class="line">    lastPendingTerm = term;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情：<br>1.调用<code>postingsWriter.writeTerm()</code>，将该term的倒排索引结构放入doc文件中。<br>2.调用<code>pushTerm()</code>将该词加入词典中。</p>
<h2 id="单个词的倒排索引结构落盘"><a href="#单个词的倒排索引结构落盘" class="headerlink" title="单个词的倒排索引结构落盘"></a>单个词的倒排索引结构落盘</h2><p>我们看下当前域的term的倒排索引结构如何放入doc文件中的，进入<code>PushPostingsWriterBase.writeTerm()</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line">public final BlockTermState writeTerm(BytesRef term, TermsEnum termsEnum, FixedBitSet docsSeen, NormsProducer norms) throws IOException &#123;</span><br><span class="line">   // 读取doc,pos等文件的起始位置</span><br><span class="line">  startTerm(normValues);</span><br><span class="line">  // 从bytePool中读取了该词的stream0,stream</span><br><span class="line">  postingsEnum = termsEnum.postings(postingsEnum, enumFlags); </span><br><span class="line">  int docFreq = 0;</span><br><span class="line">  long totalTermFreq = 0; //该文档总的词频</span><br><span class="line">  while (true) &#123; </span><br><span class="line">     // 依次读取这个docID的freq</span><br><span class="line">    int docID = postingsEnum.nextDoc();</span><br><span class="line">    if (docID == PostingsEnum.NO_MORE_DOCS) &#123;</span><br><span class="line">      break;</span><br><span class="line">    &#125;</span><br><span class="line">    // 该词在多少个文档中出现过</span><br><span class="line">    docFreq++; </span><br><span class="line">    docsSeen.set(docID); // 在这个文档中可见</span><br><span class="line">    int freq;</span><br><span class="line">    if (writeFreqs) &#123;</span><br><span class="line">      // 该文档该词的词频。已经在postingsEnum.nextDoc()中给解析出来了</span><br><span class="line">      freq = postingsEnum.freq(); </span><br><span class="line">      totalTermFreq += freq;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      freq = -1;</span><br><span class="line">    &#125; //检查读取该词的的docId，freq是否达到一个block。若达到了，则将DocId、freq压缩到doc文件中，并构建跳表结构。</span><br><span class="line">    startDoc(docID, freq); </span><br><span class="line">    if (writePositions) &#123;</span><br><span class="line">      for(int i=0;i&lt;freq;i++) &#123; // 对每个词的每个position都读取出来</span><br><span class="line">        int pos = postingsEnum.nextPosition(); // 第几个position，将startOffset和endOffset都解析出来了</span><br><span class="line">        BytesRef payload = writePayloads ? postingsEnum.getPayload() : null;</span><br><span class="line">        int startOffset;</span><br><span class="line">        int endOffset;</span><br><span class="line">        if (writeOffsets) &#123;</span><br><span class="line">          startOffset = postingsEnum.startOffset();</span><br><span class="line">          endOffset = postingsEnum.endOffset();</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          startOffset = -1;</span><br><span class="line">          endOffset = -1;</span><br><span class="line">        &#125; // 检查读取的该词position是否达到block（128个），若达到就存放到pos和pay文件中</span><br><span class="line">        addPosition(pos, payload, startOffset, endOffset); // 保存增量的freq。</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // 检查该词存储的文档个数是否达到block，写到后，更新本地保存的FilePointer，清空缓存的文档数。</span><br><span class="line">    finishDoc(); </span><br><span class="line">  &#125;</span><br><span class="line">  if (docFreq == 0) &#123; // 总文档数</span><br><span class="line">    return null;</span><br><span class="line">  &#125; else &#123; </span><br><span class="line">    // 给这个block赋予一些元数据</span><br><span class="line">    BlockTermState state = newTermState(); </span><br><span class="line">    // 存在多少个文档中</span><br><span class="line">    state.docFreq = docFreq; </span><br><span class="line">    // 该词总共出现的次数</span><br><span class="line">    state.totalTermFreq = writeFreqs ? totalTermFreq : -1; </span><br><span class="line">    // 当前term读取完了。将跳表信息填入doc</span><br><span class="line">    finishTerm(state); </span><br><span class="line">    return state;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情：<br>1.遍历当前域的该term下所有的docId,position、offset，通过<code>startDoc()</code>对每128个词建立跳表放入内存；通过<code>addPosition()</code>将每128个position构建一个block写入pos。<br>2.遍历完了该term下所有词后， 调用<code>finishTerm</code>首先将缓存doc-freq写入doc文件， 将缓存的position-offset写入pos文件， 并然后将该词的包含doc&amp;pos&amp;pay的跳表结构入doc文件。<br>我们需要关注些如何通过<code>Lucene50PostingsWriter.startDoc()</code>构建跳表结构的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line"> public void startDoc(int docID, int termDocFreq) throws IOException &#123;</span><br><span class="line">   // 已经将一批docId,freq已经被压缩到了doc文件中，作为一个block。再对这个block建立跳表点。</span><br><span class="line">   if (lastBlockDocID != -1 &amp;&amp; docBufferUpto == 0) &#123; </span><br><span class="line">     skipWriter.bufferSkip(lastBlockDocID, competitiveFreqNormAccumulator, docCount, </span><br><span class="line">         lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockPayloadByteUpto);</span><br><span class="line">   &#125;</span><br><span class="line">   final int docDelta = docID - lastDocID;</span><br><span class="line">   docDeltaBuffer[docBufferUpto] = docDelta;</span><br><span class="line">   if (writeFreqs) &#123;</span><br><span class="line">     freqBuffer[docBufferUpto] = termDocFreq; // 存储词频</span><br><span class="line">   &#125;</span><br><span class="line">   docBufferUpto++;</span><br><span class="line">   docCount++;</span><br><span class="line">   //每128个term-&gt;freq作为一个Block存储到doc中</span><br><span class="line">   if (docBufferUpto == BLOCK_SIZE) &#123; </span><br><span class="line">     forUtil.writeBlock(docDeltaBuffer, encoded, docOut); </span><br><span class="line">     if (writeFreqs) &#123;</span><br><span class="line">       // 把128个缓存的文档freq缓存到doc中</span><br><span class="line">       forUtil.writeBlock(freqBuffer, encoded, docOut); </span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情：<br>1.将该term的doc,freq保存到docDeltaBuffer、freqBuffer中<br>2.每128个docId-freq，通过<code>ForUtil.writeBlock</code>建立一个block, 放入doc文档。<br>3.针对每个block，调用<code>Lucene50SkipWriter.bufferSkip</code>建立跳表结构。</p>
<p>我们主要关注下，如何在<code>MultiLevelSkipListWriter.bufferSkip()</code>针对单个term每个block建立跳表结构的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">public void bufferSkip(int df) throws IOException &#123;</span><br><span class="line">  int numLevels = 1; </span><br><span class="line">  // 统计可以构建几级跳表</span><br><span class="line">  // 第一级别跳表针对每128个doc-freq建立一个节点</span><br><span class="line">  df /= skipInterval;  </span><br><span class="line">  // 之后第n级别级别都是128*8^(n-1)个文档建立一个跳表节点</span><br><span class="line">  while ((df % skipMultiplier) == 0 &amp;&amp; numLevels &lt; numberOfSkipLevels) &#123;</span><br><span class="line">    numLevels++;</span><br><span class="line">    df /= skipMultiplier;</span><br><span class="line">  &#125;</span><br><span class="line">  long childPointer = 0;</span><br><span class="line">  // level值从小到大，在每层 level的字节数组末尾写入skip point</span><br><span class="line">  for (int level = 0; level &lt; numLevels; level++) &#123;</span><br><span class="line">    // 将 skip point写入 skipBuffer[level]中</span><br><span class="line">    writeSkipData(level, skipBuffer[level]);</span><br><span class="line">    long newChildPointer = skipBuffer[level].getFilePointer();</span><br><span class="line">    if (level != 0) &#123; // 缓存第几级别</span><br><span class="line">      // 后一个level记录前一个level使用的缓存大小</span><br><span class="line">      skipBuffer[level].writeVLong(childPointer); </span><br><span class="line">    &#125;</span><br><span class="line">    childPointer = newChildPointer;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要是针对该term存储的每一个block（文档和freq）建立跳表结构，跳表第一级为每128（skipInterval）个block ，此后第n级跳表为每128*8^（n-1）个block。计算出了跳表需要多少级（numLevels）， 然后针对这个block在每个级别上都建立跳表结构。我们具体看下<code>Lucene50SkipWriter.writeSkipData()</code>如何构建跳表的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException &#123;</span><br><span class="line">  // 计算当前level层，docId的delta值</span><br><span class="line">  int delta = curDoc - lastSkipDoc[level];</span><br><span class="line">  skipBuffer.writeVInt(delta);</span><br><span class="line">   // 该层级上一次的文档Id</span><br><span class="line">  lastSkipDoc[level] = curDoc;</span><br><span class="line">  // 写入 doc文件的偏移量的delta值, 记录下该跳跃点在doc使用的大小</span><br><span class="line">  skipBuffer.writeVLong(curDocPointer - lastSkipDocPointer[level]);</span><br><span class="line">  // 记录当前doc占用的其实内存</span><br><span class="line">  lastSkipDocPointer[level] = curDocPointer;</span><br><span class="line">  // 向skipBuffer写入pos,doc,pay偏移量</span><br><span class="line">  if (fieldHasPositions) &#123; </span><br><span class="line">    // 记录下该跳跃点在pos使用的大小</span><br><span class="line">    skipBuffer.writeVLong(curPosPointer - lastSkipPosPointer[level]); </span><br><span class="line">    lastSkipPosPointer[level] = curPosPointer;</span><br><span class="line">    // 记录下当前缓存的文档数</span><br><span class="line">    skipBuffer.writeVInt(curPosBufferUpto);</span><br><span class="line">    if (fieldHasPayloads) &#123;</span><br><span class="line">      skipBuffer.writeVInt(curPayloadByteUpto);</span><br><span class="line">    &#125;</span><br><span class="line">    if (fieldHasOffsets || fieldHasPayloads) &#123;</span><br><span class="line">      // 记录下该跳跃点在pay使用的大小</span><br><span class="line">      skipBuffer.writeVLong(curPayPointer - lastSkipPayPointer[level]); </span><br><span class="line">      lastSkipPayPointer[level] = curPayPointer;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 向freqNormOut中写入加权系数</span><br><span class="line">  writeImpacts(competitiveFreqNorms, freqNormOut);</span><br><span class="line">  skipBuffer.writeVInt(Math.toIntExact(freqNormOut.getFilePointer()));</span><br><span class="line">  // 把freqNormOut数据向skipBuffer中写入</span><br><span class="line">  freqNormOut.writeTo(skipBuffer); </span><br><span class="line">  freqNormOut.reset();</span><br><span class="line">  competitiveFreqNorms.clear();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>每个跳表元素中主要存放了如下对象：<br>1.当前级相邻的两个跳表对应的文档差值、文档存储指针差值<br>2.pos文件跳表对应的差值.<br>3.offset&#x2F;payload文档对应的差值。<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_tim4.png" height="100" width="350"></p>
<p>doc存储的docId-freq及跳表结构如下所示：<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_tim5.png" height="300" width="800"></p>
<h2 id="每个域构建词典结构"><a href="#每个域构建词典结构" class="headerlink" title="每个域构建词典结构"></a>每个域构建词典结构</h2><p>在对单个term建立完跳表结构后，就开始将该term加入词典中，词典作为倒排索引的核心结构，通过词典，我们可以快速在doc、pos文件中找到每个词的文档ID、词频。构建词典采取了一些做法: 相似性比较高的term可以构成一个子term（专业名称是子FST结构），子term再重新再和别的term组成一个更大的FST。本节看下如何构建词典结构的，每个词都是通过<code>BlockTreeTermsWriter.pushTerm(text)</code>加入词典结构的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">private void pushTerm(BytesRef text) throws IOException &#123;</span><br><span class="line">  int limit = Math.min(lastTerm.length(), text.length); // 本term和上一个term最小值</span><br><span class="line">  int pos = 0;</span><br><span class="line">   // 计算当前term与前一个term的公共前缀长度+1, 不相同的那个</span><br><span class="line">  while (pos &lt; limit &amp;&amp; lastTerm.byteAt(pos) == text.bytes[text.offset+pos]) &#123;</span><br><span class="line">    pos++;</span><br><span class="line">  &#125;</span><br><span class="line">  // 尽量找一批term，从后向前是为了更多可能取相似性前缀</span><br><span class="line">  for(int i=lastTerm.length()-1;i&gt;=pos;i--) &#123; // last</span><br><span class="line">    // // 计算与栈顶的Entry的公共前缀为 i 的Entry的数量</span><br><span class="line">    // How many items on top of the stack share the current suffix</span><br><span class="line">    // we are closing:</span><br><span class="line">     // 当前存量与多少后缀是不同的</span><br><span class="line">    int prefixTopSize = pending.size() - prefixStarts[i];</span><br><span class="line">     // pending词的个数最少25个。</span><br><span class="line">    if (prefixTopSize &gt;= minItemsInBlock) &#123;</span><br><span class="line">      // 将最后prefixTopSize个terms产生一个fst，放入PendingBlock中，然后再放入pending中</span><br><span class="line">      writeBlocks(i+1, prefixTopSize);</span><br><span class="line">      prefixStarts[i] -= prefixTopSize-1;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  if (prefixStarts.length &lt; text.length) &#123; // prefixStarts达不到最大size的话，会不断扩容</span><br><span class="line">    prefixStarts = ArrayUtil.grow(prefixStarts, text.length);</span><br><span class="line">  &#125;</span><br><span class="line">  // 修改不同的后缀。</span><br><span class="line">  for(int i=pos;i&lt;text.length;i++) &#123; </span><br><span class="line">    prefixStarts[i] = pending.size();</span><br><span class="line">  &#125;</span><br><span class="line">  // prefixStarts[i]=count,从第i个词发生了变化，记录了变化时的长度，那么从后向前的数pending.size-count,前缀都没有发生过变化，拥有相同前缀，前缀长度为i</span><br><span class="line">  lastTerm.copyBytes(text); // 缓存上一次的term</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要是判断已经输入的哪些相似的term可以组合成一个子FST结构。因为字典term输入都是按序排好了的，很多单词相似性非常高，可以利用FST结构搜索，本节主要看如何找到这些相似的terms。我们需要了解2个关键变量：</p>
<ul>
<li>prefixStarts[i]&#x3D;count<br>存放的是term中第i个元素与前一个term该元素不一致时， pending中目前存储的term个数（pendinging中从后向前的pending.size-count个terms拥有相同前缀，相同前缀长度为i），该数组反映的是这一堆terms的相似性。我们以如下terms为例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">     当前写入顺序                   当前term写入后，prefixStarts值         pending.size() - prefixStarts[i]最大值：</span><br><span class="line">0    61 62 63                      0 0 0                               /</span><br><span class="line">1    61 62 63 64                   0 0 0 1                             1-1=0</span><br><span class="line">2    61 62 64                      0 0 2 1                             2-1=1</span><br><span class="line">3    61 62 65 66 30                0 0 3 3 3                           不满足条件，不会检测</span><br><span class="line">4    61 62 65 66 31                0 0 3 3 4                           4-4=0</span><br><span class="line">5    61 62 65 66 31 30             0 0 3 3 4 5                         不满足条件，不会检测</span><br><span class="line">6    61 62 65 66 31 30 30          0 0 3 3 4 5 6                       不满足条件，不会检测</span><br><span class="line">7    61 62 65 66 31 30 30 30       0 0 3 3 4 5 6 7                     不满足条件，不会检测</span><br><span class="line">8    61 62 65 66 31 30 30 30 30    0 0 3 3 4 5 6 7 8                   不满足条件，不会检测</span><br><span class="line">9    61 62 65 66 31 30 30 30 31    0 0 3 3 4 5 6 7 9                   9-8=1</span><br><span class="line">10   61 62 65 66 31 30 30 30 32    0 0 3 3 4 5 6 7 10                  10-9=1</span><br><span class="line">11   61 62 65 66 31 30 30 30 33    0 0 3 3 4 5 6 7 11                  11-10=1</span><br><span class="line">12   61 62 65 66 31 30 30 30 34    0 0 3 3 4 5 6 7 12                  12-11=1</span><br><span class="line">13   61 62 65 66 31 30 30 30 35    0 0 3 3 4 5 6 7 13                  13-12=1</span><br><span class="line">14   61 62 65 66 31 30 30 30 36    0 0 3 3 4 5 6 7 14                  14-5=1</span><br><span class="line">15   61 63                         还未计算                             15-5(第5个下标)=10&gt;=minItemsInBlock (因为前面的字符串突然不匹配，才有较大的差值，此时说明后10个terms拥有相同前缀的长度为5)</span><br></pre></td></tr></table></figure></li>
</ul>
<p>prefixStarts数组的变化如图所示：每层第一次出现的字母的位置为i，则currentRerm[i]!&#x3D;lastTerm[i], prefixStarts[i]&#x3D;pending.size()。而仅当pending.size() - prefixStarts[i] &gt; minItemsInBlock（线上默认配置25）时， 会截取这部分terms进行构建子FST结构。 假如minItemsInBlock&#x3D;10， 那么我们会选择从3-14行共12个term产生子FST。</p>
<ul>
<li>PendingTerm与PendingBlock<br>单个term放在PendingTerm里面；多个相似性高的term组建起来，将这些terms相似前缀作为一个term，构建成一个子字典，形成一个PendingBlock，来代替这组相似的terms和别term继续组建更广泛的字典。</li>
</ul>
<p><code>TermsWriter.writeBlocks()</code>介绍了如何将选出来的相似性terms组成一个子字典的（FST）:<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">// 表示pending中后count个term拥有相同的前缀，前缀长度为prefixLength</span><br><span class="line">void writeBlocks(int prefixLength, int count) throws IOException &#123;</span><br><span class="line">  int lastSuffixLeadLabel = -1;</span><br><span class="line">  // 如果这个值为true，那么这个block中至少有一个term</span><br><span class="line">  boolean hasTerms = false;</span><br><span class="line">  // 如果这个值为true，那么这个block中至少有一个新产生的block</span><br><span class="line">  boolean hasSubBlocks = false;</span><br><span class="line">  // 计算起始位置, 从后向前取值</span><br><span class="line">  int start = pending.size()-count;</span><br><span class="line">  // 终止位置</span><br><span class="line">  int end = pending.size();</span><br><span class="line">  // 记录下一个 block 的起始位置</span><br><span class="line">  int nextBlockStart = start;</span><br><span class="line">  // floor block 中第一个term 的后缀的第一个字符当成 Lead label</span><br><span class="line">  int nextFloorLeadLabel = -1;</span><br><span class="line">  for (int i=start; i&lt;end; i++) &#123;</span><br><span class="line">    // term 的后缀的第一个term</span><br><span class="line">    PendingEntry ent = pending.get(i);</span><br><span class="line">    // 保存了树中某个节点下的各个Term的byte</span><br><span class="line">    int suffixLeadLabel;</span><br><span class="line">    // term 的后缀的第一个字符</span><br><span class="line">    if (ent.isTerm) &#123;</span><br><span class="line">      PendingTerm term = (PendingTerm) ent;</span><br><span class="line">      // 和前缀一样长</span><br><span class="line">      if (term.termBytes.length == prefixLength) &#123; </span><br><span class="line">        suffixLeadLabel = -1; // 后缀值</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        // 就是prefixLength上某个字符</span><br><span class="line">        suffixLeadLabel = term.termBytes[prefixLength] &amp; 0xff; </span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      PendingBlock block = (PendingBlock) ent;</span><br><span class="line">      // 不同那个后缀</span><br><span class="line">      suffixLeadLabel = block.prefix.bytes[block.prefix.offset + prefixLength] &amp; 0xff; </span><br><span class="line">    &#125;</span><br><span class="line">    // 第prefixLength个字符不一致。</span><br><span class="line">    if (suffixLeadLabel != lastSuffixLeadLabel) &#123;</span><br><span class="line">      // 这个block的的长度</span><br><span class="line">      int itemsInBlock = i - nextBlockStart;</span><br><span class="line">      // 如果当前term总个数超过minItemsInBlock，且剩余term个数大于maxItemsInBlock，则将当前所有term构建一个FST, 产生一个block</span><br><span class="line">      if (itemsInBlock &gt;= minItemsInBlock &amp;&amp; end-nextBlockStart &gt; maxItemsInBlock) &#123;</span><br><span class="line">        // 若这个block小于这次满足需求的总block, 那么就拆分</span><br><span class="line">        boolean isFloor = itemsInBlock &lt; count; </span><br><span class="line">        newBlocks.add(writeBlock(prefixLength, isFloor, nextFloorLeadLabel, nextBlockStart, i, hasTerms, hasSubBlocks));</span><br><span class="line">        hasTerms = false;</span><br><span class="line">        hasSubBlocks = false;</span><br><span class="line">        // 下一个block的不相同的字母。</span><br><span class="line">        nextFloorLeadLabel = suffixLeadLabel;</span><br><span class="line">        // 记录下一个block的起始位置</span><br><span class="line">        nextBlockStart = i;</span><br><span class="line">      &#125;</span><br><span class="line">      // 更新term 的后缀的第一个字符</span><br><span class="line">      lastSuffixLeadLabel = suffixLeadLabel;</span><br><span class="line">    &#125;</span><br><span class="line">    if (ent.isTerm) &#123;</span><br><span class="line">      hasTerms = true;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      hasSubBlocks = true;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 最后一个</span><br><span class="line">  if (nextBlockStart &lt; end) &#123;</span><br><span class="line">    int itemsInBlock = end - nextBlockStart;</span><br><span class="line">    boolean isFloor = itemsInBlock &lt; count;</span><br><span class="line">    newBlocks.add(writeBlock(prefixLength, isFloor, nextFloorLeadLabel, nextBlockStart, end, hasTerms, hasSubBlocks));</span><br><span class="line">  &#125;</span><br><span class="line">  PendingBlock firstBlock = newBlocks.get(0);</span><br><span class="line">  // 将一个block的信息写入FST结构中（保存在其成员变量index中），FST是有限状态机的缩写，其实就是将一棵树的信息保存在其自身的结构中，而这颗树是由所有Term的每个byte形成的</span><br><span class="line">  firstBlock.compileIndex(newBlocks, scratchBytes, scratchIntsRef); // scratchBytes，scratchIntsRef还没有存储</span><br><span class="line">  // 对每个写入磁盘的block的前缀 prefix构建一个FST索引 // 所有block的FST索引联合成一个FST索引，并将联合的FST写入 root block</span><br><span class="line">  // Remove slice from the top of the pending stack, that we just wrote:</span><br><span class="line">  pending.subList(pending.size()-count, pending.size()).clear();</span><br><span class="line">  pending.add(firstBlock); // 向里面写入了一个block</span><br><span class="line">  newBlocks.clear();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>该函数主要的作用是：<br>1.将每minItemsInBlock(默认值25)个Term，通过<code>writeBlock()</code>组合成一个Term（目的是减少最顶层词典中词的个数，为PendingBlock），继续和这组相似的term构建FST结构。哪些相似的term可以组合成一个term呢？符合条件是这样的：</p>
<ul>
<li>该term和上一个term的第prefixLength个字符不同。</li>
<li>只要每隔minItemsInBlock（默认25）个词就可以组合成一个term。<br>这里maxItemsInBlock（默认48）的主要作用是：只要剩余terms个数太少，那么将剩余terms和当前block一起组合成PendingBlock，一个block里面term最多只能为maxItemsInBlock个。</li>
</ul>
<p>2.将剩余的PendTerm组装成一个PendingBlock。这里需要注意下，在1、2步骤组装成的PendingBlock时，会将子PengingBlock里面的FST读取出来，在第3步构建时，放入FST中。<br>3.调用<code>firstBlock.compileIndex()</code>将1,2产生的所有PendingBlock，构建成一个FST。FSP再放入第一个PendingBlock，继续放入pending中进行词典组装。</p>
<p>我们再继续看下<code>BlockTreeTermsWriter.writeBlock()</code>是怎么将这批terms组成一个PendingBlock。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">private PendingBlock writeBlock(int prefixLength, boolean isFloor, int floorLeadLabel, int start, int end,</span><br><span class="line">                                boolean hasTerms, boolean hasSubBlocks) throws IOException &#123;</span><br><span class="line">  long startFP = termsOut.getFilePointer(); // 获取当前block在tim中的起始位置</span><br><span class="line">  boolean hasFloorLeadLabel = isFloor &amp;&amp; floorLeadLabel != -1;</span><br><span class="line">  final BytesRef prefix = new BytesRef(prefixLength + (hasFloorLeadLabel ? 1 : 0));</span><br><span class="line">  System.arraycopy(lastTerm.get().bytes, 0, prefix.bytes, 0, prefixLength);</span><br><span class="line">  prefix.length = prefixLength;</span><br><span class="line">  int numEntries = end是term中第i个元素与前一个ter - start;</span><br><span class="line">  int code = numEntries &lt;&lt; 1; // block的term个数</span><br><span class="line">  if (end == pending.size()) &#123;</span><br><span class="line">    // Last block:</span><br><span class="line">    code |= 1;  // 标志是最后一个block</span><br><span class="line">  &#125;</span><br><span class="line">  termsOut.writeVInt(code); // tim</span><br><span class="line">  boolean isLeafBlock = hasSubBlocks == false;// 如果这个值为true，那么这个block中没有一个是新产生的block</span><br><span class="line">  //System.out.println(&quot;  isLeaf=&quot; + isLeafBlock);</span><br><span class="line">  final List&lt;FST&lt;BytesRef&gt;&gt; subIndices; // 存放的是子fst结构</span><br><span class="line">  boolean absolute = true;</span><br><span class="line">   // block仅有terms, 而没有PendingBlock</span><br><span class="line">  if (isLeafBlock) &#123; </span><br><span class="line">    // Block contains only ordinary terms:</span><br><span class="line">    subIndices = null;</span><br><span class="line">     // 遍历每一个term，将其不同的后缀给存储起来</span><br><span class="line">    for (int i=start;i&lt;end;i++) &#123; </span><br><span class="line">      PendingEntry ent = pending.get(i);</span><br><span class="line">      PendingTerm term = (PendingTerm) ent;</span><br><span class="line">      BlockTermState state = term.state;</span><br><span class="line">       // 该词后缀长度</span><br><span class="line">      final int suffix = term.termBytes.length - prefixLength;</span><br><span class="line">      // 写入后缀内容</span><br><span class="line">      suffixWriter.writeVInt(suffix);</span><br><span class="line">      suffixWriter.writeBytes(term.termBytes, prefixLength, suffix);</span><br><span class="line">      // 该词在多少文件中出现过</span><br><span class="line">      statsWriter.writeVInt(state.docFreq);  </span><br><span class="line">      if (fieldInfo.getIndexOptions() != IndexOptions.DOCS) &#123;</span><br><span class="line">        // 多出来的词频</span><br><span class="line">        statsWriter.writeVLong(state.totalTermFreq - state.docFreq); </span><br><span class="line">      &#125;</span><br><span class="line">      // 将state中的元数据，抽取出来放在longs,bytesWriter中</span><br><span class="line">      // Write term meta data， 将doc的block存放在这里</span><br><span class="line">      // 将doc.pos,pay信息放入longs中</span><br><span class="line">      postingsWriter.encodeTerm(longs, bytesWriter, fieldInfo, state, absolute);</span><br><span class="line">       // 写入doc + pos + pay 三个在对应文件中与上个term的差值</span><br><span class="line">      for (int pos = 0; pos &lt; longsSize; pos++) &#123;</span><br><span class="line">        metaWriter.writeVLong(longs[pos]);</span><br><span class="line">      &#125;</span><br><span class="line">      // 将byte写入metaWriter中</span><br><span class="line">      bytesWriter.writeTo(metaWriter); </span><br><span class="line">      bytesWriter.reset();</span><br><span class="line">      absolute = false;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; else &#123; </span><br><span class="line">    // 该block子pending中包含子Block</span><br><span class="line">    subIndices = new ArrayList&lt;&gt;();</span><br><span class="line">    for (int i=start;i&lt;end;i++) &#123;</span><br><span class="line">      PendingEntry ent = pending.get(i);</span><br><span class="line">      if (ent.isTerm) &#123;</span><br><span class="line">        ......</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        PendingBlock block = (PendingBlock) ent;</span><br><span class="line">        final int suffix = block.prefix.length - prefixLength;</span><br><span class="line">        suffixWriter.writeVInt((suffix&lt;&lt;1)|1);</span><br><span class="line">        suffixWriter.writeBytes(block.prefix.bytes, prefixLength, suffix);</span><br><span class="line">        suffixWriter.writeVLong(startFP - block.fp);</span><br><span class="line">        subIndices.add(block.index);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 写入suffixWriter中的长度，写入tim中</span><br><span class="line">  termsOut.writeVInt((int) (suffixWriter.getFilePointer() &lt;&lt; 1) | (isLeafBlock ? 1:0)); </span><br><span class="line">   // 将suffixWriter中的byte[]写入tim中</span><br><span class="line">  suffixWriter.writeTo(termsOut);</span><br><span class="line">  suffixWriter.reset();</span><br><span class="line">  // Write term stats byte[] blob</span><br><span class="line">  termsOut.writeVInt((int) statsWriter.getFilePointer()); //写入长度</span><br><span class="line">  statsWriter.writeTo(termsOut); // 写入内容</span><br><span class="line">  statsWriter.reset();</span><br><span class="line">  // Write term meta data byte[] blob</span><br><span class="line">  termsOut.writeVInt((int) metaWriter.getFilePointer());</span><br><span class="line">  metaWriter.writeTo(termsOut);</span><br><span class="line">  metaWriter.reset();</span><br><span class="line">   // 共同前缀长度prefix.length+1</span><br><span class="line">  if (hasFloorLeadLabel) &#123;</span><br><span class="line">    prefix.bytes[prefix.length++] = (byte) floorLeadLabel;</span><br><span class="line">  &#125;</span><br><span class="line">  // 产生一个新的PendingBlock</span><br><span class="line">  return new PendingBlock(prefix, startFP, hasTerms, isFloor, floorLeadLabel, subIndices); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要是将start-end长度的terms组装成一个PendingBlock, 针对PendingTerm采集的term信息如下：<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_tim6.png" height="350" width="800"><br>然后将suffixWriter、statsWriter、metaWriter依次写入tim文件中。若这批terms中有PendingBlock的话，只在suffixWriter记录该block前缀、该block在tim中的存储位置，并将该block下所有的FST保存在该新创建的PendingBlock中，以便后续根据这些FST产生新的FST。</p>
<p>我们再继续看下firstBlock.compileIndex()，是如何将<code>writeBlocks()</code>产生的所有Block合并产生一个FST的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">public void compileIndex(List&lt;PendingBlock&gt; blocks, RAMOutputStream scratchBytes, IntsRefBuilder scratchIntsRef) throws IOException &#123;</span><br><span class="line">   // 写入scratchBytes比较重要，作为FST数的</span><br><span class="line">   //写入scratchBytes：最高62为存放tim起始位置，低一位存放是否有terms，最低位存放是否是floor机器。</span><br><span class="line">  scratchBytes.writeVLong(encodeOutput(fp, hasTerms, isFloor));</span><br><span class="line">  if (isFloor) &#123;</span><br><span class="line">    // 同时将每个block在tim中的fp也保存起来</span><br><span class="line">    scratchBytes.writeVInt(blocks.size()-1); </span><br><span class="line">    // 遍历每一个block</span><br><span class="line">    for (int i=1;i&lt;blocks.size();i++) &#123; </span><br><span class="line">      PendingBlock sub = blocks.get(i); </span><br><span class="line">      // 下一个block与上个block相比，不同的字符</span><br><span class="line">      scratchBytes.writeByte((byte) sub.floorLeadByte); </span><br><span class="line">      scratchBytes.writeVLong((sub.fp - fp) &lt;&lt; 1 | (sub.hasTerms ? 1 : 0));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 这里会新产生以Builder</span><br><span class="line">  final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();</span><br><span class="line">  final Builder&lt;BytesRef&gt; indexBuilder = new Builder&lt;&gt;(FST.INPUT_TYPE.BYTE1,</span><br><span class="line">                                                       0, 0, true, false, Integer.MAX_VALUE,</span><br><span class="line">                                                       outputs, true, 15);</span><br><span class="line">  // scratchBytes里面的长度。存的是fp&amp;hasTerms&amp;isFloor</span><br><span class="line">  final byte[] bytes = new byte[(int) scratchBytes.getFilePointer()]; </span><br><span class="line">  scratchBytes.writeTo(bytes, 0);</span><br><span class="line">  // 将本身的字符放入FST中</span><br><span class="line">  indexBuilder.add(Util.toIntsRef(prefix, scratchIntsRef), new BytesRef(bytes, 0, bytes.length));</span><br><span class="line">  scratchBytes.reset();</span><br><span class="line">  // 将 sub-block 的所有 index 写入indexBuilder(比较重要)</span><br><span class="line">  // Copy over index for all sub-blocks</span><br><span class="line">  for(PendingBlock block : blocks) &#123;</span><br><span class="line">    if (block.subIndices != null) &#123;</span><br><span class="line">      // 遍历所有的block</span><br><span class="line">      for(FST&lt;BytesRef&gt; subIndex : block.subIndices) &#123;</span><br><span class="line">      // 当做普通字符串再加入新的fst中</span><br><span class="line">        append(indexBuilder, subIndex, scratchIntsRef);</span><br><span class="line">      &#125;</span><br><span class="line">      block.subIndices = null;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 生成新的FST</span><br><span class="line">  index = indexBuilder.finish();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要是将产生的PengingBlock组装成一个FST， 主要做了如下工作：</p>
<ol>
<li>构建FST的output, output里面存放了如下信息：<img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_tim8.png" height="180" width="650">
output能够快速定位block在tim中的存储位置，主要由一下几部分构成：</li>
</ol>
<ul>
<li>block0在tim文件中的起始位置。</li>
<li>即将构建的FST是否有term。</li>
<li>blockCount-1个数组，分别反映了该fst下blockCount-1个子block在tim中的位置。</li>
<li>floorLeadByte后续紧接着的block的terms第一个term最后一个字符(与本block的terms最后一个term不相同的字符)</li>
</ul>
<ol start="2">
<li>将输入的产生的PendingBlock列表构建成一个FST，构建原理可以参考：<a href="https://kkewwei.github.io/elasticsearch_learning/2020/02/25/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-%E8%AF%8D%E5%85%B8fst%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/">Lucene8.2.0底层架构-词典fst原理解析</a><br>构建出来的FST(放入tip中了)如下所示：<img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_tim7.png" height="400" width="400">
也就是说，只要通过FST的output0，就可以找到block0、block1、block2、block3在tim中存储的位置。其中block0存放着整个FST结构，和别的term再次构建FST.</li>
</ol>
<h2 id="FST结构写入tip中"><a href="#FST结构写入tip中" class="headerlink" title="FST结构写入tip中"></a>FST结构写入tip中</h2><p>当前域所有term放入词典后，开始将当前域的词典索引结构FSP放入tip文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">public void finish() throws IOException &#123;</span><br><span class="line">  if (numTerms &gt; 0) &#123;</span><br><span class="line">    pushTerm(new BytesRef());</span><br><span class="line">    // 将剩余的再次产生一个PendingTerm</span><br><span class="line">    pushTerm(new BytesRef());</span><br><span class="line">    writeBlocks(0, pending.size()); </span><br><span class="line">    // 有个最终root的 block</span><br><span class="line">    // We better have one final &quot;root&quot; block:</span><br><span class="line">    final PendingBlock root = (PendingBlock) pending.get(0);</span><br><span class="line">    // Write FST to index（tip中）</span><br><span class="line">    indexStartFP = indexOut.getFilePointer();  </span><br><span class="line">    // 将fst写入tip文件</span><br><span class="line">    root.index.save(indexOut); </span><br><span class="line">    // 该域写入的第一个词</span><br><span class="line">    BytesRef minTerm = new BytesRef(firstPendingTerm.termBytes); </span><br><span class="line">    // 该域写入的最后一个词</span><br><span class="line">    BytesRef maxTerm = new BytesRef(lastPendingTerm.termBytes); </span><br><span class="line">    fields.add(new FieldMetaData(fieldInfo,</span><br><span class="line">                                 ((PendingBlock) pending.get(0)).index.getEmptyOutput(),</span><br><span class="line">                                 numTerms,</span><br><span class="line">                                 indexStartFP,</span><br><span class="line">                                 sumTotalTermFreq,</span><br><span class="line">                                 sumDocFreq,</span><br><span class="line">                                  // 该词有多少个文档</span><br><span class="line">                                 docsSeen.cardinality(),</span><br><span class="line">                                 longsSize,</span><br><span class="line">                                 minTerm, maxTerm));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要走了如下事情：<br>1.通过第一次调用<code>pushTerm(new BytesRef())</code>将pending中缓存的PendingBlocks组装成一个PendingBlock。<br>2.调用<code>writeBlocks(0, pending.size())</code>将最后一个block里缓存的子fst构成一个最终的FST。<br>3.将这个FST写入tip文件中。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>单个term的词频等信息保存正在Doc文件中，使用跳表结构来快速索引。segment每个域都有个词典结构，词典索引结构FST存放在tip文件中， tim中存放了每个PendingBlock里所有term的内容，以及每个词的统计信息， 每个词的倒排索引结构其实是存放在doc文件中。doc中存放了跳表结构，可以快速定位某个词的词频等信息。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://kkewwei.github.io/elasticsearch_learning/2020/02/25/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-%E8%AF%8D%E5%85%B8fst%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/">https://kkewwei.github.io/elasticsearch_learning/2020/02/25/Lucene8-2-0%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-%E8%AF%8D%E5%85%B8fst%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/</a><br><a href="https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0401/43.html" target="_blank" rel="noopener">https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0401/43.html</a></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">Lucene、词典、tim、tip、doc、pos、fst、倒排索引</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/elasticsearch_learning/categories/Lucene//" class="article-tag-list-link color2">Lucene</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/elasticsearch_learning/2020/02/28/Lucene8-2-0底层架构-tim-tip词典结构原理研究/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-Lucene8-2-0底层架构-词典fst原理解析" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/elasticsearch_learning/2020/02/25/Lucene8-2-0底层架构-词典fst原理解析/">Lucene8.2.0底层架构-词典fst原理解析</a>
    </h1>
  

        
        <a href="/elasticsearch_learning/2020/02/25/Lucene8-2-0底层架构-词典fst原理解析/" class="archive-article-date">
  	<time datetime="2020-02-25T11:29:57.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2020-02-25</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>lucene中使用fst(Finite State Transducer)来对字典压缩存储、快速查询词典, fst结构紧凑, 在时间复杂度和空间复杂度之间均衡, 和HashMap结构相比, 该结构体共享字符串前缀和后缀以达到精简内存使用, FST逻辑上构成了一个有向无环图, 本文将对fst结构体的构建过程进行分析。</p>
<h1 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h1><p>在输入字符串时词典时, 必须经过排序, 这样产生的fst才最小。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">// 输入字符串</span><br><span class="line">String inputValues[] = &#123;&quot;abcde&quot;,&quot;abdde&quot;,&quot;abede&quot;,&quot;accde&quot;,&quot;adcde&quot;&#125;;</span><br><span class="line">// 对应字符串的附加output</span><br><span class="line">Long outputValues[] = &#123;1L,2L,3L,4L,5L&#125;;</span><br><span class="line">PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton();</span><br><span class="line">//构建FST</span><br><span class="line">Builder&lt;Long&gt; builder = new Builder&lt;&gt;(FST.INPUT_TYPE.BYTE1,outputs);</span><br><span class="line">IntsRefBuilder scratchInts = new IntsRefBuilder();</span><br><span class="line">for (int i = 0; i &lt; inputValues.length; i++) &#123;</span><br><span class="line">    BytesRef scratchBytes =  new BytesRef(inputValues[i].getBytes());</span><br><span class="line">    scratchInts.copyUTF8Bytes(scratchBytes);</span><br><span class="line">    builder.add(scratchInts.toIntsRef(),outputValues[i]);</span><br><span class="line">&#125;</span><br><span class="line">// 构建fst结构</span><br><span class="line">FST&lt;Long&gt; fst = builder.finish();</span><br><span class="line">// 读取该字符串对应的值</span><br><span class="line">Long value = Util.get(fst, new BytesRef(&quot;abcde&quot;));</span><br><span class="line">System.out.println(value);</span><br></pre></td></tr></table></figure>

<p>以上示例我们使用fst作为容器存储字符串, 通过fst查询获得相应out的过程, 看起来是不是和HashMap使用场景很像, 我们可以看下fst构成的<a href="http://examples.mikemccandless.com/fst.py?terms=abcde%2F1%0D%0Aabdde%2F2%0D%0Aabede%2F3%0D%0Aaccde%2F4%0D%0Aadcde%2F4&cmd=Build+it%21" target="_blank" rel="noopener">逻辑存储结构</a>。</p>
<h1 id="Builder"><a href="#Builder" class="headerlink" title="Builder"></a>Builder</h1><p>我们可以看到, fst主要是在Builder完成结构构建的, 实际存储是通过FST来完成的, 为了更好地理解, 我们先对该Builder属性做个介绍:<br>该类的属性:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 共享后缀使用的, 存储当前存在的后缀,</span><br><span class="line">private final NodeHash&lt;T&gt; dedupHash;</span><br><span class="line">// 真正保存构建的fst结构的地方</span><br><span class="line">final FST&lt;T&gt; fst;</span><br><span class="line">// 缓存的前一个字符串</span><br><span class="line">private final IntsRefBuilder lastInput = new IntsRefBuilder();</span><br><span class="line">// 缓存的前一个字符串形成的UnCompiledNode。前一个字符串的不同后缀会在当前字符串写入时候freeze存储到fst中。</span><br><span class="line">private UnCompiledNode&lt;T&gt;[] frontier;</span><br><span class="line">是当前添加的字符串所形成的状态节点，而前面添加的字符串形成的状态节点通过指针相互引用。</span><br><span class="line">// 记录的当前节点所有边的长度，临时变量。</span><br><span class="line">int[] reusedBytesPerArc = new int[4];</span><br><span class="line">// 默认为true</span><br><span class="line">boolean allowArrayArcs;</span><br><span class="line">// 和FST中的bytes是同一个对象, 真正存储fst结构的地方。</span><br><span class="line">BytesStore bytes;</span><br></pre></td></tr></table></figure>

<p>1.UnCompiledNode和CompiledNode<br>Node作为FST的节点, 分为以上两种角色, UnCompiledNode表示该节点还可能后来的字符共享, 由于输入的字符串有先后顺序, 若不能共享的节点经过freezeTail处理后(compile), 就变成了CompiledNode。<br>UnCompiledNode的主要属性:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">final Builder&lt;T&gt; owner;</span><br><span class="line">  // 该节点的出边条数</span><br><span class="line">  public int numArcs;</span><br><span class="line">  // 该节点总共的出边</span><br><span class="line">  public Arc&lt;T&gt;[] arcs;</span><br><span class="line">  // 表示从Node出发的Arc数量是0，当Node是Final Node时，output才有值。</span><br><span class="line">  public boolean isFinal;</span><br><span class="line">   //这个节点上UnCompiledNode进来的边。实际基本并没有使用</span><br><span class="line">  public long inputCount;</span><br><span class="line">  // 从根节点到本节点的深度</span><br><span class="line">  public final int depth;</span><br></pre></td></tr></table></figure>

<p>CompiledNode属性node表示该节点对应的边存放在fst中的address。<br>2.Arc<br>Arc表示FST中的边。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 每个表代表着一个字符,</span><br><span class="line">public int label;</span><br><span class="line">// 该边目标节点</span><br><span class="line">public Node target;</span><br><span class="line">// 表示当前Arc上附带的值, 可以认为key对应的value。</span><br><span class="line">public T output;</span><br></pre></td></tr></table></figure>

<h1 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h1><p>真正开始构建fst结构是从<code>Builder.add</code>中开始的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">public void add(IntsRef input, T output) throws IOException &#123; // output: BytesRef</span><br><span class="line">  // 比较当前字符串和前一个字符串相同的前缀长度</span><br><span class="line">  int pos1 = 0;</span><br><span class="line">  int pos2 = input.offset;</span><br><span class="line">  final int pos1Stop = Math.min(lastInput.length(), input.length);</span><br><span class="line">  while(true) &#123;</span><br><span class="line">    frontier[pos1].inputCount++;</span><br><span class="line">    //System.out.println(&quot;  incr &quot; + pos1 + &quot; ct=&quot; + frontier[pos1].inputCount + &quot; n=&quot; + frontier[pos1]);</span><br><span class="line">    if (pos1 &gt;= pos1Stop || lastInput.intAt(pos1) != input.ints[pos2]) &#123;</span><br><span class="line">      break;</span><br><span class="line">    &#125;</span><br><span class="line">    pos1++;</span><br><span class="line">    pos2++;</span><br><span class="line">  &#125;</span><br><span class="line">  final int prefixLenPlus1 = pos1+1; // 公共长度+1</span><br><span class="line"></span><br><span class="line">  // 扩容frontier, 最起码保证frontier能够存放当前的字符串(前一个字符串一定可以存放的下)</span><br><span class="line">  if (frontier.length &lt; input.length+1) &#123;</span><br><span class="line">    final UnCompiledNode&lt;T&gt;[] next = ArrayUtil.grow(frontier, input.length+1);</span><br><span class="line">    for(int idx=frontier.length;idx&lt;next.length;idx++) &#123;</span><br><span class="line">      next[idx] = new UnCompiledNode&lt;&gt;(this, idx);</span><br><span class="line">    &#125;</span><br><span class="line">    frontier = next;</span><br><span class="line">  &#125;</span><br><span class="line">  // 前一个字符串的不同后缀不会再共享了, 那么我们就冰冻前一个字符串不同的后缀, 并存储。</span><br><span class="line">  freezeTail(prefixLenPlus1);</span><br><span class="line">  // 针对当前字符串不同的后缀, 首先构建对应的边</span><br><span class="line">  for(int idx=prefixLenPlus1;idx&lt;=input.length;idx++) &#123;</span><br><span class="line">    frontier[idx-1].addArc(input.ints[input.offset + idx - 1],</span><br><span class="line">                           frontier[idx]);</span><br><span class="line">    frontier[idx].inputCount++;</span><br><span class="line">  &#125;</span><br><span class="line"> // 最后一个边设置为true</span><br><span class="line">  final UnCompiledNode&lt;T&gt; lastNode = frontier[input.length];</span><br><span class="line">  if (lastInput.length() != input.length || prefixLenPlus1 != input.length + 1) &#123;</span><br><span class="line">    lastNode.isFinal = true;</span><br><span class="line">    lastNode.output = NO_OUTPUT;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // 遍历针对冲突的output，尽可能在相同的前缀里共享。</span><br><span class="line">  for(int idx=1;idx&lt;prefixLenPlus1;idx++) &#123;</span><br><span class="line">    final UnCompiledNode&lt;T&gt; node = frontier[idx];</span><br><span class="line">    final UnCompiledNode&lt;T&gt; parentNode = frontier[idx-1];</span><br><span class="line">    // 既然可以共享前缀，那么前缀边和本边是一致的，那么一定是一条直线，没有分叉。获取当前边的output</span><br><span class="line">    final T lastOutput = parentNode.getLastOutput(input.ints[input.offset + idx - 1]);</span><br><span class="line">    final T commonOutputPrefix;// BytesRef</span><br><span class="line">    final T wordSuffix;// BytesRef</span><br><span class="line">     // 读取上一个的output，若存在，检查是否可以和新的合并</span><br><span class="line">    if (lastOutput != NO_OUTPUT) &#123;</span><br><span class="line">      // 获取第一条共享边和output的相同前缀</span><br><span class="line">      commonOutputPrefix = fst.outputs.common(output, lastOutput);</span><br><span class="line">      // 扣除相同的output前缀</span><br><span class="line">      wordSuffix = fst.outputs.subtract(lastOutput, commonOutputPrefix); // lastOutput-commonOutputPrefix=不同的后缀部分</span><br><span class="line">      //并修正该边的output</span><br><span class="line">      parentNode.setLastOutput(input.ints[input.offset + idx - 1], commonOutputPrefix);</span><br><span class="line">      //将剩余后缀一定到下一个节点对应的边。</span><br><span class="line">      node.prependOutput(wordSuffix);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      commonOutputPrefix = wordSuffix = NO_OUTPUT;</span><br><span class="line">    &#125;</span><br><span class="line">    // 检查输入时候的output, 并在此向后循环</span><br><span class="line">    output = fst.outputs.subtract(output, commonOutputPrefix);</span><br><span class="line">  &#125;</span><br><span class="line">  // 本次输入的内容和上次输入的内容一样，则可以将output给合并了。</span><br><span class="line">  if (lastInput.length() == input.length &amp;&amp; prefixLenPlus1 == 1+input.length) &#123;</span><br><span class="line">    lastNode.output = fst.outputs.merge(lastNode.output, output);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // 将剩余output放到新创建的最前的那条边</span><br><span class="line">    frontier[prefixLenPlus1-1].setLastOutput(input.ints[input.offset + prefixLenPlus1-1], output);</span><br><span class="line">  &#125;</span><br><span class="line">  // 处理完了, 替换上一次的字符串。</span><br><span class="line">  lastInput.copyInts(input);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下几个事情:</p>
<ol>
<li>获取相同前缀长度</li>
<li>调用<code>freezeTail</code>将前一个字符串保存在frontier中的不同后缀冷冻起来, 也就是说前一个字符串不同的后缀将不会被别的字符串共享, 那么我们就可以将这个后缀(前一个字符串的)给存储起来。存储的时候还会做到共享后缀</li>
<li>前一个字符串的后缀在frontier中已经废弃, 此时将当前字符串的后缀产生UnCompiledNode, 存放到frontier中。</li>
<li>前后两个字符串尽量共享相同的前缀output。</li>
<li>将当前写入的字符串output经过共享后, 剩余部分写入当前后缀第一个字符串对应的边上。</li>
</ol>
<p>我们接下来关注下<code>Builder.freezeTail()</code>是如何做到共享后缀, 并存储的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">private void freezeTail(int prefixLenPlus1) throws IOException &#123;</span><br><span class="line">  //这里downTo大于等于1可以保证根节点不会被写入到FST中去，根节点必须要所有节点写完之后才能写到FST</span><br><span class="line">  final int downTo = Math.max(1, prefixLenPlus1);</span><br><span class="line">  // 节点idx，从后向前，只压缩存储不同的后缀</span><br><span class="line">  for(int idx=lastInput.length(); idx &gt;= downTo; idx--) &#123;</span><br><span class="line">    final UnCompiledNode&lt;T&gt; node = frontier[idx]; // 结尾那个NULL的node</span><br><span class="line">    final UnCompiledNode&lt;T&gt; parent = frontier[idx-1];</span><br><span class="line">      final boolean isFinal = node.isFinal || node.numArcs == 0;</span><br><span class="line">      // 用compileNode 函数将 Node写入FST，得到一个 CompiledNode，替换掉 Arc的target, compileNode可能返回的是一个已经写入FST的 Node对应边的存储地址，这样就达到了共享 Node 的目的</span><br><span class="line">      parent.replaceLast(lastInput.intAt(idx-1),</span><br><span class="line">                           compileNode(node, 1+lastInput.length()-idx),</span><br><span class="line">                           nextFinalOutput,</span><br><span class="line">                           isFinal);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>为啥函数名称是freezeTail, 因为是从最后一个节点开始存储, 这样便于查找相同的后缀。 针对每个UnCompiledNode调用compileNode, 将该节点转变为CompiledNode。 接下来再一起看下<code>Builder.compileNode</code>是如何做到共享存储的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">private CompiledNode compileNode(UnCompiledNode&lt;T&gt; nodeIn, int tailLength) throws IOException &#123;</span><br><span class="line">  final long node;</span><br><span class="line">  long bytesPosStart = bytes.getPosition();</span><br><span class="line">  if (dedupHash != null &amp;&amp; (doShareNonSingletonNodes || nodeIn.numArcs &lt;= 1) &amp;&amp; tailLength &lt;= shareMaxTailLength) &#123;// doShareNonSingletonNodes默认为false</span><br><span class="line">     // 说明该节点是结尾。</span><br><span class="line">    if (nodeIn.numArcs == 0) &#123;</span><br><span class="line">      // Node的Arc数量为0，fst.addNode 会返回 -1。已经将nodeId的value存放了fst中</span><br><span class="line">      node = fst.addNode(this, nodeIn);</span><br><span class="line">      lastFrozenNode = node;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // 将该节点存储到dedupHash中, 若已经存在的话, 就直接共享了。</span><br><span class="line">      node = dedupHash.add(this, nodeIn);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // 把node写入fst中</span><br><span class="line">    node = fst.addNode(this, nodeIn);</span><br><span class="line">  &#125;</span><br><span class="line">  long bytesPosEnd = bytes.getPosition();</span><br><span class="line">  // 若fst中有新增存储的话, 就说明该节点没有被共享。</span><br><span class="line">  if (bytesPosEnd != bytesPosStart) &#123;</span><br><span class="line">    // The FST added a new node:</span><br><span class="line">    lastFrozenNode = node;</span><br><span class="line">  &#125;</span><br><span class="line">  // 这里将这个nodeIn给清空了，意味着该边被compile了，</span><br><span class="line">  nodeIn.clear();</span><br><span class="line">  final CompiledNode fn = new CompiledNode();</span><br><span class="line">  fn.node = node;</span><br><span class="line">  return fn;</span><br></pre></td></tr></table></figure>

<p>该函数主要作用实际调用<code>dedupHash.add</code>将当前节点存储到fst中, 若已经存在的话, 则共享已经存在的节点。这里可能有人有疑惑, 边才是存储的真正的字符串, 存储节点作用不大呀, 实际上,每个节点存储时, 将边对应的lebal给考虑进来了。<br>我们看下<code>dedupHash.add</code>是如何共享存储的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">public long add(Builder&lt;T&gt; builder, Builder.UnCompiledNode&lt;T&gt; nodeIn) throws IOException &#123;</span><br><span class="line">  // 该节点hash出一个值</span><br><span class="line">  final long h = hash(nodeIn);</span><br><span class="line">  long pos = h &amp; mask;</span><br><span class="line">  int c = 0;</span><br><span class="line">  while(true) &#123;</span><br><span class="line">    final long v = table.get(pos);// 没有便是0</span><br><span class="line">    if (v == 0) &#123;</span><br><span class="line">      // freeze &amp; add</span><br><span class="line">      // 该节点在fst中的存储位置</span><br><span class="line">      final long node = fst.addNode(builder, nodeIn);</span><br><span class="line">      //System.out.println(&quot;  now freeze node=&quot; + node);</span><br><span class="line">      assert hash(node) == h : &quot;frozenHash=&quot; + hash(node) + &quot; vs h=&quot; + h;</span><br><span class="line">      count++;</span><br><span class="line">      // node=fst中该词的存储位置</span><br><span class="line">      table.set(pos, node);</span><br><span class="line">      // Rehash at 2/3 occupancy:</span><br><span class="line">      if (count &gt; 2*table.size()/3) &#123;</span><br><span class="line">        rehash();</span><br><span class="line">      &#125;</span><br><span class="line">      return node;</span><br><span class="line">    &#125; else if (nodesEqual(nodeIn, v)) &#123;</span><br><span class="line">      // same node is already here</span><br><span class="line">      return v;</span><br><span class="line">    &#125;</span><br><span class="line">    // quadratic probe</span><br><span class="line">    pos = (pos + (++c)) &amp; mask;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要是hash值判断节点是否存在:<br>1.若不为0, 则说明该节点存在, 那么就可以共享, hash返回值即为该节点在fst中的存储位置, 并返回。<br>2.若返回值为0, 则说明该节点不存在, 调用<code>fst.addNode</code>将该节点存储起来, 并将该节点及output放入hash列表中。<br>hash取值存在, 则认为该节点开头的后缀都是已经存储, 可以直接共享。我们看下hash函数取了哪些变量:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">private long hash(Builder.UnCompiledNode&lt;T&gt; node) &#123;</span><br><span class="line">  final int PRIME = 31;</span><br><span class="line">  long h = 0;</span><br><span class="line">  for (int arcIdx=0; arcIdx &lt; node.numArcs; arcIdx++) &#123;</span><br><span class="line">    final Builder.Arc&lt;T&gt; arc = node.arcs[arcIdx];</span><br><span class="line">    h = PRIME * h + arc.label; // label  i的节点边的lebal决定了i-1边的lebal</span><br><span class="line">    long n = ((Builder.CompiledNode) arc.target).node;</span><br><span class="line">    h = PRIME * h + (int) (n^(n&gt;&gt;32));</span><br><span class="line">    h = PRIME * h + arc.output.hashCode(); // output相关</span><br><span class="line">    h = PRIME * h + arc.nextFinalOutput.hashCode(); //nextFinalOutput相关</span><br><span class="line">    if (arc.isFinal) &#123;</span><br><span class="line">      h += 17;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  return h &amp; Long.MAX_VALUE;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看下, 针对单个UnCompiledNode进行hash, 取值了如下变量:所有的边lebal, 该节点后续节点的node值、output值、nextFinalOutput值。这里可能会有疑问, 下图的n2节点和n节点, hash值一样, 那么可以确定n2和n节点对应的后缀字符串相同吗?  其实与<code>arc.target.node</code>关系很大, 该函数指的是当前节点后一个节点在内存中的存储位置, 若后一个节点共享, 使用相同的存储位置, 说明后一个节点是共享, 使用相同后缀, 只要当前节点再相同 ,那么当前节点也使用相同后缀, 而达到共享存储的目的;<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_fst1.png" height="160" width="250"><br>我们先留着这个疑问, 看下CompiledNode.node值是如何计算得到的吧。<br>我们再接着了解<code>fst.addNode</code>是如何返回node的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">long addNode(Builder&lt;T&gt; builder, Builder.UnCompiledNode&lt;T&gt; nodeIn) throws IOException &#123;</span><br><span class="line">  T NO_OUTPUT = outputs.getNoOutput();</span><br><span class="line">   // 处理没有Arc的空节点</span><br><span class="line">  if (nodeIn.numArcs == 0) &#123;</span><br><span class="line">    if (nodeIn.isFinal) &#123;</span><br><span class="line">      return FINAL_END_NODE;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      return NON_FINAL_END_NODE;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 记录当前Node的Address</span><br><span class="line">  final long startAddress = builder.bytes.getPosition();</span><br><span class="line">  // 判断是否用FIXED_ARRAY方式存Arc的信息</span><br><span class="line">  final boolean doFixedArray = shouldExpand(builder, nodeIn);</span><br><span class="line">  if (doFixedArray) &#123; // 此时为false</span><br><span class="line">    if (builder.reusedBytesPerArc.length &lt; nodeIn.numArcs) &#123;</span><br><span class="line">      builder.reusedBytesPerArc = new int[ArrayUtil.oversize(nodeIn.numArcs, 1)];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  long lastArcStart = builder.bytes.getPosition();</span><br><span class="line">  int maxBytesPerArc = 0;</span><br><span class="line">  // 对该节点直连的下游遍历</span><br><span class="line">  for(int arcIdx=0; arcIdx &lt; nodeIn.numArcs; arcIdx++) &#123;</span><br><span class="line">    final Builder.Arc&lt;T&gt; arc = nodeIn.arcs[arcIdx];</span><br><span class="line">    final Builder.CompiledNode target = (Builder.CompiledNode) arc.target; // 目标节点</span><br><span class="line">    int flags = 0;</span><br><span class="line">    ......</span><br><span class="line">    builder.bytes.writeByte((byte) flags);</span><br><span class="line">    writeLabel(builder.bytes, arc.label);</span><br><span class="line">    ......</span><br><span class="line">     // 若使用相同长度, 那么记录下每条边占用的存储bytes的最大值</span><br><span class="line">    if (doFixedArray) &#123;</span><br><span class="line">      builder.reusedBytesPerArc[arcIdx] = (int) (builder.bytes.getPosition() - lastArcStart); // 记录下每个Arc的长度</span><br><span class="line">      lastArcStart = builder.bytes.getPosition();</span><br><span class="line">      maxBytesPerArc = Math.max(maxBytesPerArc, builder.reusedBytesPerArc[arcIdx]);// 记录最长的Arc的长度</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 该节点每条边占用fst存储相同的长度</span><br><span class="line">  if (doFixedArray) &#123;</span><br><span class="line">    // header(byte) + numArcs(vint) + numBytes(vint)</span><br><span class="line">    final int MAX_HEADER_SIZE = 11;</span><br><span class="line">    assert maxBytesPerArc &gt; 0;</span><br><span class="line">    int labelRange = nodeIn.arcs[nodeIn.numArcs - 1].label - nodeIn.arcs[0].label + 1;</span><br><span class="line">    // 直接赋值为false</span><br><span class="line">    boolean writeDirectly = labelRange &gt; 0 &amp;&amp; labelRange &lt; Builder.DIRECT_ARC_LOAD_FACTOR * nodeIn.numArcs;</span><br><span class="line">    writeDirectly = false;</span><br><span class="line">    byte header[] = new byte[MAX_HEADER_SIZE];</span><br><span class="line">    ByteArrayDataOutput bad = new ByteArrayDataOutput(header);</span><br><span class="line">    if (writeDirectly) &#123;</span><br><span class="line">      bad.writeByte(ARCS_AS_ARRAY_WITH_GAPS);</span><br><span class="line">      bad.writeVInt(labelRange);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">    // 写入标志位，代表ARC是FIXED_ARRAY，就是前面header长度内容</span><br><span class="line">      bad.writeByte(ARCS_AS_ARRAY_PACKED);</span><br><span class="line">      bad.writeVInt(nodeIn.numArcs);// 写入Arc的数量</span><br><span class="line">    &#125;</span><br><span class="line">    bad.writeVInt(maxBytesPerArc); // 写入最大的那个outtput</span><br><span class="line">    int headerLen = bad.getPosition();</span><br><span class="line">    final long fixedArrayStart = startAddress + headerLen;</span><br><span class="line">    // 默认为false，直接存储到fst中</span><br><span class="line">    if (writeDirectly) &#123;</span><br><span class="line">      writeArrayWithGaps(builder, nodeIn, fixedArrayStart, maxBytesPerArc, labelRange);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // 每条边按照相同的长度存储到fst</span><br><span class="line">      writeArrayPacked(builder, nodeIn, fixedArrayStart, maxBytesPerArc);</span><br><span class="line">    &#125;</span><br><span class="line">    // now write the header</span><br><span class="line">    builder.bytes.writeBytes(startAddress, header, 0, headerLen);</span><br><span class="line">  &#125;</span><br><span class="line">  当前节点在fst中存储的终点位置</span><br><span class="line">  final long thisNodeAddress = builder.bytes.getPosition()-1;</span><br><span class="line">  // 二进制数组值收尾颠倒</span><br><span class="line">  builder.bytes.reverse(startAddress, thisNodeAddress);</span><br><span class="line">  builder.nodeCount++;</span><br><span class="line">  return thisNodeAddress;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情:<br>1.将该节点的flags、label存储到fst的bytes中。<br>2.首先判断存储存储该节点每条边是否需要等长存储, 若等长存储的话, 然后再调用<code>writeArrayPacked</code>重写bytes进行等长存储。<br>3.调用<code>bytes.reverse</code>将该节点的存储byte颠倒存储。</p>
<p>当写入所有字符串后, 再调用<code>builder.finish()</code>完成了整个fst的构建。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public FST&lt;T&gt; finish() throws IOException &#123;</span><br><span class="line">  final UnCompiledNode&lt;T&gt; root = frontier[0];</span><br><span class="line">   // 参数为0, 表示将所有frontier维护的除根节点之外的所有缓存节点都写入FST</span><br><span class="line">  freezeTail(0);</span><br><span class="line">  //结束FST的写入</span><br><span class="line">  fst.finish(compileNode(root, lastInput.length()).node); // 将头也写入</span><br><span class="line">  return fst;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情:<br>1.首先调用<code>freezeTail(0)</code>将frontier除根之外的所有节点存储到fst中。<br>2.调用<code>compileNode</code>将根节点存储到fst中<br>3.通过<code>fst.finish</code>将fst的根边缓存起来。</p>
<p>我们再看下<code>fst.finish</code>是怎么缓存根节点的所有边的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">void finish(long newStartNode) throws IOException &#123;</span><br><span class="line">  startNode = newStartNode;</span><br><span class="line">  // 将bytes实际缓存的数据写入真正的存储中</span><br><span class="line">  bytes.finish();</span><br><span class="line">  cacheRootArcs(); //</span><br><span class="line">&#125;</span><br><span class="line"> private void cacheRootArcs() throws IOException &#123;</span><br><span class="line">  final Arc&lt;T&gt; arc = new Arc&lt;&gt;();</span><br><span class="line">  //读取root节点的第一条边</span><br><span class="line">  getFirstArc(arc);</span><br><span class="line">  // 边存在的话</span><br><span class="line">  if (targetHasArcs(arc)) &#123;</span><br><span class="line">    final BytesReader in = getBytesReader();</span><br><span class="line">    // 128个。第一个节点是根节点</span><br><span class="line">    Arc&lt;T&gt;[] arcs = (Arc&lt;T&gt;[]) new Arc[0x80];</span><br><span class="line">    // 从in的target处开始读取边信息，放入arc中。读取第一个节点也即根节点</span><br><span class="line">    readFirstRealTargetArc(arc.target, arc, in);</span><br><span class="line">    int count = 0;</span><br><span class="line">    while(true) &#123;</span><br><span class="line">      // 若读取lebel大于128. 就退出</span><br><span class="line">      if (arc.label &lt; arcs.length) &#123;</span><br><span class="line">        arcs[arc.label] = new Arc&lt;T&gt;().copyFrom(arc);</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        break;</span><br><span class="line">      &#125;</span><br><span class="line">      if (arc.isLast()) &#123;</span><br><span class="line">        break;</span><br><span class="line">      &#125;</span><br><span class="line">      readNextRealArc(arc, in);</span><br><span class="line">      count++;</span><br><span class="line">    &#125;</span><br><span class="line">    int cacheRAM = (int) ramBytesUsed(arcs);</span><br><span class="line">    // Don&apos;t cache if there are only a few arcs or if the cache would use &gt; 20% RAM of the FST itself:</span><br><span class="line">    if (count &gt;= FIXED_ARRAY_NUM_ARCS_SHALLOW &amp;&amp; cacheRAM &lt; ramBytesUsed()/5) &#123;</span><br><span class="line">      cachedRootArcs = arcs;</span><br><span class="line">      cachedArcsBytesUsed = cacheRAM;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下事情:<br>1.生成128的数组, root节点最多有128条边, 为什么呢? asic码规定了字母最多128个, 字符串也就最多拥有128个不同首字母。这里将字母编码成了10进制的asic码。<br>2.依次读取root节点的所有边。<br>3.若root的边小于5条, 或者缓存占用内存大于1&#x2F;5的话, 那么就没有缓存必要了。</p>
<p>最终存放在fst.bytes中的结构<a href="http://examples.mikemccandless.com/fst.py?terms=abcde%2F1%0D%0Aabdde%2F2%0D%0Aabede%2F3%0D%0Aaccde%2F4%0D%0Aadcde%2F5%0D%0A&cmd=Build+it%21" target="_blank" rel="noopener">如下</a>:<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_fst5.png" height="150" width="600"></p>
<h2 id="构建fst图示例"><a href="#构建fst图示例" class="headerlink" title="构建fst图示例"></a>构建fst图示例</h2><p>我们就以开始写入第二个字符串abdde的过程进行介绍fst的构建过程。<br>1.在第二个字符串写入之前存储如下:<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_fst2.png" height="140" width="700"><br>2.在写入abdde时,对第一个字符串freezeTail后的存储如下:<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_fst3.png" height="300" width="850"><br>3.完成abdde写入后的存储如下:<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_fst4.png" height="300" width="850"></p>
<h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><p>查询主要通过<code> Long value = Util.get(fst, new BytesRef(&quot;abcde&quot;));</code>来实现的, 我们看下该函数具体做了哪些事情:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">public static&lt;T&gt; T get(FST&lt;T&gt; fst, BytesRef input) throws IOException &#123;</span><br><span class="line">  // // 虚拟出来的一条边，它的下游是root,仅仅将根节点的存储位置放入arc中</span><br><span class="line">  final FST.Arc&lt;T&gt; arc = fst.getFirstArc(new FST.Arc&lt;T&gt;());</span><br><span class="line">  // 遍历每一个边</span><br><span class="line">  for(int i=0;i&lt;input.length;i++) &#123;</span><br><span class="line">    if (fst.findTargetArc(input.bytes[i+input.offset] &amp; 0xFF, arc, arc, fstReader) == null) &#123;</span><br><span class="line">      return null;</span><br><span class="line">    &#125;</span><br><span class="line">    // 对查找的边的output进行组合</span><br><span class="line">    output = fst.outputs.add(output, arc.output);</span><br><span class="line">  &#125;</span><br><span class="line">  if (arc.isFinal()) &#123;</span><br><span class="line">    return fst.outputs.add(output, arc.nextFinalOutput);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们主要看下针对每个字母, <code>fst.findTargetArc</code>是如何进行查找的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">// 从in中读取一个字符串labelToMatch对应的边, 该边的终点和follow指向的终点相同, 将该边放入arc中</span><br><span class="line">private Arc&lt;T&gt; findTargetArc(int labelToMatch, Arc&lt;T&gt; follow, Arc&lt;T&gt; arc, BytesReader in, boolean useRootArcCache) throws IOException &#123;</span><br><span class="line">  in.setPosition(follow.target);</span><br><span class="line">  byte flags = in.readByte();</span><br><span class="line">  // 首先判断缓存的root节点的边是否缓存了, 若缓存了。直接到对应的元素中取值</span><br><span class="line">  if (useRootArcCache &amp;&amp; cachedRootArcs != null &amp;&amp; follow.target == startNode &amp;&amp; labelToMatch &lt; cachedRootArcs.length) &#123;</span><br><span class="line">  final Arc&lt;T&gt; result = cachedRootArcs[labelToMatch];</span><br><span class="line">  // 该字母对用的</span><br><span class="line">  if (result == null) &#123;</span><br><span class="line">      return null;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      arc.copyFrom(result);</span><br><span class="line">      return arc;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  if (flags == ARCS_AS_ARRAY_WITH_GAPS) &#123;</span><br><span class="line">     ......</span><br><span class="line">  &#125; else if (flags == ARCS_AS_ARRAY_PACKED) &#123;</span><br><span class="line">  // 该节点下游的所有边被长存储</span><br><span class="line">    arc.numArcs = in.readVInt();</span><br><span class="line">    arc.bytesPerArc = in.readVInt();</span><br><span class="line">    arc.posArcsStart = in.getPosition();</span><br><span class="line">    // 节点所有的边都排好序了, 二分查找该边。</span><br><span class="line">    int low = 0;</span><br><span class="line">    int high = arc.numArcs - 1;</span><br><span class="line">    while (low &lt;= high) &#123;</span><br><span class="line">      int mid = (low + high) &gt;&gt;&gt; 1;</span><br><span class="line">      in.setPosition(arc.posArcsStart - (arc.bytesPerArc * mid + 1));</span><br><span class="line">      int midLabel = readLabel(in);</span><br><span class="line">      final int cmp = midLabel - labelToMatch;</span><br><span class="line">      if (cmp &lt; 0) &#123;</span><br><span class="line">        low = mid + 1;</span><br><span class="line">      &#125; else if (cmp &gt; 0) &#123;</span><br><span class="line">        high = mid - 1;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        arc.arcIdx = mid - 1;</span><br><span class="line">        return readNextRealArc(arc, in);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return null;</span><br><span class="line">  &#125;</span><br><span class="line">  // 首先读取根节点的第一条边, 查看是否符合查询。</span><br><span class="line">  readFirstRealTargetArc(follow.target, arc, in);</span><br><span class="line">  while(true) &#123;</span><br><span class="line">    if (arc.label == labelToMatch) &#123;</span><br><span class="line">      return arc;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // 继续读取该节点下一条边。</span><br><span class="line">      readNextRealArc(arc, in);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从该节点的所有边查找条件分4种情况:<br>1.若根缓存有数据, 那么直接从缓存中查找数据。<br>2.使用ARCS_AS_ARRAY_WITH_GAPS方式读取边。(满足等长存储的情况下, 是否直接存储。已经废弃, 可参考写入时FST类中656行, writeDirectly直接被置为false).<br>3.若该节点所有边都按照等长排序, 那么二分查找该节点下所有的边。这里存储时, 若边太多或者太深, 将使用空间换时间的方式加快查找速度, 每条边使用相同的存储空间以快速遍历。<br>4.否则线性遍历该节点下所有的边, 直到找到符合的边。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://blog.51cto.com/sbp810050504/1361551" target="_blank" rel="noopener">https://blog.51cto.com/sbp810050504/1361551</a><br><a href="http://examples.mikemccandless.com/fst.py" target="_blank" rel="noopener">http://examples.mikemccandless.com/fst.py</a></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Lucene、词典、FST</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/elasticsearch_learning/categories/Lucene//" class="article-tag-list-link color2">Lucene</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/elasticsearch_learning/2020/02/25/Lucene8-2-0底层架构-词典fst原理解析/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-Lucene底层架构-dvm-dvm构建过程" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/elasticsearch_learning/2019/11/15/Lucene底层架构-dvm-dvm构建过程/">Lucene8.2.0底层架构-dvd/dvm构建过程</a>
    </h1>
  

        
        <a href="/elasticsearch_learning/2019/11/15/Lucene底层架构-dvm-dvm构建过程/" class="archive-article-date">
  	<time datetime="2019-11-15T05:54:10.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-11-15</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>DocValue主要作用是聚合, 不对字段分词, 属于正排索引结构, 面向列式存储。   分为两部分: DocValueData(dvd)、DocValueMeta(dvm)、</p>
<h1 id="内存索引结构的建立"><a href="#内存索引结构的建立" class="headerlink" title="内存索引结构的建立"></a>内存索引结构的建立</h1><p>DocValue不对域的value分词, 而termVector会对域进行分词, 也就是说, 若设置了docValue, 那么再设置termVector将抛异常。对于每次即将刷新到文档dvd&#x2F;dvm中缓存的文档, 对于相同域名称的域的term将存放到一起, 形成列式存储, 然后再放下个域的term。 字段只有如下设置, 才会存储DocValue索引:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fieldType.setDocValuesType(DocValuesType.SORTED_SET);</span><br><span class="line">document.add(new Field(&quot;city&quot;, &quot;sssaas&quot;.getBytes(), fieldType));</span><br></pre></td></tr></table></figure>

<p>将在<code>DefaultIndexingChain.processField()</code>里面开始检查字段是否设置了该参数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DocValuesType dvType = fieldType.docValuesType();</span><br><span class="line">// docValue必须不分词的字段才行</span><br><span class="line">if (dvType != DocValuesType.NONE) &#123;</span><br><span class="line">  if (fp == null) &#123;</span><br><span class="line">  该字段初始化PerField, 本次刷新到文当前, 相同域的PerField全局共享。</span><br><span class="line">    fp = getOrAddField(fieldName, fieldType, false);</span><br><span class="line">  &#125;</span><br><span class="line">  //创建docValue，主要是为了聚合使用, 面向列的存储，一列的元素放在一个field中。</span><br><span class="line">  indexDocValue(fp, dvType, field);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>真正开始建立docValue的逻辑是在<code>indexDocValue()</code>中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">private void indexDocValue(PerField fp, DocValuesType dvType, IndexableField field) throws IOException &#123;</span><br><span class="line">    if (fp.fieldInfo.getDocValuesType() == DocValuesType.NONE) &#123;</span><br><span class="line">      // 把字段-docvalu全局存储起来</span><br><span class="line">      fieldInfos.globalFieldNumbers.setDocValuesType(fp.fieldInfo.number, fp.fieldInfo.name, dvType);</span><br><span class="line">    &#125;</span><br><span class="line">    fp.fieldInfo.setDocValuesType(dvType);</span><br><span class="line"></span><br><span class="line">    int docID = docState.docID;</span><br><span class="line"></span><br><span class="line">    switch(dvType) &#123;</span><br><span class="line">      case NUMERIC:</span><br><span class="line">        if (fp.docValuesWriter == null) &#123;</span><br><span class="line">          fp.docValuesWriter = new NumericDocValuesWriter(fp.fieldInfo, bytesUsed);</span><br><span class="line">        &#125;</span><br><span class="line">        ((NumericDocValuesWriter) fp.docValuesWriter).addValue(docID, field.numericValue().longValue());</span><br><span class="line">        break;</span><br><span class="line">      ......</span><br><span class="line">      case SORTED_SET:</span><br><span class="line">        if (fp.docValuesWriter == null) &#123; // 所有文档所有域全局唯一</span><br><span class="line">          fp.docValuesWriter = new SortedSetDocValuesWriter(fp.fieldInfo, bytesUsed);</span><br><span class="line">        &#125;</span><br><span class="line">        ((SortedSetDocValuesWriter) fp.docValuesWriter).addValue(docID, field.binaryValue());</span><br><span class="line">        break;</span><br><span class="line">      default:</span><br><span class="line">        throw new AssertionError(&quot;unrecognized DocValues.Type: &quot; + dvType);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下几件事:<br>1.将该域关于docValue的设置存储在<code>FieldInfos.docValuesType</code>中, 同时验证传递进来的文档该域docValue设置是否发生了变化。<br>2.针对不同类型的docValue, 产生不同的docValuesWriter, 然后在内存中建立docValue的索引类型。<br>docValue类型<code>DocValuesType</code>分为6种:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// 不开启docvalue时的状态，默认</span><br><span class="line">NONE,</span><br><span class="line">// 单个数值类型的docvalue主要包括（int，long，float，double）</span><br><span class="line">NUMERIC,</span><br><span class="line">// 二进制类型值对应不同的codes最大值可能超过32766字节</span><br><span class="line">BINARY,</span><br><span class="line">// 存储字符串+单值</span><br><span class="line">SORTED,</span><br><span class="line">// 存储数值类型的有序数组列表   数值或日期或枚举字段+多值</span><br><span class="line">SORTED_NUMERIC,</span><br><span class="line">//同一个文档同一个域可以存储多值域的docvalue值，但返回时，仅仅只能返回多值域的第一个docvalue</span><br><span class="line">// es针对不分词字段，默认是这个设置</span><br><span class="line">SORTED_SET,</span><br></pre></td></tr></table></figure>

<p>我们这里就以域设置:<code>SORTED_SET</code>为例进行介绍, 真正来建立docValue结构使用的是:SortedSetDocValuesWriter, 在此将对该类进行简单介绍:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">// 真正存放value值的地方，每个value都是唯一的</span><br><span class="line">final BytesRefHash hash;</span><br><span class="line">// 存储每个文档每个域termId，相同Id的算两个。在未刷新到文档前，所有文档相同域的值都会放到这里</span><br><span class="line">private PackedLongValues.Builder pending; // stream of all termIDs,</span><br><span class="line">// pendingCounts统计的是每个文档中同名字段的个数，而pending统计的是每个同名字段的termId</span><br><span class="line">private PackedLongValues.Builder pendingCounts; // termIDs per doc</span><br><span class="line">private DocsWithFieldSet docsWithField;</span><br><span class="line">private final FieldInfo fieldInfo;</span><br><span class="line">// 正在处理每个doc的field</span><br><span class="line">private int currentDoc = -1;</span><br><span class="line">// 当前文档当前域对每个value分配的一个termId，根据这个termId去hash里面映射value。为数组的原因可能是域值是数组型</span><br><span class="line">private int currentValues[] = new int[8];</span><br><span class="line"> // 当前文档当前域存放的第几个词，作为currentValues的下标。每写完一个文档的一个域，把数据转到pending中了，就清0。</span><br><span class="line">private int currentUpto;</span><br></pre></td></tr></table></figure>

<p>我们进入addValue看下是如何存储的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  // 该doc第一次写入, 说明上个文档的value已经写完了, 将上个文档给存储起来</span><br><span class="line">  if (docID != currentDoc) &#123;</span><br><span class="line">    finishCurrentDoc();</span><br><span class="line">    currentDoc = docID;</span><br><span class="line">  &#125;</span><br><span class="line">  // 存储当前文档的value存放到currentValues中</span><br><span class="line">  addOneValue(value);</span><br><span class="line">  updateBytesUsed();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了如下两件事:<br>1.检查文档ID是否与上一个一致, 若是的话, 则说明上一个文档该域已经写完了, 则调用<code>finishCurrentDoc()</code>。<br>2.将当前文档termId当前域写入currentValues<br>这两个变量将在写完一个文档时处理, 具体是在<code>finishCurrentDoc()</code>中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">private void finishCurrentDoc() &#123; // 把上个doc的值给存储起来</span><br><span class="line">    if (currentDoc == -1) &#123; // 目前是提交commit时候内存中的文档数</span><br><span class="line">      return;</span><br><span class="line">    &#125;</span><br><span class="line">    Arrays.sort(currentValues, 0, currentUpto);</span><br><span class="line">    int lastValue = -1;</span><br><span class="line">    int count = 0; // 一个文档中，同名的域有几个</span><br><span class="line">    for (int i = 0; i &lt; currentUpto; i++) &#123; // 最大只能为0</span><br><span class="line">      int termID = currentValues[i];</span><br><span class="line">      // if it&apos;s not a duplicate</span><br><span class="line">      if (termID != lastValue) &#123;</span><br><span class="line">        pending.add(termID); // record the term id</span><br><span class="line">        count++;</span><br><span class="line">      &#125;</span><br><span class="line">      lastValue = termID;</span><br><span class="line">    &#125;</span><br><span class="line">    // record the number of unique term ids for this doc</span><br><span class="line">    pendingCounts.add(count);</span><br><span class="line">    maxCount = Math.max(maxCount, count);</span><br><span class="line">    currentUpto = 0; // 这里清0了</span><br><span class="line">    docsWithField.add(currentDoc); // 存起来</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>1.首先对currentValues按照字符大小进行排序(默认一个文档一个域只有一个value, 极少数一个域多个value)，这里体现了Sort的功能，是对当前文档当前域的多个value进行排序<br>2.轮询将该域每个value的termId放入pending中, 并将域的value个数放入pendingCounts(默认每个域的value个数都是1)。<br>3.将该文档ID放入docsWithField中。<br>存储结构如下:<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_docvalue1.png" height="250" width="850"><br>其中docsWithField采取压缩方式存储所有的文档Id, 若所有文档都是连续的, 则cost&#x3D;lastCost, 此时不占用任何内存。<br>若pending、pendingCounts中存放的termId个数超过1024个, 将按照bit位数进行压缩存储, 每次压缩就是values中的一个元素:<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_docvalue2.png" height="250" width="850"></p>
<h1 id="刷新到文件"><a href="#刷新到文件" class="headerlink" title="刷新到文件"></a>刷新到文件</h1><p>本文就以每个文档每个域只有一个value进行示例介绍，为了更方面的了解dvd&#x2F;dvm结构, 先上这两个文件的结构图:<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_docvalue3.png" height="320" width="900"><br>DocValue刷新到文件的情况与fdt&#x2F;fdx(详情参考<a href="https://kkewwei.github.io/elasticsearch_learning/2019/10/29/Lucenec%E5%BA%95%E5%B1%82%E6%9E%B6%E6%9E%84-fdt-fdx%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B/">Lucene底层架构-fdt&#x2F;fdx构建过程</a>)一样:<br>1.lucene建立的索引结构占用内存超过阈值, 会在每次索引一个文档的时候检查。<br>2.用户主动通过indexWriter.flush()触发。<br>可能有些人有些疑问，我们既然知道了TermId, 那如何根据知道当前termId是属于哪个DocId的，本文以单个文档单个域只有一个value进行介绍的，那么termId就等于DocId。至于单个文档单个域内有多个value的话，在dvd中会记录每个docId里面每个field的value个数<br>本文就从DefaultIndexingChain.writeDocValues()开始介绍:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">private void writeDocValues(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException &#123;</span><br><span class="line">  DocValuesConsumer dvConsumer = null;</span><br><span class="line">  try &#123;</span><br><span class="line">    // 是个hash链表结构, segment内唯一的域</span><br><span class="line">    for (int i=0;i&lt;fieldHash.length;i++) &#123;</span><br><span class="line">      PerField perField = fieldHash[i];</span><br><span class="line">      while (perField != null) &#123;</span><br><span class="line">        if (perField.docValuesWriter != null) &#123;</span><br><span class="line">          if (dvConsumer == null) &#123;</span><br><span class="line">            // lazy init</span><br><span class="line">            DocValuesFormat fmt = state.segmentInfo.getCodec().docValuesFormat();</span><br><span class="line">            dvConsumer = fmt.fieldsConsumer(state); // PerFieldDocValuesFormat$FieldsWriter</span><br><span class="line">          &#125;</span><br><span class="line">          // 把内存缓存的数据给刷到pending中</span><br><span class="line">          if (finishedDocValues.contains(perField.fieldInfo.name) == false) &#123;</span><br><span class="line">            perField.docValuesWriter.finish(maxDoc); // 进来SortedSetDocValuesWriter.finish()</span><br><span class="line">          &#125;</span><br><span class="line">          perField.docValuesWriter.flush(state, sortMap, dvConsumer); // 要进来看下，docvalue真正向磁盘写入</span><br><span class="line">          perField.docValuesWriter = null; // 置空了</span><br><span class="line">        &#125;</span><br><span class="line">        perField = perField.next;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    success = true;</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    if (success) &#123;</span><br><span class="line">      // 向dvd/dvm写入footer并关闭文档</span><br><span class="line">      IOUtils.close(dvConsumer);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数将循环每个域,针对每个域分别处理:<br>1.调用docValuesWriter.finish()将该域内存中的数据刷新到pending中。<br>2.调用docValuesWriter.flush()将该域中的值刷新到dvd&#x2F;dvm中。<br>3.将docValuesWriter置空, 下次将在该域的写入时重新创建该对象(在内存中创建docValue索引结构的时候:<code>DefaultIndexingChain.processField()</code>会去检查)<br>4.调用IOUtils.close(dvConsumer)向文档dvd&#x2F;dvm写入footer并关闭文档。</p>
<p>我们看下docValuesWriter.flush()做了哪些事情:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">@Override</span><br><span class="line"> public void flush(SegmentWriteState state, Sorter.DocMap sortMap, DocValuesConsumer dvConsumer) throws IOException &#123;</span><br><span class="line">   final int valueCount = hash.size();</span><br><span class="line">   final PackedLongValues ords;</span><br><span class="line">   final PackedLongValues ordCounts;</span><br><span class="line">   final int[] sortedValues;</span><br><span class="line">   final int[] ordMap;</span><br><span class="line">   if (finalOrdCounts == null) &#123;</span><br><span class="line">     // 将pending中的剩余元素也压缩成一个values中的元素, 存放的是所有文档中该域的termdId</span><br><span class="line">     ords = pending.build();</span><br><span class="line">     // 存放每个文档该域value的个数, 默认都是1</span><br><span class="line">     ordCounts = pendingCounts.build();</span><br><span class="line">     // 按照每个termId对应的value进行排序, 返回的是对应termId数组</span><br><span class="line">     sortedValues = hash.sort();</span><br><span class="line">     // ordMap元素的对应的hash中的存储值就是termId的写入顺序, ordMap与sortedValues映射关系正好相反</span><br><span class="line">     ordMap = new int[valueCount];</span><br><span class="line">     for(int ord=0;ord&lt;valueCount;ord++) &#123;</span><br><span class="line">        // // dvd只存放按照字符排序的字符。比如想知道第三个写入的数据是哪个,就需要ordMap和排序好的字符数组结合获取</span><br><span class="line">       ordMap[sortedValues[ord]] = ord;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   ......</span><br><span class="line">   dvConsumer.addSortedSetField(fieldInfo,</span><br><span class="line">        new EmptyDocValuesProducer() &#123;</span><br><span class="line">           @Override</span><br><span class="line">           public SortedSetDocValues getSortedSet(FieldInfo fieldInfoIn) &#123;</span><br><span class="line">                 final SortedSetDocValues buf =  // distinct(doc词)的个数,</span><br><span class="line">                  new BufferedSortedSetDocValues(sortedValues, ordMap, hash, ords, ordCounts, maxCount, docsWithField.iterator());</span><br><span class="line">                  if (sorted == null) &#123;</span><br><span class="line">                      return buf;</span><br><span class="line">                  &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要是获取如下变量:</p>
<ul>
<li>ords: 按写入顺序存放每个文档的该域对应的termIds, 不过termIds已经被压缩了(按照占用byte空间压缩)。</li>
<li>ordCounts: 按写入顺序存放每个文档该域对应的值个数, 默认都是1。</li>
<li>sortedValues: 按照每个termId对应的value进行排序, 存放对应termId顺序</li>
<li>ordMap: 存放的是文档每个域的写入顺序, ordMap与sortedValues映射关系正好相反, 主要是为了获取term的写入顺序。<br>描述比较抽象, 我们举例说明:<br>写入顺序:        b   d    c    a<br>order:          0   1    2    3<br>sortedValues:   3   0    2    1<br>ordMap:         1   3    2    0<br>currentDoc:     1   3    2    0<br>实际存储的是排序好的数组:[a,b,c,d]。若此时我们想知道termId为3的term是啥? ordMap[3]&#x3D;0, 结合[a,b,c,d]可知, 第三个写入的是a, 也就是termId&#x3D;3的value是a。</li>
</ul>
<p>接着调用PerFieldDocValuesFormat$FieldsWriter.addSortedSetField()继续刷新:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> public void addSortedSetField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException &#123;</span><br><span class="line">  getInstance(field).addSortedSetField(field, valuesProducer);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数做了两件事情:<br>1.调用getInstance(field), 对本次刷新创建_n_Lucene80_0.dvm、_n_Lucene80_0.dvd文件。其中n代表segment编号, 0代表不止Lucene80格式一种。这两个索引文件对该segment所有域共享的, 当写完第一个域docValue索引结构, 会append第二个域的docValue索引结构。<br>2.调用Lucene80DocValuesConsumer.addSortedSetField()向域dvd、dvm文件写入docValue结构。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">public void addSortedSetField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException &#123;</span><br><span class="line">    // 域number</span><br><span class="line">    meta.writeInt(field.number);</span><br><span class="line">    // 该域docValue类型</span><br><span class="line">    meta.writeByte(Lucene80DocValuesFormat.SORTED_SET);</span><br><span class="line">    SortedSetDocValues values = valuesProducer.getSortedSet(field); // BufferedSortedSetDocValues</span><br><span class="line">    int numDocsWithField = 0;</span><br><span class="line">    long numOrds = 0;</span><br><span class="line">    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) &#123;</span><br><span class="line">      numDocsWithField++;  // 可能存在一个文档域名相同的域存在俩个</span><br><span class="line">      for (long ord = values.nextOrd(); ord != SortedSetDocValues.NO_MORE_ORDS; ord = values.nextOrd()) &#123;</span><br><span class="line">        numOrds++; // 该域有几个termId</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // 每个文档域名相同的域只有一个</span><br><span class="line">    if (numDocsWithField == numOrds) &#123; //</span><br><span class="line">      meta.writeByte((byte) 0); // multiValued (0 = singleValued)</span><br><span class="line">      doAddSortedField(field, new EmptyDocValuesProducer() &#123; //单值类型</span><br><span class="line">        @Override</span><br><span class="line">        public SortedDocValues getSorted(FieldInfo field) throws IOException &#123;</span><br><span class="line">          // 返回MaxValue</span><br><span class="line">          return SortedSetSelector.wrap(valuesProducer.getSortedSet(field), SortedSetSelector.Type.MIN);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">      return;</span><br><span class="line">    &#125;</span><br><span class="line">    // 此时是多值类型的</span><br><span class="line">    ......</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>本函数主要是向dvm存储部分字段:<br>1.fieldNumer: 字段编号<br>2.该字段的docValueType<br>3.检查该域所有文档中相同域名的域有几个, 我们仅考虑最常见的情况: 每个文档相同域名的域只有一个。在进入Lucene80DocValuesConsumer.doAddSortedField做了哪些事情:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">private void doAddSortedField(FieldInfo field, DocValuesProducer valuesProducer) throws IOException &#123;</span><br><span class="line">   //SortedSetSelector$MinValue 域中多个value, 排序后只取最小的那个。</span><br><span class="line">  SortedDocValues values = valuesProducer.getSorted(field);</span><br><span class="line">  int numDocsWithField = 0;</span><br><span class="line">  for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) &#123;</span><br><span class="line">    // 同一个文档相同域名的域可能有两个, 这里统计该域所有域的个数</span><br><span class="line">    numDocsWithField++;</span><br><span class="line">  &#125;</span><br><span class="line">  if (numDocsWithField == 0) &#123;</span><br><span class="line">    ....</span><br><span class="line">  &#125; else if (numDocsWithField == maxDoc) &#123;</span><br><span class="line">  // 每个文档都有个该字段，</span><br><span class="line">    meta.writeLong(-1); // docsWithFieldOffset</span><br><span class="line">    meta.writeLong(0L); // docsWithFieldLength</span><br><span class="line">    meta.writeShort((short) -1); // jumpTableEntryCount</span><br><span class="line">    meta.writeByte((byte) -1);   // denseRankPower</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    // 并不是每个文档都有一个该字段</span><br><span class="line">    long offset = data.getFilePointer(); </span><br><span class="line">    meta.writeLong(offset); </span><br><span class="line">    values = valuesProducer.getSorted(field);</span><br><span class="line">    final short jumpTableentryCount = IndexedDISI.writeBitSet(values, data, IndexedDISI.DEFAULT_DENSE_RANK_POWER);</span><br><span class="line">    meta.writeLong(data.getFilePointer() - offset); </span><br><span class="line">    meta.writeShort(jumpTableentryCount);</span><br><span class="line">    meta.writeByte(IndexedDISI.DEFAULT_DENSE_RANK_POWER);</span><br><span class="line">  &#125;</span><br><span class="line">  meta.writeInt(numDocsWithField); // 文档个数</span><br><span class="line">  if (values.getValueCount() &lt;= 1) &#123;</span><br><span class="line">    ......</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    int numberOfBitsPerOrd = DirectWriter.unsignedBitsRequired(values.getValueCount() - 1); // 获取的是segment内唯一的域个数</span><br><span class="line">    meta.writeByte((byte) numberOfBitsPerOrd); // bitsPerValue</span><br><span class="line">    // 获取data文档目前写入的位置</span><br><span class="line">    long start = data.getFilePointer();</span><br><span class="line">    // ordsOffset   开始从文档这里写入</span><br><span class="line">    meta.writeLong(start);</span><br><span class="line">    DirectWriter writer = DirectWriter.getInstance(data, numDocsWithField, numberOfBitsPerOrd); // DirectWriter，就是简单地长度压缩</span><br><span class="line">     //SortedSetSelector$MinValue</span><br><span class="line">    values = valuesProducer.getSorted(field);</span><br><span class="line">    for (int doc = values.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = values.nextDoc()) &#123; // 遍历这numDocsWithField</span><br><span class="line">       // dvd中存储经过排序后的每个termId, 仅仅存入缓存</span><br><span class="line">      writer.add(values.ordValue());</span><br><span class="line">    &#125;</span><br><span class="line">     // 将termId编码写入dvd中</span><br><span class="line">    writer.finish();</span><br><span class="line">    meta.writeLong(data.getFilePointer() - start); // ordsLength</span><br><span class="line">  &#125;</span><br><span class="line">  addTermsDict(DocValues.singleton(valuesProducer.getSorted(field)));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了3件事：<br>1.存储每个文档编号<br>2.向dvd中按照写入顺序写入所有的termId<br>3.对termId构建一级和二级索引</p>
<h2 id="存储doc编号"><a href="#存储doc编号" class="headerlink" title="存储doc编号"></a>存储doc编号</h2><p>存储doc使用了高效的位图存储：RoaringBitmap，相关数据结构可看<a href="https://zhuanlan.zhihu.com/p/351365841" target="_blank" rel="noopener">roaringBitMap原理</a><br>进入IndexedDISI.writeBitSet看下如何存储docId的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">static short writeBitSet(DocIdSetIterator it, IndexOutput out, byte denseRankPower) throws IOException &#123;</span><br><span class="line">  final long origo = out.getFilePointer();</span><br><span class="line">   //所有block存在多少个词</span><br><span class="line">  int totalCardinality = 0;</span><br><span class="line">  // 当前block存在多少个词</span><br><span class="line">  int blockCardinality = 0; </span><br><span class="line">  // 66636个词，稀疏矩阵存储每个文档id</span><br><span class="line">  final FixedBitSet buffer = new FixedBitSet(1&lt;&lt;16);</span><br><span class="line">  // 首先用4位来存储，实现在block之间的跳转</span><br><span class="line">  int[] jumps = new int[ArrayUtil.oversize(1, Integer.BYTES*2)]; </span><br><span class="line">  int prevBlock = -1;</span><br><span class="line">  // 当前这个block是第几个block</span><br><span class="line">  int jumpBlockIndex = 0;</span><br><span class="line">  // 文档号是递增的</span><br><span class="line">  for (int doc = it.nextDoc(); doc != DocIdSetIterator.NO_MORE_DOCS; doc = it.nextDoc()) &#123;</span><br><span class="line">     // 每个block区间docId间隔65536</span><br><span class="line">    final int block = doc &gt;&gt;&gt; 16;</span><br><span class="line">    if (prevBlock != -1 &amp;&amp; block != prevBlock) &#123;</span><br><span class="line">      // Track offset+index from previous block up to current</span><br><span class="line">      jumps = addJumps(jumps, out.getFilePointer()-origo, totalCardinality, jumpBlockIndex, prevBlock+1);</span><br><span class="line">      jumpBlockIndex = prevBlock+1;</span><br><span class="line">      // Flush block</span><br><span class="line">      flush(prevBlock, buffer, blockCardinality, denseRankPower, out);</span><br><span class="line">      // Reset for next block</span><br><span class="line">      buffer.clear(0, buffer.length());</span><br><span class="line">      totalCardinality += blockCardinality;</span><br><span class="line">      blockCardinality = 0;</span><br><span class="line">    &#125;</span><br><span class="line">    buffer.set(doc &amp; 0xFFFF);</span><br><span class="line">     // 看存在于多少个block</span><br><span class="line">    blockCardinality++;</span><br><span class="line">    prevBlock = block;</span><br><span class="line">  &#125;</span><br><span class="line">   // 当前block还有这么多文档位建立</span><br><span class="line">  if (blockCardinality &gt; 0) &#123;</span><br><span class="line">    jumps = addJumps(jumps, out.getFilePointer()-origo, totalCardinality, jumpBlockIndex, prevBlock+1);</span><br><span class="line">    totalCardinality += blockCardinality;</span><br><span class="line">    flush(prevBlock, buffer, blockCardinality, denseRankPower, out);</span><br><span class="line">    buffer.clear(0, buffer.length());</span><br><span class="line">    prevBlock++;</span><br><span class="line">  &#125;</span><br><span class="line">  ......</span><br><span class="line">  return flushBlockJumps(jumps, lastBlock+1, out, origo);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>1.按照RoaringBitmap存储规则，将文档按照固定区别划分block，区间大小为1&lt;&lt;16&#x3D;65536<br>2.存储每个block分为稠密和稀疏存储，以4096个文档为界。<br>3.使用jumps来在block之间快速跳跃，使用rank在单个block内快速查找文档。<br>4.jumps由2部分构成，前面所有block累加的文档个数；当前block写入时，dvd中文件位置。<br>5.其中每隔512个文档数（需要512&#x2F;64&#x3D;8个long）来统计一次当前总共的文档个数，称为1个rank。<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_docvalue4.png" height="280" width="1000"><br>稠密存储如上所示：其中使用FixedBitSet存储文档数，共需要longsPerRank个long来存储文档号。</p>
<p>稀疏存储如下所示：<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_docvalue5.png" height="280" width="1000"><br>1.和稠密存储的区别是，会存储每个文档的详细id，而不是rank</p>
<h2 id="按排好的序，存储termId"><a href="#按排好的序，存储termId" class="headerlink" title="按排好的序，存储termId"></a>按排好的序，存储termId</h2><p>比如第一位为9，说明第一个写入的docId在该docValue所有temd中排第8大</p>
<h2 id="对termId构建一级和二级索引"><a href="#对termId构建一级和二级索引" class="headerlink" title="对termId构建一级和二级索引"></a>对termId构建一级和二级索引</h2><p>调用Lucene80DocValuesConsumer.addTermsDict针对这些termId建立一级索引结构:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">private void addTermsDict(SortedSetDocValues values) throws IOException &#123;</span><br><span class="line">  final long size = values.getValueCount(); // 词的个数</span><br><span class="line">  meta.writeVLong(size);</span><br><span class="line">  long numBlocks = (size + Lucene80DocValuesFormat.TERMS_DICT_BLOCK_MASK) &gt;&gt;&gt; Lucene80DocValuesFormat.TERMS_DICT_BLOCK_SHIFT; // 一个block为16</span><br><span class="line">  long ord = 0;</span><br><span class="line">  long start = data.getFilePointer();</span><br><span class="line">  int maxLength = 0;</span><br><span class="line">  TermsEnum iterator = values.termsEnum(); // SortedDocValuesTermsEnum</span><br><span class="line">  // 遍历按照字母排序好的所有distinct(term)词</span><br><span class="line">  for (BytesRef term = iterator.next(); term != null; term = iterator.next()) &#123;</span><br><span class="line">    // 每16个词, 记录一次索引</span><br><span class="line">    if ((ord &amp; Lucene80DocValuesFormat.TERMS_DICT_BLOCK_MASK) == 0) &#123;</span><br><span class="line">      writer.add(data.getFilePointer() - start);</span><br><span class="line">      // 每16个词, 记录一次整个词</span><br><span class="line">      data.writeVInt(term.length);</span><br><span class="line">      data.writeBytes(term.bytes, term.offset, term.length);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      final int prefixLength = StringHelper.bytesDifference(previous.get(), term);</span><br><span class="line">      final int suffixLength = term.length - prefixLength;</span><br><span class="line">      // 相同前缀用低4位，不同的后缀用高4位。在dvd中记录每个词的与前个词比较的相同个数,不同后缀长度</span><br><span class="line">      data.writeByte((byte) (Math.min(prefixLength, 15) | (Math.min(15, suffixLength - 1) &lt;&lt; 4))); // 高4位和低4位</span><br><span class="line">     // 记录不同的后缀长度</span><br><span class="line">      data.writeBytes(term.bytes, term.offset + prefixLength, term.length - prefixLength);</span><br><span class="line">    &#125;</span><br><span class="line">    maxLength = Math.max(maxLength, term.length);</span><br><span class="line">    previous.copyBytes(term);</span><br><span class="line">    ++ord;</span><br><span class="line">  &#125;</span><br><span class="line">  writer.finish(); // 将每隔15个词在dvm中的位置给记录下来</span><br><span class="line">  ......</span><br><span class="line">  // 第2层，记录 term 字典的索引，values 是按照值 hash 排过序的，这里每 1024 条抽取一个作为索引，加速查询</span><br><span class="line">  // Now write the reverse terms index</span><br><span class="line">  writeTermsIndex(values);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要有三个作用:<br>1.将所有的term按照字符串排序的顺序写入dvd中。写入term时采用压缩方式, 通过和前一个写入的词做比较, 获取相同的前缀长度、不同的后缀长度并存储, 再存储词的后缀内容到dvd。<br>2.每隔16个词作为一级索引的元素,写入dvd中, 将此时dvd中的写入pointer写入dvm中, 以便快速定位。<br>3.调用writeTermsIndex, 对term建立第二级索引结构。<br>同时向dvm中存放如下变量:</p>
<ul>
<li>TermDictBlockShift: 提示一级索引词的间隔为16。</li>
<li>termCounts: 词的个数。</li>
</ul>
<p>我们接着看下Lucene80DocValuesConsumer.writeTermsIndex中二级索引结构是如何创建的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">// TermsDict是16个term一个索引，而 TermsIndex是1024个词一个索引结构</span><br><span class="line">private void writeTermsIndex(SortedSetDocValues values) throws IOException &#123;</span><br><span class="line">  // segment范围内所有文档相同域distinct(term)的个数</span><br><span class="line">  final long size = values.getValueCount();</span><br><span class="line">  long start = data.getFilePointer();</span><br><span class="line">  long numBlocks = 1L + ((size + Lucene80DocValuesFormat.TERMS_DICT_REVERSE_INDEX_MASK) &gt;&gt;&gt; Lucene80DocValuesFormat.TERMS_DICT_REVERSE_INDEX_SHIFT);</span><br><span class="line">  ByteBuffersDataOutput addressBuffer = new ByteBuffersDataOutput();</span><br><span class="line">  DirectMonotonicWriter writer;</span><br><span class="line">  try (ByteBuffersIndexOutput addressOutput = new ByteBuffersIndexOutput(addressBuffer, &quot;temp&quot;, &quot;temp&quot;)) &#123;</span><br><span class="line">    writer = DirectMonotonicWriter.getInstance(meta, addressOutput, numBlocks, DIRECT_MONOTONIC_BLOCK_SHIFT); // 也是使用这玩意写入数据</span><br><span class="line">    TermsEnum iterator = values.termsEnum();// SortedDocValuesTermsEnum</span><br><span class="line">    BytesRefBuilder previous = new BytesRefBuilder();</span><br><span class="line">    long offset = 0;</span><br><span class="line">    long ord = 0;</span><br><span class="line">    // 遍历按照字母排序好的所有distinct(term)词</span><br><span class="line">    for (BytesRef term = iterator.next(); term != null; term = iterator.next()) &#123;</span><br><span class="line">      //第1024*(x+1)个词建立索引</span><br><span class="line">      if ((ord &amp; Lucene80DocValuesFormat.TERMS_DICT_REVERSE_INDEX_MASK) == 0) &#123;</span><br><span class="line">      // 存储的是第二级别相同长度</span><br><span class="line">        writer.add(offset);</span><br><span class="line">        final int sortKeyLength;</span><br><span class="line">        if (ord == 0) &#123;</span><br><span class="line">          // no previous term: no bytes to write</span><br><span class="line">          sortKeyLength = 0;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          sortKeyLength = StringHelper.sortKeyLength(previous.get(), term); // 相同的前缀</span><br><span class="line">        &#125;</span><br><span class="line">        offset += sortKeyLength;</span><br><span class="line">        data.writeBytes(term.bytes, term.offset, sortKeyLength);</span><br><span class="line">      &#125; else if ((ord &amp; Lucene80DocValuesFormat.TERMS_DICT_REVERSE_INDEX_MASK) == Lucene80DocValuesFormat.TERMS_DICT_REVERSE_INDEX_MASK) &#123; // 1024*x + 1023</span><br><span class="line">        // 每次找到第1024*x+1023个词，主要是为了获取该词，为第1024*(x+1)个词找相同的前缀。</span><br><span class="line">        previous.copyBytes(term);</span><br><span class="line">      &#125;</span><br><span class="line">      ++ord;</span><br><span class="line">    &#125;</span><br><span class="line">    writer.add(offset);</span><br><span class="line">    writer.finish();</span><br><span class="line">    meta.writeLong(start);</span><br><span class="line">    meta.writeLong(data.getFilePointer() - start);</span><br><span class="line">    start = data.getFilePointer();</span><br><span class="line">    addressBuffer.copyTo(data);</span><br><span class="line">    meta.writeLong(start); // 往meta中写入</span><br><span class="line">    meta.writeLong(data.getFilePointer() - start);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要是为了构建二级索引结构, 遍历排序好的词典:<br>1.每隔1024<em>x+1023个词, 记录当前termId<br>2.每1024</em>x+1023+1个词, 将该词与前面一个词进行比较, 记录prefix长度并累加到offset, 存放在dvm中, 记录suffix内容到dvd中。<br>同时存放了如下变量: TERMS_DICT_REVERSE_INDEX_SHIFT, 提示二级索引间隔为1024个term。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>docValue作为列式存储, 不对域值分词, 所以整个域就会当成一个单独的term存储。在dvd&#x2F;dvm中, 每次将一个域的所有term存储完后, 再接着存储第二个域的所有词。dvd中主要存储termId、termValue。 dvm中存储term的一级、二级索引元素, 以便于快速找到termDict中的每个词。一级索引是每16个词,存储一个词的value, 其中前15个词由于已经排序存储, 采用相同前缀压缩存储, 词典存储可以节省很多空间。对于二级索引, 每1024个词, 取样一个节点作为索引节点。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">Lucene、DocValue</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/elasticsearch_learning/categories/Lucene//" class="article-tag-list-link color2">Lucene</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/elasticsearch_learning/2019/11/15/Lucene底层架构-dvm-dvm构建过程/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
    <article id="post-Lucenec底层架构-fdt-fdx构建过程" class="article article-type-post  article-index" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/elasticsearch_learning/2019/10/29/Lucenec底层架构-fdt-fdx构建过程/">Lucene8.2.0底层架构-fdt/fdx构建过程</a>
    </h1>
  

        
        <a href="/elasticsearch_learning/2019/10/29/Lucenec底层架构-fdt-fdx构建过程/" class="archive-article-date">
  	<time datetime="2019-10-28T23:47:47.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2019-10-29</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>StoredField文件主要存放文档-&gt;域-&gt;value的关系, 每个文档的所有域都保存一起,再保存下一行, 行式存储, 正排索引结果, 不对字段分词, 便于读取任何字段的value。 fdt存放文档的value及一级索引索引, fdx存放二级索引结构。 本文就以这两个文件结构的建立-&gt;文件写入磁盘的过程来进行详细介绍, 代码主要可见CompressingStoredFieldsWriter。</p>
<h1 id="内存索引结构的建立"><a href="#内存索引结构的建立" class="headerlink" title="内存索引结构的建立"></a>内存索引结构的建立</h1><p>索引结构建立是指将文档按照规范存储到内存中, 当对字段进行如下设置时, 才会建立该索引文档:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FieldType fieldType = new FieldType();</span><br><span class="line">fieldType.setStored(true);</span><br></pre></td></tr></table></figure>

<p>实际是在<code>DefaultIndexingChain.processField()</code>里面开始检查字段是否设置了该参数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">// Add stored fields:</span><br><span class="line">if (fieldType.stored()) &#123;</span><br><span class="line">  if (fp == null) &#123;</span><br><span class="line">    fp = getOrAddField(fieldName, fieldType, false);</span><br><span class="line">  &#125;</span><br><span class="line">  if (fieldType.stored()) &#123;</span><br><span class="line">    // 域的值</span><br><span class="line">    String value = field.stringValue();</span><br><span class="line">    try &#123;</span><br><span class="line">      // 面向行的存储，docValue是面向列的存储, field值存放在CompressingStoredFieldsWriter的bufferedDocs中</span><br><span class="line">      storedFieldsConsumer.writeField(fp.fieldInfo, field);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要做了两件事:</p>
<ol>
<li>检查该域之前文档是否已经写入过。详情可参考:</li>
<li>若设置了stored&#x3D;true, 那么最终进入到进入CompressingStoredFieldsWriter.writeField, 将字段的value缓存起来。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">public void writeField(FieldInfo info, IndexableField field) &#123;</span><br><span class="line">  ++numStoredFieldsInDoc;</span><br><span class="line">  int bits = 0;</span><br><span class="line">  Number number = field.numericValue();</span><br><span class="line">  if (number != null) &#123;</span><br><span class="line">    if (number instanceof Byte || number instanceof Short || number instanceof Integer) &#123;</span><br><span class="line">      bits = NUMERIC_INT;</span><br><span class="line">    &#125; else if (number instanceof Long) &#123;</span><br><span class="line">      bits = NUMERIC_LONG;</span><br><span class="line">    &#125;</span><br><span class="line">    ......</span><br><span class="line">    string = null;</span><br><span class="line">    bytes = null;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    bytes = field.binaryValue();</span><br><span class="line">    // 是二进制数的话</span><br><span class="line">    if (bytes != null) &#123;</span><br><span class="line">      bits = BYTE_ARR;</span><br><span class="line">      string = null;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      bits = STRING; // 是字符串的话</span><br><span class="line">      string = field.stringValue();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 低三位代表类型，高5位代表字段编号</span><br><span class="line">  final long infoAndBits = (((long) info.number) &lt;&lt; TYPE_BITS) | bits;</span><br><span class="line">  //写入字段编号，字段类型</span><br><span class="line">  bufferedDocs.writeVLong(infoAndBits);</span><br><span class="line">  if (bytes != null) &#123;</span><br><span class="line">    bufferedDocs.writeVInt(bytes.length);</span><br><span class="line">    bufferedDocs.writeBytes(bytes.bytes, bytes.offset, bytes.length);</span><br><span class="line">  &#125; else if (string != null) &#123;</span><br><span class="line">    bufferedDocs.writeString(string);</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    if (number instanceof Byte || number instanceof Short || number instanceof Integer) &#123;</span><br><span class="line">      bufferedDocs.writeZInt(number.intValue());</span><br><span class="line">    &#125; else if (number instanceof Long) &#123;</span><br><span class="line">      writeTLong(bufferedDocs, number.longValue());</span><br><span class="line">    &#125;</span><br><span class="line">    ......</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>该函数主要做了如下两件事情:<br>1.读取字段value, 判断字段类型。字段value类型分为以下五种:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">static final int         STRING = 0x00;</span><br><span class="line">static final int       BYTE_ARR = 0x01;</span><br><span class="line">static final int    NUMERIC_INT = 0x02;</span><br><span class="line">static final int  NUMERIC_FLOAT = 0x03;</span><br><span class="line">static final int   NUMERIC_LONG = 0x04;</span><br><span class="line">static final int NUMERIC_DOUBLE = 0x05;</span><br></pre></td></tr></table></figure>

<p>存储时使用3bit表示。<br>2.向内存对象bufferedDocs存储value值, bufferedDocs使用<code>byte[] bytes</code>存储数据, 内存中的建立的索引结构如下所示:<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_fdt1.png" height="160" width="900"><br>需要明确以下两点:</p>
<ul>
<li>所有文档相同字段名称的fieldNuber是一样的。</li>
<li>所有doc所有域依次按先后顺序存储起来, stored&#x3D;true构建的存储结构是按行存储的, 属于正排索引, 通过该索引结构很方便读取每个文档每个字段的值。</li>
</ul>
<h1 id="flush-x2F-产生fdt文件"><a href="#flush-x2F-产生fdt文件" class="headerlink" title="flush()&#x2F;产生fdt文件"></a>flush()&#x2F;产生fdt文件</h1><p>这里flush的主要作用是将内存中的所有文档结构生成一个chunk, 并将这些结构刷新到fdt文件中, 成为一个chunk, 并不会产生一个segment。每当完成对一个文档建立了索引后, 便会检查缓存在内存中的文档数&#x2F;bufferedDocs树是否超过阈值,超过了则会触发flush。一个chunk在内存中最大占用16KB&#x2F;128个文档, 参数在构建CompressingStoredFieldsFormat时给写死了, 详细代码可参考: DefaultIndexingChain.finishStoredFields(), 本节从CompressingStoredFieldsWriter.finishDocument开始介绍。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public void finishDocument() throws IOException &#123;</span><br><span class="line">   // this.numStoredFields存放的当前缓存的每个文档中设置了stred=true的域个数</span><br><span class="line">  if (numBufferedDocs == this.numStoredFields.length) &#123;</span><br><span class="line">    final int newLength = ArrayUtil.oversize(numBufferedDocs + 1, 4);</span><br><span class="line">    this.numStoredFields = ArrayUtil.growExact(this.numStoredFields, newLength);</span><br><span class="line">    endOffsets = ArrayUtil.growExact(endOffsets, newLength);</span><br><span class="line">  &#125;</span><br><span class="line">  // 该doc有几个域需要存储</span><br><span class="line">  this.numStoredFields[numBufferedDocs] = numStoredFieldsInDoc;</span><br><span class="line">  numStoredFieldsInDoc = 0;</span><br><span class="line">  // endOffsets存储的是该文档的内容在bufferedDocs中的位置, 便于查找每个文档所有域内容</span><br><span class="line">  endOffsets[numBufferedDocs] = bufferedDocs.getPosition();</span><br><span class="line">  // 缓存文档数+1</span><br><span class="line">  ++numBufferedDocs;</span><br><span class="line">  // 该bufferedDocs使用大小超过chunk限制16k了，或者该bufferedDocs存储文档书超过chunk限制128了。那么就该创建一个chunk</span><br><span class="line">  if (triggerFlush()) &#123;</span><br><span class="line">    // 将缓存中的文档创建一个chunk, 建立一级索引</span><br><span class="line">    flush();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该函数主要功能:</p>
<ul>
<li>缓存该文档的信息, 包括该文档中stored&#x3D;true的域个数、文档byte在bufferedDocs占用的结束位置。</li>
<li>检查建立索引缓存的文档个数&#x2F;bufferedDocs长度是否超过阈值, 超过的话, 调用flush将缓存的数据建立一个chunk, 作为一级索引。<br>我们看下flush怎么用缓存的文档产生一个chunk:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">// 两个地方会调用：1. 每次写完128/16K文档时 2.用户调用flush，若还有缓存文档，则会主动调用</span><br><span class="line">private void flush() throws IOException &#123;</span><br><span class="line">  // 该文档占用的空间  OutputStreamIndexOutput   fdx</span><br><span class="line">  indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());</span><br><span class="line">  // transform end offsets into lengths</span><br><span class="line">  final int[] lengths = endOffsets;</span><br><span class="line">  for (int i = numBufferedDocs - 1; i &gt; 0; --i) &#123;</span><br><span class="line">    // 转化每个文档的offset-&gt;文档的长度</span><br><span class="line">    lengths[i] = endOffsets[i] - endOffsets[i - 1];</span><br><span class="line">  &#125;</span><br><span class="line">  // 若存储总长度大于32kb, 就需要分段分开压缩</span><br><span class="line">  final boolean sliced = bufferedDocs.getPosition() &gt;= 2 * chunkSize;</span><br><span class="line">  // 完成numStoredFields与lengths的落盘</span><br><span class="line">  writeHeader(docBase, numBufferedDocs, numStoredFields, lengths, sliced);</span><br><span class="line">  // compress stored fields to fieldsStream</span><br><span class="line">  // 若切分的话，就切分压缩存储，一次压缩写入不能超过chunk大小（16k）</span><br><span class="line">  if (sliced) &#123;</span><br><span class="line">    // chunk太大了, 分16k分别存储</span><br><span class="line">    for (int compressed = 0; compressed &lt; bufferedDocs.getPosition(); compressed += chunkSize) &#123;</span><br><span class="line">      compressor.compress(bufferedDocs.getBytes(), compressed, Math.min(chunkSize, bufferedDocs.getPosition() - compressed), fieldsStream);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    compressor.compress(bufferedDocs.getBytes(), 0, bufferedDocs.getPosition(), fieldsStream); // LZ4FastCompressor.compress并写入了</span><br><span class="line">  &#125;</span><br><span class="line">  // reset</span><br><span class="line">  docBase += numBufferedDocs;</span><br><span class="line">  numBufferedDocs = 0;</span><br><span class="line">  bufferedDocs.reset();</span><br><span class="line">  numChunks++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>该刷新过程主要是将部分索引结构刷到fdt中, 具体事情如下:</p>
<ol>
<li>调用indexWriter.writeIndex(), 缓存该chunk的文档数、在fdt中记录的起始位置, 为fdx文件构建一级索引结构。</li>
<li>获取每个文档在fdt中的长度。</li>
<li>若内存中缓存的所有文档长度大于2*16kb, 则将bufferedDocs中的数据切分压缩存储到fdt中。</li>
<li>清空bufferedDocs中的数据。<br>fdt文件结构如下所示:<img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_fdt2.png" height="160" width="550"></li>
</ol>
<p>我们需要了解下indexWriter.writeIndex()缓存了哪些索引数据:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">void writeIndex(int numDocs, long startPointer) throws IOException &#123;</span><br><span class="line">  // 在block满的时候才会一起写入fdx</span><br><span class="line">  if (blockChunks == blockSize) &#123;</span><br><span class="line">    writeBlock(); // 足够一个block了</span><br><span class="line">    reset();</span><br><span class="line">  &#125;</span><br><span class="line">  // 只有经过writeBlock后才会置-1</span><br><span class="line">  if (firstStartPointer == -1) &#123;</span><br><span class="line">    firstStartPointer = maxStartPointer = startPointer;</span><br><span class="line">  &#125;</span><br><span class="line">  // 这个chunk的文档数</span><br><span class="line">  docBaseDeltas[blockChunks] = numDocs;</span><br><span class="line">  // fdt文件的绝对位置</span><br><span class="line">  startPointerDeltas[blockChunks] = startPointer - maxStartPointer;</span><br><span class="line">  ++blockChunks;</span><br><span class="line">  blockDocs += numDocs;</span><br><span class="line">  totalDocs += numDocs;</span><br><span class="line">  maxStartPointer = startPointer; // 当前写入的最大值</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>1.检查当前缓存的chunk个数是否达到阈值1024, 每个chunk文档数最大个数128个。若达到了, 调用writeBlock()将chunk的索引结构写入fdx中, 我们在第三节会详细介绍该函数。<br>2.记录该chunk的文档数、该chunk在fdt中的起始位置。</p>
<h1 id="刷到fdx文件"><a href="#刷到fdx文件" class="headerlink" title="刷到fdx文件"></a>刷到fdx文件</h1><p>刷新到fdx文件，只会产生一个block, block记录的是一批chunk的索引结构, 一个fdx可以存储不止一个block, 产生一个block主要有一下两种情况:<br>1.内存缓存的chunk个数超过阈值1024, 会在每次产生一个block的时候检查。<br>2.用户主动通过indexWriter.flush()触发（此时一个block的chunk数小于1024）。<br>结束一个fdx文件的写入（产生一个segment）, 有一下两种情况:<br>1.lucene建立的索引结构占用内存或者缓存文档书超过阈值, 会在每次索引一个文档的时候检查。<br>2.用户主动通过indexWriter.flush()触发。<br>针对第一种情况, 每当对一个文档建立完成后, 就会检查缓存的文档数或者lucene构建索引共占用的内存是否超过阈值, 我们可以通过如下参数设置这些阈值:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">indexWriterConfig = (IndexWriterConfig) indexWriter.getConfig();</span><br><span class="line">//统计的是距上次flush到目前内存中缓存的数据</span><br><span class="line">indexWriterConfig.setMaxBufferedDocs(201);</span><br><span class="line">indexWriterConfig.indexWriterConfig.setRAMBufferSizeMB(16)</span><br></pre></td></tr></table></figure>

<p>lucene默认内存参数设置16mb, 文档个数缓存无限制。而在es中, 默认也没有对文档个数限制, 也没有通过Lucene的内存使用来进行限制, 在ES7版本中, 对Lucene该值设置的非常大,就是为了不生效。而使用indices.memory.index_buffer_size对内存限制, 是在es层面的限制, 默认10%内存, 每次写入时都在InternalEngine中检查所有shard的内存写入占用。若超过了内存限制, 则会进行所有索引文件的刷新工作, 当然也包括fdt&#x2F;fdx文件的构建。 检查是否超过阈值的代码可参考DocumentsWriterFlushControl.doAfterDocument()。<br>实际完成一个fdx文件写入的代码可以看下StoredFieldsConsumer.flush()函数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">void flush(SegmentWriteState state, Sorter.DocMap sortMap) throws IOException &#123;</span><br><span class="line">  try &#123;</span><br><span class="line">    // CompressingStoredFieldsWriter, 将所有chunk的索引结构写入fdx。新建的索引结构写入新的fdt、fdt中</span><br><span class="line">    writer.finish(state.fieldInfos, state.segmentInfo.maxDoc());</span><br><span class="line">  &#125; finally &#123;</span><br><span class="line">    IOUtils.close(writer);</span><br><span class="line">    // 每flush一次, 就会清空writer。下次写入时产生一个新的writer</span><br><span class="line">    writer = null;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里我们可以看到一个事实: 每次由于内存满了而刷新时, CompressingStoredFieldsWriter对象就被置空。也就是说, 若因为构建索引的内存超过阈值, 那么将所有chunk组成一个block, 写入fdx文档。 之后新来的数据全部写到新的fdt及fdx文件中。<br>我们需要重点看下这里的finish做了那些事情:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public void finish(FieldInfos fis, int numDocs) throws IOException &#123;</span><br><span class="line">  // 若此时还有缓存的文档不够一个chunk文档数, 则将这些缓存的文档构建一个chunk。</span><br><span class="line">  if (numBufferedDocs &gt; 0) &#123;</span><br><span class="line">    flush();</span><br><span class="line">    // chunk不够128个文档</span><br><span class="line">    numDirtyChunks++; // incomplete: we had to force this flush</span><br><span class="line">  &#125;</span><br><span class="line">  // 向fdx写入chunk的索引结构。</span><br><span class="line">  indexWriter.finish(numDocs, fieldsStream.getFilePointer());</span><br><span class="line">  fieldsStream.writeVLong(numChunks); // fdt</span><br><span class="line">  fieldsStream.writeVLong(numDirtyChunks);</span><br><span class="line">  // 关闭fdt文件。</span><br><span class="line">  CodecUtil.writeFooter(fieldsStream);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>主要事情如下:<br>1.若内存缓存有文档(不足一个chunk), 则强制产生一个chunk。比如用户主动调用indexWriter.flush()时, 内幕才能缓存的文档就不足一个chunk。<br>2.调用indexWriter.finish()将chunk的索引结构写入fdx。<br>3.结尾fdt文件写入。</p>
<p>再接着看下fdx中的block存储了一批chunk的哪些索引结构:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">// 只有fdx/fdt文件时候,</span><br><span class="line">void finish(int numDocs, long maxPointer) throws IOException &#123;</span><br><span class="line">  // 内存中有chunk还没刷到fdx中, 则将这些chunk</span><br><span class="line">  if (blockChunks &gt; 0) &#123;</span><br><span class="line">    writeBlock();</span><br><span class="line">  &#125;</span><br><span class="line">  fieldsIndexOut.writeVInt(0); // end marker</span><br><span class="line">  fieldsIndexOut.writeVLong(maxPointer);</span><br><span class="line">  CodecUtil.writeFooter(fieldsIndexOut);</span><br><span class="line">&#125;</span><br><span class="line">private void writeBlock() throws IOException &#123;</span><br><span class="line">  assert blockChunks &gt; 0;</span><br><span class="line">  // 向fdx中写入当前block中chunk的个数</span><br><span class="line">  fieldsIndexOut.writeVInt(blockChunks);</span><br><span class="line">  // doc bases</span><br><span class="line">  final int avgChunkDocs;</span><br><span class="line">  if (blockChunks == 1) &#123;</span><br><span class="line">    avgChunkDocs = 0;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">   // 去掉最后一个chun空的主要原因是chunk可能不满128个文档</span><br><span class="line">    avgChunkDocs = Math.round((float) (blockDocs - docBaseDeltas[blockChunks - 1]) / (blockChunks - 1)); // 这个block中每个chunk的平均文档个数</span><br><span class="line">  &#125;</span><br><span class="line">  // 这个block的其实文档在整个fdt中的文档起始位置</span><br><span class="line">  fieldsIndexOut.writeVInt(totalDocs - blockDocs);</span><br><span class="line">   // 再储存这个block的chunk平均文档树</span><br><span class="line">  fieldsIndexOut.writeVInt(avgChunkDocs);</span><br><span class="line">  int docBase = 0;</span><br><span class="line">  long maxDelta = 0;</span><br><span class="line">  // 仅仅是为了获取delta的最大差值</span><br><span class="line">  for (int i = 0; i &lt; blockChunks; ++i) &#123;</span><br><span class="line">   // 与标准相差多少文档</span><br><span class="line">    final int delta = docBase - avgChunkDocs * i;</span><br><span class="line">     // 编码映射：比如-n映射成2n-1。 获取最大的误差</span><br><span class="line">    maxDelta |= zigZagEncode(delta);</span><br><span class="line">    docBase += docBaseDeltas[i];</span><br><span class="line">  &#125;</span><br><span class="line">  final int bitsPerDocBase = PackedInts.bitsRequired(maxDelta);</span><br><span class="line">   // 根据maxDelta来确定需要几位来装数</span><br><span class="line">  fieldsIndexOut.writeVInt(bitsPerDocBase);</span><br><span class="line">  PackedInts.Writer writer = PackedInts.getWriterNoHeader(fieldsIndexOut,</span><br><span class="line">      PackedInts.Format.PACKED, blockChunks, bitsPerDocBase, 1);</span><br><span class="line">  docBase = 0;</span><br><span class="line">  // 这里只存储实际文档个数与理论个数的差值</span><br><span class="line">  for (int i = 0; i &lt; blockChunks; ++i) &#123;</span><br><span class="line">    final long delta = docBase - avgChunkDocs * i;</span><br><span class="line">    assert PackedInts.bitsRequired(zigZagEncode(delta)) &lt;= writer.bitsPerValue();</span><br><span class="line">    writer.add(zigZagEncode(delta));</span><br><span class="line">    docBase += docBaseDeltas[i];</span><br><span class="line">  &#125;</span><br><span class="line">  writer.finish();</span><br><span class="line">  // 该 block 在 fdx 文件的起始位置指针</span><br><span class="line">  fieldsIndexOut.writeVLong(firstStartPointer);</span><br><span class="line">  final long avgChunkSize;</span><br><span class="line">  if (blockChunks == 1) &#123;</span><br><span class="line">    avgChunkSize = 0;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    avgChunkSize = (maxStartPointer - firstStartPointer) / (blockChunks - 1);</span><br><span class="line">  &#125;</span><br><span class="line">  fieldsIndexOut.writeVLong(avgChunkSize);</span><br><span class="line">  long startPointer = 0;</span><br><span class="line">  maxDelta = 0; // 为了找到最大的delta</span><br><span class="line">  for (int i = 0; i &lt; blockChunks; ++i) &#123;</span><br><span class="line">    startPointer += startPointerDeltas[i];</span><br><span class="line">    final long delta = startPointer - avgChunkSize * i;</span><br><span class="line">    maxDelta |= zigZagEncode(delta);</span><br><span class="line">  &#125;</span><br><span class="line">  final int bitsPerStartPointer = PackedInts.bitsRequired(maxDelta);</span><br><span class="line">  fieldsIndexOut.writeVInt(bitsPerStartPointer); // fdx</span><br><span class="line">  writer = PackedInts.getWriterNoHeader(fieldsIndexOut, PackedInts.Format.PACKED,</span><br><span class="line">      blockChunks, bitsPerStartPointer, 1);</span><br><span class="line">  startPointer = 0;</span><br><span class="line">  for (int i = 0; i &lt; blockChunks; ++i) &#123;</span><br><span class="line">    startPointer += startPointerDeltas[i];</span><br><span class="line">    final long delta = startPointer - avgChunkSize * i;</span><br><span class="line">    assert PackedInts.bitsRequired(zigZagEncode(delta)) &lt;= writer.bitsPerValue();</span><br><span class="line">    writer.add(zigZagEncode(delta));</span><br><span class="line">  &#125;</span><br><span class="line">  writer.finish();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>writeBlock主要记录以下几个指标:<br>1.blockChunks: 当前block的chunks数。<br>2.docBase: 这个block的其实文档在整个fdt中的文档起始位置, 一个fdt可能存放不只一个block。<br>3.avgChunkDocs: 该block所有chunk的平均文档数<br>4.bitsPerDocBase: 每个chunk文档书与平均文档数相差个数需要几位bit表示: 比如chunk文档数分别是: 5,3,8, 则需要1位。 该参数是为了保存他们的差值, 减少存储所需要的空间。<br>5.blockChunks个docBaseDelta: 存储blockChunks个chunk与标准文档的差值, 这里不存储每个文档的原值, 而是存储差值, 是为了减少存储所需的空间。<br>6.firstStartPointer: 该block在整个fdt存储的起始位置<br>7.avgChunksSize: 该block所有chunk的平均length<br>8.blockChunks个startPointDelta: 存储blockChunks个chunk与标准size的差值。 通过该值可以知道每个chunk在fdt中的起始位置, 存储原理与第5项一致。<br>9.numberChunks: 所有block中总的chunk个数。<br>10.numberDirtyChunks: fdx可能存放多个block, 一个block最多存放1024个chunk, 该值统计的是所有block的chunk小于1024的个数, 其中有的chunk可能不够128个文档, 称之为dirtyChunk。一般fdx文件中, 只有最后一个block的最后一个chunk小于128个文档。<br>下图展示了fdt文件和fdx文件的结构:<br><img src="https://kkewwei.github.io/elasticsearch_learning/img/lucene_fdt.png" height="350" width="900"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>整体过程可以总结如下: 依次将文档写入内存中, 若缓存的文档数或者bufferedDocs占用的内存超过阈值, 则触发一次flush, 该flush将之前的存储在内存中的文档产生一个chunk, 当达到blockSize次flush后, 产生一个block。当达到一个block后, 才开始向fdt文件和fdx文件写入索引结构。一个fdx可能包含多个block的索引结构。</p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color4">Lucene、StoredField</a>
        		</li>
      		
		</ul>
	</div>

      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/elasticsearch_learning/categories/Lucene//" class="article-tag-list-link color2">Lucene</a>
        		</li>
      		
		</ul>
	</div>


      
        <p class="article-more-link">
          <a class="article-more-a" href="/elasticsearch_learning/2019/10/29/Lucenec底层架构-fdt-fdx构建过程/">展开全文 >></a>
        </p>
      

      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

<aside class="wrap-side-operation">
    <div class="mod-side-operation">
        
        <div class="jump-container" id="js-jump-container" style="display:none;">
            <a href="javascript:void(0)" class="mod-side-operation__jump-to-top">
                <i class="icon-font icon-back"></i>
            </a>
            <div id="js-jump-plan-container" class="jump-plan-container" style="top: -11px;">
                <i class="icon-font icon-plane jump-plane"></i>
            </div>
        </div>
        
        
    </div>
</aside>




  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/elasticsearch_learning/page/2/">2</a><a class="page-number" href="/elasticsearch_learning/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/elasticsearch_learning/page/6/">6</a><a class="extend next" rel="next" href="/elasticsearch_learning/page/2/">Next &raquo;</a>
    </nav>
  


          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2022 jianguo
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: true,
		toc_hide_index: true,
		root: "/elasticsearch_learning/",
		innerArchive: true,
		showTags: false
	}
</script>

<script>!function(t){function n(e){if(r[e])return r[e].exports;var i=r[e]={exports:{},id:e,loaded:!1};return t[e].call(i.exports,i,i.exports,n),i.loaded=!0,i.exports}var r={};n.m=t,n.c=r,n.p="./",n(0)}([function(t,n,r){r(195),t.exports=r(191)},function(t,n,r){var e=r(3),i=r(52),o=r(27),u=r(28),c=r(53),f="prototype",a=function(t,n,r){var s,l,h,v,p=t&a.F,d=t&a.G,y=t&a.S,g=t&a.P,b=t&a.B,m=d?e:y?e[n]||(e[n]={}):(e[n]||{})[f],x=d?i:i[n]||(i[n]={}),w=x[f]||(x[f]={});d&&(r=n);for(s in r)l=!p&&m&&void 0!==m[s],h=(l?m:r)[s],v=b&&l?c(h,e):g&&"function"==typeof h?c(Function.call,h):h,m&&u(m,s,h,t&a.U),x[s]!=h&&o(x,s,v),g&&w[s]!=h&&(w[s]=h)};e.core=i,a.F=1,a.G=2,a.S=4,a.P=8,a.B=16,a.W=32,a.U=64,a.R=128,t.exports=a},function(t,n,r){var e=r(6);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n){var r=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=r)},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n,r){var e=r(126)("wks"),i=r(76),o=r(3).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n,r){var e=r(94),i=r(33);t.exports=function(t){return e(i(t))}},function(t,n,r){t.exports=!r(4)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(2),i=r(167),o=r(50),u=Object.defineProperty;n.f=r(10)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){t.exports=!r(18)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(14),i=r(22);t.exports=r(12)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(20),i=r(58),o=r(42),u=Object.defineProperty;n.f=r(12)?Object.defineProperty:function(t,n,r){if(e(t),n=o(n,!0),e(r),i)try{return u(t,n,r)}catch(t){}if("get"in r||"set"in r)throw TypeError("Accessors not supported!");return"value"in r&&(t[n]=r.value),t}},function(t,n,r){var e=r(40)("wks"),i=r(23),o=r(5).Symbol,u="function"==typeof o;(t.exports=function(t){return e[t]||(e[t]=u&&o[t]||(u?o:i)("Symbol."+t))}).store=e},function(t,n,r){var e=r(67),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){var e=r(46);t.exports=function(t){return Object(e(t))}},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,r){var e=r(63),i=r(34);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(21);t.exports=function(t){if(!e(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n){var r={}.hasOwnProperty;t.exports=function(t,n){return r.call(t,n)}},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n,r){var e=r(11),i=r(66);t.exports=r(10)?function(t,n,r){return e.f(t,n,i(1,r))}:function(t,n,r){return t[n]=r,t}},function(t,n,r){var e=r(3),i=r(27),o=r(24),u=r(76)("src"),c="toString",f=Function[c],a=(""+f).split(c);r(52).inspectSource=function(t){return f.call(t)},(t.exports=function(t,n,r,c){var f="function"==typeof r;f&&(o(r,"name")||i(r,"name",n)),t[n]!==r&&(f&&(o(r,u)||i(r,u,t[n]?""+t[n]:a.join(String(n)))),t===e?t[n]=r:c?t[n]?t[n]=r:i(t,n,r):(delete t[n],i(t,n,r)))})(Function.prototype,c,function(){return"function"==typeof this&&this[u]||f.call(this)})},function(t,n,r){var e=r(1),i=r(4),o=r(46),u=function(t,n,r,e){var i=String(o(t)),u="<"+n;return""!==r&&(u+=" "+r+'="'+String(e).replace(/"/g,"&quot;")+'"'),u+">"+i+"</"+n+">"};t.exports=function(t,n){var r={};r[t]=n(u),e(e.P+e.F*i(function(){var n=""[t]('"');return n!==n.toLowerCase()||n.split('"').length>3}),"String",r)}},function(t,n,r){var e=r(115),i=r(46);t.exports=function(t){return e(i(t))}},function(t,n,r){var e=r(116),i=r(66),o=r(30),u=r(50),c=r(24),f=r(167),a=Object.getOwnPropertyDescriptor;n.f=r(10)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(24),i=r(17),o=r(145)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(14).f,i=r(8),o=r(15)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(40)("keys"),i=r(23);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(5),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n,r){var e=r(21);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(36),u=r(44),c=r(14).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){n.f=r(15)},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n,r){var e=r(4);t.exports=function(t,n){return!!t&&e(function(){n?t.call(null,function(){},1):t.call(null)})}},function(t,n,r){var e=r(53),i=r(115),o=r(17),u=r(16),c=r(203);t.exports=function(t,n){var r=1==t,f=2==t,a=3==t,s=4==t,l=6==t,h=5==t||l,v=n||c;return function(n,c,p){for(var d,y,g=o(n),b=i(g),m=e(c,p,3),x=u(b.length),w=0,S=r?v(n,x):f?v(n,0):void 0;x>w;w++)if((h||w in b)&&(d=b[w],y=m(d,w,g),t))if(r)S[w]=y;else if(y)switch(t){case 3:return!0;case 5:return d;case 6:return w;case 2:S.push(d)}else if(s)return!1;return l?-1:a||s?s:S}}},function(t,n,r){var e=r(1),i=r(52),o=r(4);t.exports=function(t,n){var r=(i.Object||{})[t]||Object[t],u={};u[t]=n(r),e(e.S+e.F*o(function(){r(1)}),"Object",u)}},function(t,n,r){var e=r(6);t.exports=function(t,n){if(!e(t))return t;var r,i;if(n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;if("function"==typeof(r=t.valueOf)&&!e(i=r.call(t)))return i;if(!n&&"function"==typeof(r=t.toString)&&!e(i=r.call(t)))return i;throw TypeError("Can't convert object to primitive value")}},function(t,n,r){var e=r(5),i=r(25),o=r(91),u=r(13),c="prototype",f=function(t,n,r){var a,s,l,h=t&f.F,v=t&f.G,p=t&f.S,d=t&f.P,y=t&f.B,g=t&f.W,b=v?i:i[n]||(i[n]={}),m=b[c],x=v?e:p?e[n]:(e[n]||{})[c];v&&(r=n);for(a in r)(s=!h&&x&&void 0!==x[a])&&a in b||(l=s?x[a]:r[a],b[a]=v&&"function"!=typeof x[a]?r[a]:y&&s?o(l,e):g&&x[a]==l?function(t){var n=function(n,r,e){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,r)}return new t(n,r,e)}return t.apply(this,arguments)};return n[c]=t[c],n}(l):d&&"function"==typeof l?o(Function.call,l):l,d&&((b.virtual||(b.virtual={}))[a]=l,t&f.R&&m&&!m[a]&&u(m,a,l)))};f.F=1,f.G=2,f.S=4,f.P=8,f.B=16,f.W=32,f.U=64,f.R=128,t.exports=f},function(t,n){var r=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=r)},function(t,n,r){var e=r(26);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(183),i=r(1),o=r(126)("metadata"),u=o.store||(o.store=new(r(186))),c=function(t,n,r){var i=u.get(t);if(!i){if(!r)return;u.set(t,i=new e)}var o=i.get(n);if(!o){if(!r)return;i.set(n,o=new e)}return o},f=function(t,n,r){var e=c(n,r,!1);return void 0!==e&&e.has(t)},a=function(t,n,r){var e=c(n,r,!1);return void 0===e?void 0:e.get(t)},s=function(t,n,r,e){c(r,e,!0).set(t,n)},l=function(t,n){var r=c(t,n,!1),e=[];return r&&r.forEach(function(t,n){e.push(n)}),e},h=function(t){return void 0===t||"symbol"==typeof t?t:String(t)},v=function(t){i(i.S,"Reflect",t)};t.exports={store:u,map:c,has:f,get:a,set:s,keys:l,key:h,exp:v}},function(t,n,r){"use strict";if(r(10)){var e=r(69),i=r(3),o=r(4),u=r(1),c=r(127),f=r(152),a=r(53),s=r(68),l=r(66),h=r(27),v=r(73),p=r(67),d=r(16),y=r(75),g=r(50),b=r(24),m=r(180),x=r(114),w=r(6),S=r(17),_=r(137),O=r(70),E=r(32),P=r(71).f,j=r(154),F=r(76),M=r(7),A=r(48),N=r(117),T=r(146),I=r(155),k=r(80),L=r(123),R=r(74),C=r(130),D=r(160),U=r(11),W=r(31),G=U.f,B=W.f,V=i.RangeError,z=i.TypeError,q=i.Uint8Array,K="ArrayBuffer",J="Shared"+K,Y="BYTES_PER_ELEMENT",H="prototype",$=Array[H],X=f.ArrayBuffer,Q=f.DataView,Z=A(0),tt=A(2),nt=A(3),rt=A(4),et=A(5),it=A(6),ot=N(!0),ut=N(!1),ct=I.values,ft=I.keys,at=I.entries,st=$.lastIndexOf,lt=$.reduce,ht=$.reduceRight,vt=$.join,pt=$.sort,dt=$.slice,yt=$.toString,gt=$.toLocaleString,bt=M("iterator"),mt=M("toStringTag"),xt=F("typed_constructor"),wt=F("def_constructor"),St=c.CONSTR,_t=c.TYPED,Ot=c.VIEW,Et="Wrong length!",Pt=A(1,function(t,n){return Tt(T(t,t[wt]),n)}),jt=o(function(){return 1===new q(new Uint16Array([1]).buffer)[0]}),Ft=!!q&&!!q[H].set&&o(function(){new q(1).set({})}),Mt=function(t,n){if(void 0===t)throw z(Et);var r=+t,e=d(t);if(n&&!m(r,e))throw V(Et);return e},At=function(t,n){var r=p(t);if(r<0||r%n)throw V("Wrong offset!");return r},Nt=function(t){if(w(t)&&_t in t)return t;throw z(t+" is not a typed array!")},Tt=function(t,n){if(!(w(t)&&xt in t))throw z("It is not a typed array constructor!");return new t(n)},It=function(t,n){return kt(T(t,t[wt]),n)},kt=function(t,n){for(var r=0,e=n.length,i=Tt(t,e);e>r;)i[r]=n[r++];return i},Lt=function(t,n,r){G(t,n,{get:function(){return this._d[r]}})},Rt=function(t){var n,r,e,i,o,u,c=S(t),f=arguments.length,s=f>1?arguments[1]:void 0,l=void 0!==s,h=j(c);if(void 0!=h&&!_(h)){for(u=h.call(c),e=[],n=0;!(o=u.next()).done;n++)e.push(o.value);c=e}for(l&&f>2&&(s=a(s,arguments[2],2)),n=0,r=d(c.length),i=Tt(this,r);r>n;n++)i[n]=l?s(c[n],n):c[n];return i},Ct=function(){for(var t=0,n=arguments.length,r=Tt(this,n);n>t;)r[t]=arguments[t++];return r},Dt=!!q&&o(function(){gt.call(new q(1))}),Ut=function(){return gt.apply(Dt?dt.call(Nt(this)):Nt(this),arguments)},Wt={copyWithin:function(t,n){return D.call(Nt(this),t,n,arguments.length>2?arguments[2]:void 0)},every:function(t){return rt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},fill:function(t){return C.apply(Nt(this),arguments)},filter:function(t){return It(this,tt(Nt(this),t,arguments.length>1?arguments[1]:void 0))},find:function(t){return et(Nt(this),t,arguments.length>1?arguments[1]:void 0)},findIndex:function(t){return it(Nt(this),t,arguments.length>1?arguments[1]:void 0)},forEach:function(t){Z(Nt(this),t,arguments.length>1?arguments[1]:void 0)},indexOf:function(t){return ut(Nt(this),t,arguments.length>1?arguments[1]:void 0)},includes:function(t){return ot(Nt(this),t,arguments.length>1?arguments[1]:void 0)},join:function(t){return vt.apply(Nt(this),arguments)},lastIndexOf:function(t){return st.apply(Nt(this),arguments)},map:function(t){return Pt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},reduce:function(t){return lt.apply(Nt(this),arguments)},reduceRight:function(t){return ht.apply(Nt(this),arguments)},reverse:function(){for(var t,n=this,r=Nt(n).length,e=Math.floor(r/2),i=0;i<e;)t=n[i],n[i++]=n[--r],n[r]=t;return n},some:function(t){return nt(Nt(this),t,arguments.length>1?arguments[1]:void 0)},sort:function(t){return pt.call(Nt(this),t)},subarray:function(t,n){var r=Nt(this),e=r.length,i=y(t,e);return new(T(r,r[wt]))(r.buffer,r.byteOffset+i*r.BYTES_PER_ELEMENT,d((void 0===n?e:y(n,e))-i))}},Gt=function(t,n){return It(this,dt.call(Nt(this),t,n))},Bt=function(t){Nt(this);var n=At(arguments[1],1),r=this.length,e=S(t),i=d(e.length),o=0;if(i+n>r)throw V(Et);for(;o<i;)this[n+o]=e[o++]},Vt={entries:function(){return at.call(Nt(this))},keys:function(){return ft.call(Nt(this))},values:function(){return ct.call(Nt(this))}},zt=function(t,n){return w(t)&&t[_t]&&"symbol"!=typeof n&&n in t&&String(+n)==String(n)},qt=function(t,n){return zt(t,n=g(n,!0))?l(2,t[n]):B(t,n)},Kt=function(t,n,r){return!(zt(t,n=g(n,!0))&&w(r)&&b(r,"value"))||b(r,"get")||b(r,"set")||r.configurable||b(r,"writable")&&!r.writable||b(r,"enumerable")&&!r.enumerable?G(t,n,r):(t[n]=r.value,t)};St||(W.f=qt,U.f=Kt),u(u.S+u.F*!St,"Object",{getOwnPropertyDescriptor:qt,defineProperty:Kt}),o(function(){yt.call({})})&&(yt=gt=function(){return vt.call(this)});var Jt=v({},Wt);v(Jt,Vt),h(Jt,bt,Vt.values),v(Jt,{slice:Gt,set:Bt,constructor:function(){},toString:yt,toLocaleString:Ut}),Lt(Jt,"buffer","b"),Lt(Jt,"byteOffset","o"),Lt(Jt,"byteLength","l"),Lt(Jt,"length","e"),G(Jt,mt,{get:function(){return this[_t]}}),t.exports=function(t,n,r,f){f=!!f;var a=t+(f?"Clamped":"")+"Array",l="Uint8Array"!=a,v="get"+t,p="set"+t,y=i[a],g=y||{},b=y&&E(y),m=!y||!c.ABV,S={},_=y&&y[H],j=function(t,r){var e=t._d;return e.v[v](r*n+e.o,jt)},F=function(t,r,e){var i=t._d;f&&(e=(e=Math.round(e))<0?0:e>255?255:255&e),i.v[p](r*n+i.o,e,jt)},M=function(t,n){G(t,n,{get:function(){return j(this,n)},set:function(t){return F(this,n,t)},enumerable:!0})};m?(y=r(function(t,r,e,i){s(t,y,a,"_d");var o,u,c,f,l=0,v=0;if(w(r)){if(!(r instanceof X||(f=x(r))==K||f==J))return _t in r?kt(y,r):Rt.call(y,r);o=r,v=At(e,n);var p=r.byteLength;if(void 0===i){if(p%n)throw V(Et);if((u=p-v)<0)throw V(Et)}else if((u=d(i)*n)+v>p)throw V(Et);c=u/n}else c=Mt(r,!0),u=c*n,o=new X(u);for(h(t,"_d",{b:o,o:v,l:u,e:c,v:new Q(o)});l<c;)M(t,l++)}),_=y[H]=O(Jt),h(_,"constructor",y)):L(function(t){new y(null),new y(t)},!0)||(y=r(function(t,r,e,i){s(t,y,a);var o;return w(r)?r instanceof X||(o=x(r))==K||o==J?void 0!==i?new g(r,At(e,n),i):void 0!==e?new g(r,At(e,n)):new g(r):_t in r?kt(y,r):Rt.call(y,r):new g(Mt(r,l))}),Z(b!==Function.prototype?P(g).concat(P(b)):P(g),function(t){t in y||h(y,t,g[t])}),y[H]=_,e||(_.constructor=y));var A=_[bt],N=!!A&&("values"==A.name||void 0==A.name),T=Vt.values;h(y,xt,!0),h(_,_t,a),h(_,Ot,!0),h(_,wt,y),(f?new y(1)[mt]==a:mt in _)||G(_,mt,{get:function(){return a}}),S[a]=y,u(u.G+u.W+u.F*(y!=g),S),u(u.S,a,{BYTES_PER_ELEMENT:n,from:Rt,of:Ct}),Y in _||h(_,Y,n),u(u.P,a,Wt),R(a),u(u.P+u.F*Ft,a,{set:Bt}),u(u.P+u.F*!N,a,Vt),u(u.P+u.F*(_.toString!=yt),a,{toString:yt}),u(u.P+u.F*o(function(){new y(1).slice()}),a,{slice:Gt}),u(u.P+u.F*(o(function(){return[1,2].toLocaleString()!=new y([1,2]).toLocaleString()})||!o(function(){_.toLocaleString.call([1,2])})),a,{toLocaleString:Ut}),k[a]=N?A:T,e||N||h(_,bt,T)}}else t.exports=function(){}},function(t,n){var r={}.toString;t.exports=function(t){return r.call(t).slice(8,-1)}},function(t,n,r){var e=r(21),i=r(5).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n,r){t.exports=!r(12)&&!r(18)(function(){return 7!=Object.defineProperty(r(57)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){"use strict";var e=r(36),i=r(51),o=r(64),u=r(13),c=r(8),f=r(35),a=r(96),s=r(38),l=r(103),h=r(15)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n,r){var e=r(20),i=r(100),o=r(34),u=r(39)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(57)("iframe"),e=o.length;for(n.style.display="none",r(93).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(63),i=r(34).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(8),i=r(9),o=r(90)(!1),u=r(39)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){t.exports=r(13)},function(t,n,r){var e=r(76)("meta"),i=r(6),o=r(24),u=r(11).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(4)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var r=Math.ceil,e=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?e:r)(t)}},function(t,n){t.exports=function(t,n,r,e){if(!(t instanceof n)||void 0!==e&&e in t)throw TypeError(r+": incorrect invocation!");return t}},function(t,n){t.exports=!1},function(t,n,r){var e=r(2),i=r(173),o=r(133),u=r(145)("IE_PROTO"),c=function(){},f="prototype",a=function(){var t,n=r(132)("iframe"),e=o.length;for(n.style.display="none",r(135).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write("<script>document.F=Object<\/script>"),t.close(),a=t.F;e--;)delete a[f][o[e]];return a()};t.exports=Object.create||function(t,n){var r;return null!==t?(c[f]=e(t),r=new c,c[f]=null,r[u]=t):r=a(),void 0===n?r:i(r,n)}},function(t,n,r){var e=r(175),i=r(133).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return e(t,i)}},function(t,n,r){var e=r(175),i=r(133);t.exports=Object.keys||function(t){return e(t,i)}},function(t,n,r){var e=r(28);t.exports=function(t,n,r){for(var i in n)e(t,i,n[i],r);return t}},function(t,n,r){"use strict";var e=r(3),i=r(11),o=r(10),u=r(7)("species");t.exports=function(t){var n=e[t];o&&n&&!n[u]&&i.f(n,u,{configurable:!0,get:function(){return this}})}},function(t,n,r){var e=r(67),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n){var r=0,e=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++r+e).toString(36))}},function(t,n,r){var e=r(33);t.exports=function(t){return Object(e(t))}},function(t,n,r){var e=r(7)("unscopables"),i=Array.prototype;void 0==i[e]&&r(27)(i,e,{}),t.exports=function(t){i[e][t]=!0}},function(t,n,r){var e=r(53),i=r(169),o=r(137),u=r(2),c=r(16),f=r(154),a={},s={},n=t.exports=function(t,n,r,l,h){var v,p,d,y,g=h?function(){return t}:f(t),b=e(r,l,n?2:1),m=0;if("function"!=typeof g)throw TypeError(t+" is not iterable!");if(o(g)){for(v=c(t.length);v>m;m++)if((y=n?b(u(p=t[m])[0],p[1]):b(t[m]))===a||y===s)return y}else for(d=g.call(t);!(p=d.next()).done;)if((y=i(d,b,p.value,n))===a||y===s)return y};n.BREAK=a,n.RETURN=s},function(t,n){t.exports={}},function(t,n,r){var e=r(11).f,i=r(24),o=r(7)("toStringTag");t.exports=function(t,n,r){t&&!i(t=r?t:t.prototype,o)&&e(t,o,{configurable:!0,value:n})}},function(t,n,r){var e=r(1),i=r(46),o=r(4),u=r(150),c="["+u+"]",f="​",a=RegExp("^"+c+c+"*"),s=RegExp(c+c+"*$"),l=function(t,n,r){var i={},c=o(function(){return!!u[t]()||f[t]()!=f}),a=i[t]=c?n(h):u[t];r&&(i[r]=a),e(e.P+e.F*c,"String",i)},h=l.trim=function(t,n){return t=String(i(t)),1&n&&(t=t.replace(a,"")),2&n&&(t=t.replace(s,"")),t};t.exports=l},function(t,n,r){t.exports={default:r(86),__esModule:!0}},function(t,n,r){t.exports={default:r(87),__esModule:!0}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var i=r(84),o=e(i),u=r(83),c=e(u),f="function"==typeof c.default&&"symbol"==typeof o.default?function(t){return typeof t}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":typeof t};n.default="function"==typeof c.default&&"symbol"===f(o.default)?function(t){return void 0===t?"undefined":f(t)}:function(t){return t&&"function"==typeof c.default&&t.constructor===c.default&&t!==c.default.prototype?"symbol":void 0===t?"undefined":f(t)}},function(t,n,r){r(110),r(108),r(111),r(112),t.exports=r(25).Symbol},function(t,n,r){r(109),r(113),t.exports=r(44).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,r){var e=r(9),i=r(106),o=r(105);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){var e=r(88);t.exports=function(t,n,r){if(e(t),void 0===n)return t;switch(r){case 1:return function(r){return t.call(n,r)};case 2:return function(r,e){return t.call(n,r,e)};case 3:return function(r,e,i){return t.call(n,r,e,i)}}return function(){return t.apply(n,arguments)}}},function(t,n,r){var e=r(19),i=r(62),o=r(37);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){t.exports=r(5).document&&document.documentElement},function(t,n,r){var e=r(56);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n,r){var e=r(56);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(60),i=r(22),o=r(38),u={};r(13)(u,r(15)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,r){var e=r(19),i=r(9);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){var e=r(23)("meta"),i=r(21),o=r(8),u=r(14).f,c=0,f=Object.isExtensible||function(){return!0},a=!r(18)(function(){return f(Object.preventExtensions({}))}),s=function(t){u(t,e,{value:{i:"O"+ ++c,w:{}}})},l=function(t,n){if(!i(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!o(t,e)){if(!f(t))return"F";if(!n)return"E";s(t)}return t[e].i},h=function(t,n){if(!o(t,e)){if(!f(t))return!0;if(!n)return!1;s(t)}return t[e].w},v=function(t){return a&&p.NEED&&f(t)&&!o(t,e)&&s(t),t},p=t.exports={KEY:e,NEED:!1,fastKey:l,getWeak:h,onFreeze:v}},function(t,n,r){var e=r(14),i=r(20),o=r(19);t.exports=r(12)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(37),i=r(22),o=r(9),u=r(42),c=r(8),f=r(58),a=Object.getOwnPropertyDescriptor;n.f=r(12)?a:function(t,n){if(t=o(t),n=u(n,!0),f)try{return a(t,n)}catch(t){}if(c(t,n))return i(!e.f.call(t,n),t[n])}},function(t,n,r){var e=r(9),i=r(61).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(8),i=r(77),o=r(39)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=i(t),e(t,o)?t[o]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,r){var e=r(41),i=r(33);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(41),i=Math.max,o=Math.min;t.exports=function(t,n){return t=e(t),t<0?i(t+n,0):o(t,n)}},function(t,n,r){var e=r(41),i=Math.min;t.exports=function(t){return t>0?i(e(t),9007199254740991):0}},function(t,n,r){"use strict";var e=r(89),i=r(97),o=r(35),u=r(9);t.exports=r(59)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){},function(t,n,r){"use strict";var e=r(104)(!0);r(59)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";var e=r(5),i=r(8),o=r(12),u=r(51),c=r(64),f=r(99).KEY,a=r(18),s=r(40),l=r(38),h=r(23),v=r(15),p=r(44),d=r(43),y=r(98),g=r(92),b=r(95),m=r(20),x=r(9),w=r(42),S=r(22),_=r(60),O=r(102),E=r(101),P=r(14),j=r(19),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(61).f=O.f=Z,r(37).f=X,r(62).f=tt,o&&!r(36)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(13)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){r(43)("asyncIterator")},function(t,n,r){r(43)("observable")},function(t,n,r){r(107);for(var e=r(5),i=r(13),o=r(35),u=r(15)("toStringTag"),c=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],f=0;f<5;f++){var a=c[f],s=e[a],l=s&&s.prototype;l&&!l[u]&&i(l,u,a),o[a]=o.Array}},function(t,n,r){var e=r(45),i=r(7)("toStringTag"),o="Arguments"==e(function(){return arguments}()),u=function(t,n){try{return t[n]}catch(t){}};t.exports=function(t){var n,r,c;return void 0===t?"Undefined":null===t?"Null":"string"==typeof(r=u(n=Object(t),i))?r:o?e(n):"Object"==(c=e(n))&&"function"==typeof n.callee?"Arguments":c}},function(t,n,r){var e=r(45);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==e(t)?t.split(""):Object(t)}},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,r){var e=r(30),i=r(16),o=r(75);t.exports=function(t){return function(n,r,u){var c,f=e(n),a=i(f.length),s=o(u,a);if(t&&r!=r){for(;a>s;)if((c=f[s++])!=c)return!0}else for(;a>s;s++)if((t||s in f)&&f[s]===r)return t||s||0;return!t&&-1}}},function(t,n,r){"use strict";var e=r(3),i=r(1),o=r(28),u=r(73),c=r(65),f=r(79),a=r(68),s=r(6),l=r(4),h=r(123),v=r(81),p=r(136);t.exports=function(t,n,r,d,y,g){var b=e[t],m=b,x=y?"set":"add",w=m&&m.prototype,S={},_=function(t){var n=w[t];o(w,t,"delete"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"has"==t?function(t){return!(g&&!s(t))&&n.call(this,0===t?0:t)}:"get"==t?function(t){return g&&!s(t)?void 0:n.call(this,0===t?0:t)}:"add"==t?function(t){return n.call(this,0===t?0:t),this}:function(t,r){return n.call(this,0===t?0:t,r),this})};if("function"==typeof m&&(g||w.forEach&&!l(function(){(new m).entries().next()}))){var O=new m,E=O[x](g?{}:-0,1)!=O,P=l(function(){O.has(1)}),j=h(function(t){new m(t)}),F=!g&&l(function(){for(var t=new m,n=5;n--;)t[x](n,n);return!t.has(-0)});j||(m=n(function(n,r){a(n,m,t);var e=p(new b,n,m);return void 0!=r&&f(r,y,e[x],e),e}),m.prototype=w,w.constructor=m),(P||F)&&(_("delete"),_("has"),y&&_("get")),(F||E)&&_(x),g&&w.clear&&delete w.clear}else m=d.getConstructor(n,t,y,x),u(m.prototype,r),c.NEED=!0;return v(m,t),S[t]=m,i(i.G+i.W+i.F*(m!=b),S),g||d.setStrong(m,t,y),m}},function(t,n,r){"use strict";var e=r(27),i=r(28),o=r(4),u=r(46),c=r(7);t.exports=function(t,n,r){var f=c(t),a=r(u,f,""[t]),s=a[0],l=a[1];o(function(){var n={};return n[f]=function(){return 7},7!=""[t](n)})&&(i(String.prototype,t,s),e(RegExp.prototype,f,2==n?function(t,n){return l.call(t,this,n)}:function(t){return l.call(t,this)}))}
},function(t,n,r){"use strict";var e=r(2);t.exports=function(){var t=e(this),n="";return t.global&&(n+="g"),t.ignoreCase&&(n+="i"),t.multiline&&(n+="m"),t.unicode&&(n+="u"),t.sticky&&(n+="y"),n}},function(t,n){t.exports=function(t,n,r){var e=void 0===r;switch(n.length){case 0:return e?t():t.call(r);case 1:return e?t(n[0]):t.call(r,n[0]);case 2:return e?t(n[0],n[1]):t.call(r,n[0],n[1]);case 3:return e?t(n[0],n[1],n[2]):t.call(r,n[0],n[1],n[2]);case 4:return e?t(n[0],n[1],n[2],n[3]):t.call(r,n[0],n[1],n[2],n[3])}return t.apply(r,n)}},function(t,n,r){var e=r(6),i=r(45),o=r(7)("match");t.exports=function(t){var n;return e(t)&&(void 0!==(n=t[o])?!!n:"RegExp"==i(t))}},function(t,n,r){var e=r(7)("iterator"),i=!1;try{var o=[7][e]();o.return=function(){i=!0},Array.from(o,function(){throw 2})}catch(t){}t.exports=function(t,n){if(!n&&!i)return!1;var r=!1;try{var o=[7],u=o[e]();u.next=function(){return{done:r=!0}},o[e]=function(){return u},t(o)}catch(t){}return r}},function(t,n,r){t.exports=r(69)||!r(4)(function(){var t=Math.random();__defineSetter__.call(null,t,function(){}),delete r(3)[t]})},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,r){var e=r(3),i="__core-js_shared__",o=e[i]||(e[i]={});t.exports=function(t){return o[t]||(o[t]={})}},function(t,n,r){for(var e,i=r(3),o=r(27),u=r(76),c=u("typed_array"),f=u("view"),a=!(!i.ArrayBuffer||!i.DataView),s=a,l=0,h="Int8Array,Uint8Array,Uint8ClampedArray,Int16Array,Uint16Array,Int32Array,Uint32Array,Float32Array,Float64Array".split(",");l<9;)(e=i[h[l++]])?(o(e.prototype,c,!0),o(e.prototype,f,!0)):s=!1;t.exports={ABV:a,CONSTR:s,TYPED:c,VIEW:f}},function(t,n){"use strict";var r={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&-1==t.indexOf("KHTML"),mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:-1==t.indexOf("Safari"),weixin:-1==t.indexOf("MicroMessenger")}}()};t.exports=r},function(t,n,r){"use strict";var e=r(85),i=function(t){return t&&t.__esModule?t:{default:t}}(e),o=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):r[t]||t}function n(t){return e[t]}var r={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},e={};for(var u in r)e[r[u]]=u;return r["&apos;"]="'",e["'"]="&#39;",{encode:function(t){return t?(""+t).replace(/['<> "&]/g,n).replace(/\r?\n/g,"<br/>").replace(/\s/g,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(/<br\s*\/?>/gi,"\n").replace(/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,t).replace(/\u00a0/g," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],r=0,e=t.length;e>r;r++)n.push(t.charCodeAt(r).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],r=0,e=t.length;e>r;r+=2)n.push(String.fromCharCode("0x"+t.slice(r,r+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,r=t.length;r>n;n++)t[n]=o.encodeObject(t[n]);else if("object"==(void 0===t?"undefined":(0,i.default)(t)))for(var e in t)t[e]=o.encodeObject(t[e]);else if("string"==typeof t)return o.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=o},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=function(t){for(var n=e(this),r=o(n.length),u=arguments.length,c=i(u>1?arguments[1]:void 0,r),f=u>2?arguments[2]:void 0,a=void 0===f?r:i(f,r);a>c;)n[c++]=t;return n}},function(t,n,r){"use strict";var e=r(11),i=r(66);t.exports=function(t,n,r){n in t?e.f(t,n,i(0,r)):t[n]=r}},function(t,n,r){var e=r(6),i=r(3).document,o=e(i)&&e(i.createElement);t.exports=function(t){return o?i.createElement(t):{}}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n,r){var e=r(7)("match");t.exports=function(t){var n=/./;try{"/./"[t](n)}catch(r){try{return n[e]=!1,!"/./"[t](n)}catch(t){}}return!0}},function(t,n,r){t.exports=r(3).document&&document.documentElement},function(t,n,r){var e=r(6),i=r(144).set;t.exports=function(t,n,r){var o,u=n.constructor;return u!==r&&"function"==typeof u&&(o=u.prototype)!==r.prototype&&e(o)&&i&&i(t,o),t}},function(t,n,r){var e=r(80),i=r(7)("iterator"),o=Array.prototype;t.exports=function(t){return void 0!==t&&(e.Array===t||o[i]===t)}},function(t,n,r){var e=r(45);t.exports=Array.isArray||function(t){return"Array"==e(t)}},function(t,n,r){"use strict";var e=r(70),i=r(66),o=r(81),u={};r(27)(u,r(7)("iterator"),function(){return this}),t.exports=function(t,n,r){t.prototype=e(u,{next:i(1,r)}),o(t,n+" Iterator")}},function(t,n,r){"use strict";var e=r(69),i=r(1),o=r(28),u=r(27),c=r(24),f=r(80),a=r(139),s=r(81),l=r(32),h=r(7)("iterator"),v=!([].keys&&"next"in[].keys()),p="keys",d="values",y=function(){return this};t.exports=function(t,n,r,g,b,m,x){a(r,n,g);var w,S,_,O=function(t){if(!v&&t in F)return F[t];switch(t){case p:case d:return function(){return new r(this,t)}}return function(){return new r(this,t)}},E=n+" Iterator",P=b==d,j=!1,F=t.prototype,M=F[h]||F["@@iterator"]||b&&F[b],A=M||O(b),N=b?P?O("entries"):A:void 0,T="Array"==n?F.entries||M:M;if(T&&(_=l(T.call(new t)))!==Object.prototype&&(s(_,E,!0),e||c(_,h)||u(_,h,y)),P&&M&&M.name!==d&&(j=!0,A=function(){return M.call(this)}),e&&!x||!v&&!j&&F[h]||u(F,h,A),f[n]=A,f[E]=y,b)if(w={values:P?A:O(d),keys:m?A:O(p),entries:N},x)for(S in w)S in F||o(F,S,w[S]);else i(i.P+i.F*(v||j),n,w);return w}},function(t,n){var r=Math.expm1;t.exports=!r||r(10)>22025.465794806718||r(10)<22025.465794806718||-2e-17!=r(-2e-17)?function(t){return 0==(t=+t)?t:t>-1e-6&&t<1e-6?t+t*t/2:Math.exp(t)-1}:r},function(t,n){t.exports=Math.sign||function(t){return 0==(t=+t)||t!=t?t:t<0?-1:1}},function(t,n,r){var e=r(3),i=r(151).set,o=e.MutationObserver||e.WebKitMutationObserver,u=e.process,c=e.Promise,f="process"==r(45)(u);t.exports=function(){var t,n,r,a=function(){var e,i;for(f&&(e=u.domain)&&e.exit();t;){i=t.fn,t=t.next;try{i()}catch(e){throw t?r():n=void 0,e}}n=void 0,e&&e.enter()};if(f)r=function(){u.nextTick(a)};else if(o){var s=!0,l=document.createTextNode("");new o(a).observe(l,{characterData:!0}),r=function(){l.data=s=!s}}else if(c&&c.resolve){var h=c.resolve();r=function(){h.then(a)}}else r=function(){i.call(e,a)};return function(e){var i={fn:e,next:void 0};n&&(n.next=i),t||(t=i,r()),n=i}}},function(t,n,r){var e=r(6),i=r(2),o=function(t,n){if(i(t),!e(n)&&null!==n)throw TypeError(n+": can't set as prototype!")};t.exports={set:Object.setPrototypeOf||("__proto__"in{}?function(t,n,e){try{e=r(53)(Function.call,r(31).f(Object.prototype,"__proto__").set,2),e(t,[]),n=!(t instanceof Array)}catch(t){n=!0}return function(t,r){return o(t,r),n?t.__proto__=r:e(t,r),t}}({},!1):void 0),check:o}},function(t,n,r){var e=r(126)("keys"),i=r(76);t.exports=function(t){return e[t]||(e[t]=i(t))}},function(t,n,r){var e=r(2),i=r(26),o=r(7)("species");t.exports=function(t,n){var r,u=e(t).constructor;return void 0===u||void 0==(r=e(u)[o])?n:i(r)}},function(t,n,r){var e=r(67),i=r(46);t.exports=function(t){return function(n,r){var o,u,c=String(i(n)),f=e(r),a=c.length;return f<0||f>=a?t?"":void 0:(o=c.charCodeAt(f),o<55296||o>56319||f+1===a||(u=c.charCodeAt(f+1))<56320||u>57343?t?c.charAt(f):o:t?c.slice(f,f+2):u-56320+(o-55296<<10)+65536)}}},function(t,n,r){var e=r(122),i=r(46);t.exports=function(t,n,r){if(e(n))throw TypeError("String#"+r+" doesn't accept regex!");return String(i(t))}},function(t,n,r){"use strict";var e=r(67),i=r(46);t.exports=function(t){var n=String(i(this)),r="",o=e(t);if(o<0||o==1/0)throw RangeError("Count can't be negative");for(;o>0;(o>>>=1)&&(n+=n))1&o&&(r+=n);return r}},function(t,n){t.exports="\t\n\v\f\r   ᠎             　\u2028\u2029\ufeff"},function(t,n,r){var e,i,o,u=r(53),c=r(121),f=r(135),a=r(132),s=r(3),l=s.process,h=s.setImmediate,v=s.clearImmediate,p=s.MessageChannel,d=0,y={},g="onreadystatechange",b=function(){var t=+this;if(y.hasOwnProperty(t)){var n=y[t];delete y[t],n()}},m=function(t){b.call(t.data)};h&&v||(h=function(t){for(var n=[],r=1;arguments.length>r;)n.push(arguments[r++]);return y[++d]=function(){c("function"==typeof t?t:Function(t),n)},e(d),d},v=function(t){delete y[t]},"process"==r(45)(l)?e=function(t){l.nextTick(u(b,t,1))}:p?(i=new p,o=i.port2,i.port1.onmessage=m,e=u(o.postMessage,o,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts?(e=function(t){s.postMessage(t+"","*")},s.addEventListener("message",m,!1)):e=g in a("script")?function(t){f.appendChild(a("script"))[g]=function(){f.removeChild(this),b.call(t)}}:function(t){setTimeout(u(b,t,1),0)}),t.exports={set:h,clear:v}},function(t,n,r){"use strict";var e=r(3),i=r(10),o=r(69),u=r(127),c=r(27),f=r(73),a=r(4),s=r(68),l=r(67),h=r(16),v=r(71).f,p=r(11).f,d=r(130),y=r(81),g="ArrayBuffer",b="DataView",m="prototype",x="Wrong length!",w="Wrong index!",S=e[g],_=e[b],O=e.Math,E=e.RangeError,P=e.Infinity,j=S,F=O.abs,M=O.pow,A=O.floor,N=O.log,T=O.LN2,I="buffer",k="byteLength",L="byteOffset",R=i?"_b":I,C=i?"_l":k,D=i?"_o":L,U=function(t,n,r){var e,i,o,u=Array(r),c=8*r-n-1,f=(1<<c)-1,a=f>>1,s=23===n?M(2,-24)-M(2,-77):0,l=0,h=t<0||0===t&&1/t<0?1:0;for(t=F(t),t!=t||t===P?(i=t!=t?1:0,e=f):(e=A(N(t)/T),t*(o=M(2,-e))<1&&(e--,o*=2),t+=e+a>=1?s/o:s*M(2,1-a),t*o>=2&&(e++,o/=2),e+a>=f?(i=0,e=f):e+a>=1?(i=(t*o-1)*M(2,n),e+=a):(i=t*M(2,a-1)*M(2,n),e=0));n>=8;u[l++]=255&i,i/=256,n-=8);for(e=e<<n|i,c+=n;c>0;u[l++]=255&e,e/=256,c-=8);return u[--l]|=128*h,u},W=function(t,n,r){var e,i=8*r-n-1,o=(1<<i)-1,u=o>>1,c=i-7,f=r-1,a=t[f--],s=127&a;for(a>>=7;c>0;s=256*s+t[f],f--,c-=8);for(e=s&(1<<-c)-1,s>>=-c,c+=n;c>0;e=256*e+t[f],f--,c-=8);if(0===s)s=1-u;else{if(s===o)return e?NaN:a?-P:P;e+=M(2,n),s-=u}return(a?-1:1)*e*M(2,s-n)},G=function(t){return t[3]<<24|t[2]<<16|t[1]<<8|t[0]},B=function(t){return[255&t]},V=function(t){return[255&t,t>>8&255]},z=function(t){return[255&t,t>>8&255,t>>16&255,t>>24&255]},q=function(t){return U(t,52,8)},K=function(t){return U(t,23,4)},J=function(t,n,r){p(t[m],n,{get:function(){return this[r]}})},Y=function(t,n,r,e){var i=+r,o=l(i);if(i!=o||o<0||o+n>t[C])throw E(w);var u=t[R]._b,c=o+t[D],f=u.slice(c,c+n);return e?f:f.reverse()},H=function(t,n,r,e,i,o){var u=+r,c=l(u);if(u!=c||c<0||c+n>t[C])throw E(w);for(var f=t[R]._b,a=c+t[D],s=e(+i),h=0;h<n;h++)f[a+h]=s[o?h:n-h-1]},$=function(t,n){s(t,S,g);var r=+n,e=h(r);if(r!=e)throw E(x);return e};if(u.ABV){if(!a(function(){new S})||!a(function(){new S(.5)})){S=function(t){return new j($(this,t))};for(var X,Q=S[m]=j[m],Z=v(j),tt=0;Z.length>tt;)(X=Z[tt++])in S||c(S,X,j[X]);o||(Q.constructor=S)}var nt=new _(new S(2)),rt=_[m].setInt8;nt.setInt8(0,2147483648),nt.setInt8(1,2147483649),!nt.getInt8(0)&&nt.getInt8(1)||f(_[m],{setInt8:function(t,n){rt.call(this,t,n<<24>>24)},setUint8:function(t,n){rt.call(this,t,n<<24>>24)}},!0)}else S=function(t){var n=$(this,t);this._b=d.call(Array(n),0),this[C]=n},_=function(t,n,r){s(this,_,b),s(t,S,b);var e=t[C],i=l(n);if(i<0||i>e)throw E("Wrong offset!");if(r=void 0===r?e-i:h(r),i+r>e)throw E(x);this[R]=t,this[D]=i,this[C]=r},i&&(J(S,k,"_l"),J(_,I,"_b"),J(_,k,"_l"),J(_,L,"_o")),f(_[m],{getInt8:function(t){return Y(this,1,t)[0]<<24>>24},getUint8:function(t){return Y(this,1,t)[0]},getInt16:function(t){var n=Y(this,2,t,arguments[1]);return(n[1]<<8|n[0])<<16>>16},getUint16:function(t){var n=Y(this,2,t,arguments[1]);return n[1]<<8|n[0]},getInt32:function(t){return G(Y(this,4,t,arguments[1]))},getUint32:function(t){return G(Y(this,4,t,arguments[1]))>>>0},getFloat32:function(t){return W(Y(this,4,t,arguments[1]),23,4)},getFloat64:function(t){return W(Y(this,8,t,arguments[1]),52,8)},setInt8:function(t,n){H(this,1,t,B,n)},setUint8:function(t,n){H(this,1,t,B,n)},setInt16:function(t,n){H(this,2,t,V,n,arguments[2])},setUint16:function(t,n){H(this,2,t,V,n,arguments[2])},setInt32:function(t,n){H(this,4,t,z,n,arguments[2])},setUint32:function(t,n){H(this,4,t,z,n,arguments[2])},setFloat32:function(t,n){H(this,4,t,K,n,arguments[2])},setFloat64:function(t,n){H(this,8,t,q,n,arguments[2])}});y(S,g),y(_,b),c(_[m],u.VIEW,!0),n[g]=S,n[b]=_},function(t,n,r){var e=r(3),i=r(52),o=r(69),u=r(182),c=r(11).f;t.exports=function(t){var n=i.Symbol||(i.Symbol=o?{}:e.Symbol||{});"_"==t.charAt(0)||t in n||c(n,t,{value:u.f(t)})}},function(t,n,r){var e=r(114),i=r(7)("iterator"),o=r(80);t.exports=r(52).getIteratorMethod=function(t){if(void 0!=t)return t[i]||t["@@iterator"]||o[e(t)]}},function(t,n,r){"use strict";var e=r(78),i=r(170),o=r(80),u=r(30);t.exports=r(140)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,r=this._i++;return!t||r>=t.length?(this._t=void 0,i(1)):"keys"==n?i(0,r):"values"==n?i(0,t[r]):i(0,[r,t[r]])},"values"),o.Arguments=o.Array,e("keys"),e("values"),e("entries")},function(t,n){function r(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=r},function(t,n){function r(t,n){if(t.classList)t.classList.remove(n);else{var r=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(r," ")}}t.exports=r},function(t,n){function r(){throw new Error("setTimeout has not been defined")}function e(){throw new Error("clearTimeout has not been defined")}function i(t){if(s===setTimeout)return setTimeout(t,0);if((s===r||!s)&&setTimeout)return s=setTimeout,setTimeout(t,0);try{return s(t,0)}catch(n){try{return s.call(null,t,0)}catch(n){return s.call(this,t,0)}}}function o(t){if(l===clearTimeout)return clearTimeout(t);if((l===e||!l)&&clearTimeout)return l=clearTimeout,clearTimeout(t);try{return l(t)}catch(n){try{return l.call(null,t)}catch(n){return l.call(this,t)}}}function u(){d&&v&&(d=!1,v.length?p=v.concat(p):y=-1,p.length&&c())}function c(){if(!d){var t=i(u);d=!0;for(var n=p.length;n;){for(v=p,p=[];++y<n;)v&&v[y].run();y=-1,n=p.length}v=null,d=!1,o(t)}}function f(t,n){this.fun=t,this.array=n}function a(){}var s,l,h=t.exports={};!function(){try{s="function"==typeof setTimeout?setTimeout:r}catch(t){s=r}try{l="function"==typeof clearTimeout?clearTimeout:e}catch(t){l=e}}();var v,p=[],d=!1,y=-1;h.nextTick=function(t){var n=new Array(arguments.length-1);if(arguments.length>1)for(var r=1;r<arguments.length;r++)n[r-1]=arguments[r];p.push(new f(t,n)),1!==p.length||d||i(c)},f.prototype.run=function(){this.fun.apply(null,this.array)},h.title="browser",h.browser=!0,h.env={},h.argv=[],h.version="",h.versions={},h.on=a,h.addListener=a,h.once=a,h.off=a,h.removeListener=a,h.removeAllListeners=a,h.emit=a,h.prependListener=a,h.prependOnceListener=a,h.listeners=function(t){return[]},h.binding=function(t){throw new Error("process.binding is not supported")},h.cwd=function(){return"/"},h.chdir=function(t){throw new Error("process.chdir is not supported")},h.umask=function(){return 0}},function(t,n,r){var e=r(45);t.exports=function(t,n){if("number"!=typeof t&&"Number"!=e(t))throw TypeError(n);return+t}},function(t,n,r){"use strict";var e=r(17),i=r(75),o=r(16);t.exports=[].copyWithin||function(t,n){var r=e(this),u=o(r.length),c=i(t,u),f=i(n,u),a=arguments.length>2?arguments[2]:void 0,s=Math.min((void 0===a?u:i(a,u))-f,u-c),l=1;for(f<c&&c<f+s&&(l=-1,f+=s-1,c+=s-1);s-- >0;)f in r?r[c]=r[f]:delete r[c],c+=l,f+=l;return r}},function(t,n,r){var e=r(79);t.exports=function(t,n){var r=[];return e(t,!1,r.push,r,n),r}},function(t,n,r){var e=r(26),i=r(17),o=r(115),u=r(16);t.exports=function(t,n,r,c,f){e(n);var a=i(t),s=o(a),l=u(a.length),h=f?l-1:0,v=f?-1:1;if(r<2)for(;;){if(h in s){c=s[h],h+=v;break}if(h+=v,f?h<0:l<=h)throw TypeError("Reduce of empty array with no initial value")}for(;f?h>=0:l>h;h+=v)h in s&&(c=n(c,s[h],h,a));return c}},function(t,n,r){"use strict";var e=r(26),i=r(6),o=r(121),u=[].slice,c={},f=function(t,n,r){if(!(n in c)){for(var e=[],i=0;i<n;i++)e[i]="a["+i+"]";c[n]=Function("F,a","return new F("+e.join(",")+")")}return c[n](t,r)};t.exports=Function.bind||function(t){var n=e(this),r=u.call(arguments,1),c=function(){var e=r.concat(u.call(arguments));return this instanceof c?f(n,e.length,e):o(n,e,t)};return i(n.prototype)&&(c.prototype=n.prototype),c}},function(t,n,r){"use strict";var e=r(11).f,i=r(70),o=r(73),u=r(53),c=r(68),f=r(46),a=r(79),s=r(140),l=r(170),h=r(74),v=r(10),p=r(65).fastKey,d=v?"_s":"size",y=function(t,n){var r,e=p(n);if("F"!==e)return t._i[e];for(r=t._f;r;r=r.n)if(r.k==n)return r};t.exports={getConstructor:function(t,n,r,s){var l=t(function(t,e){c(t,l,n,"_i"),t._i=i(null),t._f=void 0,t._l=void 0,t[d]=0,void 0!=e&&a(e,r,t[s],t)});return o(l.prototype,{clear:function(){for(var t=this,n=t._i,r=t._f;r;r=r.n)r.r=!0,r.p&&(r.p=r.p.n=void 0),delete n[r.i];t._f=t._l=void 0,t[d]=0},delete:function(t){var n=this,r=y(n,t);if(r){var e=r.n,i=r.p;delete n._i[r.i],r.r=!0,i&&(i.n=e),e&&(e.p=i),n._f==r&&(n._f=e),n._l==r&&(n._l=i),n[d]--}return!!r},forEach:function(t){c(this,l,"forEach");for(var n,r=u(t,arguments.length>1?arguments[1]:void 0,3);n=n?n.n:this._f;)for(r(n.v,n.k,this);n&&n.r;)n=n.p},has:function(t){return!!y(this,t)}}),v&&e(l.prototype,"size",{get:function(){return f(this[d])}}),l},def:function(t,n,r){var e,i,o=y(t,n);return o?o.v=r:(t._l=o={i:i=p(n,!0),k:n,v:r,p:e=t._l,n:void 0,r:!1},t._f||(t._f=o),e&&(e.n=o),t[d]++,"F"!==i&&(t._i[i]=o)),t},getEntry:y,setStrong:function(t,n,r){s(t,n,function(t,n){this._t=t,this._k=n,this._l=void 0},function(){for(var t=this,n=t._k,r=t._l;r&&r.r;)r=r.p;return t._t&&(t._l=r=r?r.n:t._t._f)?"keys"==n?l(0,r.k):"values"==n?l(0,r.v):l(0,[r.k,r.v]):(t._t=void 0,l(1))},r?"entries":"values",!r,!0),h(n)}}},function(t,n,r){var e=r(114),i=r(161);t.exports=function(t){return function(){if(e(this)!=t)throw TypeError(t+"#toJSON isn't generic");return i(this)}}},function(t,n,r){"use strict";var e=r(73),i=r(65).getWeak,o=r(2),u=r(6),c=r(68),f=r(79),a=r(48),s=r(24),l=a(5),h=a(6),v=0,p=function(t){return t._l||(t._l=new d)},d=function(){this.a=[]},y=function(t,n){return l(t.a,function(t){return t[0]===n})};d.prototype={get:function(t){var n=y(this,t);if(n)return n[1]},has:function(t){return!!y(this,t)},set:function(t,n){var r=y(this,t);r?r[1]=n:this.a.push([t,n])},delete:function(t){var n=h(this.a,function(n){return n[0]===t});return~n&&this.a.splice(n,1),!!~n}},t.exports={getConstructor:function(t,n,r,o){var a=t(function(t,e){c(t,a,n,"_i"),t._i=v++,t._l=void 0,void 0!=e&&f(e,r,t[o],t)});return e(a.prototype,{delete:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).delete(t):n&&s(n,this._i)&&delete n[this._i]},has:function(t){if(!u(t))return!1;var n=i(t);return!0===n?p(this).has(t):n&&s(n,this._i)}}),a},def:function(t,n,r){var e=i(o(n),!0);return!0===e?p(t).set(n,r):e[t._i]=r,t},ufstore:p}},function(t,n,r){t.exports=!r(10)&&!r(4)(function(){return 7!=Object.defineProperty(r(132)("div"),"a",{get:function(){return 7}}).a})},function(t,n,r){var e=r(6),i=Math.floor;t.exports=function(t){return!e(t)&&isFinite(t)&&i(t)===t}},function(t,n,r){var e=r(2);t.exports=function(t,n,r,i){try{return i?n(e(r)[0],r[1]):n(r)}catch(n){var o=t.return;throw void 0!==o&&e(o.call(t)),n}}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n){t.exports=Math.log1p||function(t){return(t=+t)>-1e-8&&t<1e-8?t-t*t/2:Math.log(1+t)}},function(t,n,r){"use strict";var e=r(72),i=r(125),o=r(116),u=r(17),c=r(115),f=Object.assign;t.exports=!f||r(4)(function(){var t={},n={},r=Symbol(),e="abcdefghijklmnopqrst";return t[r]=7,e.split("").forEach(function(t){n[t]=t}),7!=f({},t)[r]||Object.keys(f({},n)).join("")!=e})?function(t,n){for(var r=u(t),f=arguments.length,a=1,s=i.f,l=o.f;f>a;)for(var h,v=c(arguments[a++]),p=s?e(v).concat(s(v)):e(v),d=p.length,y=0;d>y;)l.call(v,h=p[y++])&&(r[h]=v[h]);return r}:f},function(t,n,r){var e=r(11),i=r(2),o=r(72);t.exports=r(10)?Object.defineProperties:function(t,n){i(t);for(var r,u=o(n),c=u.length,f=0;c>f;)e.f(t,r=u[f++],n[r]);return t}},function(t,n,r){var e=r(30),i=r(71).f,o={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],c=function(t){try{return i(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==o.call(t)?c(t):i(e(t))}},function(t,n,r){var e=r(24),i=r(30),o=r(117)(!1),u=r(145)("IE_PROTO");t.exports=function(t,n){var r,c=i(t),f=0,a=[];for(r in c)r!=u&&e(c,r)&&a.push(r);for(;n.length>f;)e(c,r=n[f++])&&(~o(a,r)||a.push(r));return a}},function(t,n,r){var e=r(72),i=r(30),o=r(116).f;t.exports=function(t){return function(n){for(var r,u=i(n),c=e(u),f=c.length,a=0,s=[];f>a;)o.call(u,r=c[a++])&&s.push(t?[r,u[r]]:u[r]);return s}}},function(t,n,r){var e=r(71),i=r(125),o=r(2),u=r(3).Reflect;t.exports=u&&u.ownKeys||function(t){var n=e.f(o(t)),r=i.f;return r?n.concat(r(t)):n}},function(t,n,r){var e=r(3).parseFloat,i=r(82).trim;t.exports=1/e(r(150)+"-0")!=-1/0?function(t){var n=i(String(t),3),r=e(n);return 0===r&&"-"==n.charAt(0)?-0:r}:e},function(t,n,r){var e=r(3).parseInt,i=r(82).trim,o=r(150),u=/^[\-+]?0[xX]/;t.exports=8!==e(o+"08")||22!==e(o+"0x16")?function(t,n){var r=i(String(t),3);return e(r,n>>>0||(u.test(r)?16:10))}:e},function(t,n){t.exports=Object.is||function(t,n){return t===n?0!==t||1/t==1/n:t!=t&&n!=n}},function(t,n,r){var e=r(16),i=r(149),o=r(46);t.exports=function(t,n,r,u){var c=String(o(t)),f=c.length,a=void 0===r?" ":String(r),s=e(n);if(s<=f||""==a)return c;var l=s-f,h=i.call(a,Math.ceil(l/a.length));return h.length>l&&(h=h.slice(0,l)),u?h+c:c+h}},function(t,n,r){n.f=r(7)},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Map",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{get:function(t){var n=e.getEntry(this,t);return n&&n.v},set:function(t,n){return e.def(this,0===t?0:t,n)}},e,!0)},function(t,n,r){r(10)&&"g"!=/./g.flags&&r(11).f(RegExp.prototype,"flags",{configurable:!0,get:r(120)})},function(t,n,r){"use strict";var e=r(164);t.exports=r(118)("Set",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t=0===t?0:t,t)}},e)},function(t,n,r){"use strict";var e,i=r(48)(0),o=r(28),u=r(65),c=r(172),f=r(166),a=r(6),s=u.getWeak,l=Object.isExtensible,h=f.ufstore,v={},p=function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},d={get:function(t){if(a(t)){var n=s(t);return!0===n?h(this).get(t):n?n[this._i]:void 0}},set:function(t,n){return f.def(this,t,n)}},y=t.exports=r(118)("WeakMap",p,d,f,!0,!0);7!=(new y).set((Object.freeze||Object)(v),7).get(v)&&(e=f.getConstructor(p),c(e.prototype,d),u.NEED=!0,i(["delete","has","get","set"],function(t){var n=y.prototype,r=n[t];o(n,t,function(n,i){if(a(n)&&!l(n)){this._f||(this._f=new e);var o=this._f[t](n,i);return"set"==t?this:o}return r.call(this,n,i)})}))},,,,function(t,n){"use strict";function r(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){document.querySelectorAll(".article-entry a:not(.article-more-a)").forEach(function(t){var n=t.getAttribute("target");n&&""!==n||t.setAttribute("target","_blank")})}if(yiliaConfig&&yiliaConfig.toc_hide_index){document.querySelectorAll(".toc-number").forEach(function(t){t.style.display="none"})}var n=document.querySelector("#js-aboutme");n&&0!==n.length&&(n.innerHTML=n.innerText)}t.exports={init:r}},function(t,n,r){"use strict";function e(t){return t&&t.__esModule?t:{default:t}}function i(t,n){var r=/\/|index.html/g;return t.replace(r,"")===n.replace(r,"")}function o(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,r=0,e=t.length;r<e;r++){var o=t[r];i(n,o.getAttribute("href"))&&(0,h.default)(o,"active")}}function u(t){for(var n=t.offsetLeft,r=t.offsetParent;null!==r;)n+=r.offsetLeft,r=r.offsetParent;return n}function c(t){for(var n=t.offsetTop,r=t.offsetParent;null!==r;)n+=r.offsetTop,r=r.offsetParent;return n}function f(t,n,r,e,i){var o=u(t),f=c(t)-n;if(f-r<=i){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,d.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(r||f)+"px",a.style.left=o+"px",a.style.zIndex=e||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");f(t,document.body.scrollTop,-63,2,0),f(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}var l=r(156),h=e(l),v=r(157),p=(e(v),r(382)),d=e(p),y=r(128),g=e(y),b=r(190),m=e(b),x=r(129);(function(){g.default.versions.mobile&&window.screen.width<800&&(o(),s())})(),(0,x.addLoadEvent)(function(){m.default.init()}),t.exports={}},,,,function(t,n,r){(function(t){"use strict";function n(t,n,r){t[n]||Object[e](t,n,{writable:!0,configurable:!0,value:r})}if(r(381),r(391),r(198),t._babelPolyfill)throw new Error("only one instance of babel-polyfill is allowed");t._babelPolyfill=!0;var e="defineProperty";n(String.prototype,"padLeft","".padStart),n(String.prototype,"padRight","".padEnd),"pop,reverse,shift,keys,values,entries,indexOf,every,some,forEach,map,filter,find,findIndex,includes,join,slice,concat,push,splice,unshift,sort,lastIndexOf,reduce,reduceRight,copyWithin,fill".split(",").forEach(function(t){[][t]&&n(Array,t,Function.call.bind([][t]))})}).call(n,function(){return this}())},,,function(t,n,r){r(210),t.exports=r(52).RegExp.escape},,,,function(t,n,r){var e=r(6),i=r(138),o=r(7)("species");t.exports=function(t){var n;return i(t)&&(n=t.constructor,"function"!=typeof n||n!==Array&&!i(n.prototype)||(n=void 0),e(n)&&null===(n=n[o])&&(n=void 0)),void 0===n?Array:n}},function(t,n,r){var e=r(202);t.exports=function(t,n){return new(e(t))(n)}},function(t,n,r){"use strict";var e=r(2),i=r(50),o="number";t.exports=function(t){if("string"!==t&&t!==o&&"default"!==t)throw TypeError("Incorrect hint");return i(e(this),t!=o)}},function(t,n,r){var e=r(72),i=r(125),o=r(116);t.exports=function(t){var n=e(t),r=i.f;if(r)for(var u,c=r(t),f=o.f,a=0;c.length>a;)f.call(t,u=c[a++])&&n.push(u);return n}},function(t,n,r){var e=r(72),i=r(30);t.exports=function(t,n){for(var r,o=i(t),u=e(o),c=u.length,f=0;c>f;)if(o[r=u[f++]]===n)return r}},function(t,n,r){"use strict";var e=r(208),i=r(121),o=r(26);t.exports=function(){for(var t=o(this),n=arguments.length,r=Array(n),u=0,c=e._,f=!1;n>u;)(r[u]=arguments[u++])===c&&(f=!0);return function(){var e,o=this,u=arguments.length,a=0,s=0;if(!f&&!u)return i(t,r,o);if(e=r.slice(),f)for(;n>a;a++)e[a]===c&&(e[a]=arguments[s++]);for(;u>s;)e.push(arguments[s++]);return i(t,e,o)}}},function(t,n,r){t.exports=r(3)},function(t,n){t.exports=function(t,n){var r=n===Object(n)?function(t){return n[t]}:n;return function(n){return String(n).replace(t,r)}}},function(t,n,r){var e=r(1),i=r(209)(/[\\^$*+?.()|[\]{}]/g,"\\$&");e(e.S,"RegExp",{escape:function(t){return i(t)}})},function(t,n,r){var e=r(1);e(e.P,"Array",{copyWithin:r(160)}),r(78)("copyWithin")},function(t,n,r){"use strict";var e=r(1),i=r(48)(4);e(e.P+e.F*!r(47)([].every,!0),"Array",{every:function(t){return i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.P,"Array",{fill:r(130)}),r(78)("fill")},function(t,n,r){"use strict";var e=r(1),i=r(48)(2);e(e.P+e.F*!r(47)([].filter,!0),"Array",{filter:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(6),o="findIndex",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{findIndex:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(5),o="find",u=!0;o in[]&&Array(1)[o](function(){u=!1}),e(e.P+e.F*u,"Array",{find:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)(o)},function(t,n,r){"use strict";var e=r(1),i=r(48)(0),o=r(47)([].forEach,!0);e(e.P+e.F*!o,"Array",{forEach:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(53),i=r(1),o=r(17),u=r(169),c=r(137),f=r(16),a=r(131),s=r(154);i(i.S+i.F*!r(123)(function(t){Array.from(t)}),"Array",{from:function(t){var n,r,i,l,h=o(t),v="function"==typeof this?this:Array,p=arguments.length,d=p>1?arguments[1]:void 0,y=void 0!==d,g=0,b=s(h);if(y&&(d=e(d,p>2?arguments[2]:void 0,2)),void 0==b||v==Array&&c(b))for(n=f(h.length),r=new v(n);n>g;g++)a(r,g,y?d(h[g],g):h[g]);else for(l=b.call(h),r=new v;!(i=l.next()).done;g++)a(r,g,y?u(l,d,[i.value,g],!0):i.value);return r.length=g,r}})},function(t,n,r){"use strict";var e=r(1),i=r(117)(!1),o=[].indexOf,u=!!o&&1/[1].indexOf(1,-0)<0;e(e.P+e.F*(u||!r(47)(o)),"Array",{indexOf:function(t){return u?o.apply(this,arguments)||0:i(this,t,arguments[1])}})},function(t,n,r){var e=r(1);e(e.S,"Array",{isArray:r(138)})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=[].join;e(e.P+e.F*(r(115)!=Object||!r(47)(o)),"Array",{join:function(t){return o.call(i(this),void 0===t?",":t)}})},function(t,n,r){"use strict";var e=r(1),i=r(30),o=r(67),u=r(16),c=[].lastIndexOf,f=!!c&&1/[1].lastIndexOf(1,-0)<0;e(e.P+e.F*(f||!r(47)(c)),"Array",{lastIndexOf:function(t){if(f)return c.apply(this,arguments)||0;var n=i(this),r=u(n.length),e=r-1;for(arguments.length>1&&(e=Math.min(e,o(arguments[1]))),e<0&&(e=r+e);e>=0;e--)if(e in n&&n[e]===t)return e||0;return-1}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(1);e(e.P+e.F*!r(47)([].map,!0),"Array",{map:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(131);e(e.S+e.F*r(4)(function(){function t(){}return!(Array.of.call(t)instanceof t)}),"Array",{of:function(){for(var t=0,n=arguments.length,r=new("function"==typeof this?this:Array)(n);n>t;)i(r,t,arguments[t++]);return r.length=n,r}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduceRight,!0),"Array",{reduceRight:function(t){return i(this,t,arguments.length,arguments[1],!0)}})},function(t,n,r){"use strict";var e=r(1),i=r(162);e(e.P+e.F*!r(47)([].reduce,!0),"Array",{reduce:function(t){return i(this,t,arguments.length,arguments[1],!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(135),o=r(45),u=r(75),c=r(16),f=[].slice;e(e.P+e.F*r(4)(function(){i&&f.call(i)}),"Array",{slice:function(t,n){var r=c(this.length),e=o(this);if(n=void 0===n?r:n,"Array"==e)return f.call(this,t,n);for(var i=u(t,r),a=u(n,r),s=c(a-i),l=Array(s),h=0;h<s;h++)l[h]="String"==e?this.charAt(i+h):this[i+h];return l}})},function(t,n,r){"use strict";var e=r(1),i=r(48)(3);e(e.P+e.F*!r(47)([].some,!0),"Array",{some:function(t){return i(this,t,arguments[1])}})},function(t,n,r){"use strict";var e=r(1),i=r(26),o=r(17),u=r(4),c=[].sort,f=[1,2,3];e(e.P+e.F*(u(function(){f.sort(void 0)})||!u(function(){f.sort(null)})||!r(47)(c)),"Array",{sort:function(t){return void 0===t?c.call(o(this)):c.call(o(this),i(t))}})},function(t,n,r){r(74)("Array")},function(t,n,r){var e=r(1);e(e.S,"Date",{now:function(){return(new Date).getTime()}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=Date.prototype.getTime,u=function(t){return t>9?t:"0"+t};e(e.P+e.F*(i(function(){return"0385-07-25T07:06:39.999Z"!=new Date(-5e13-1).toISOString()})||!i(function(){new Date(NaN).toISOString()})),"Date",{toISOString:function(){
if(!isFinite(o.call(this)))throw RangeError("Invalid time value");var t=this,n=t.getUTCFullYear(),r=t.getUTCMilliseconds(),e=n<0?"-":n>9999?"+":"";return e+("00000"+Math.abs(n)).slice(e?-6:-4)+"-"+u(t.getUTCMonth()+1)+"-"+u(t.getUTCDate())+"T"+u(t.getUTCHours())+":"+u(t.getUTCMinutes())+":"+u(t.getUTCSeconds())+"."+(r>99?r:"0"+u(r))+"Z"}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50);e(e.P+e.F*r(4)(function(){return null!==new Date(NaN).toJSON()||1!==Date.prototype.toJSON.call({toISOString:function(){return 1}})}),"Date",{toJSON:function(t){var n=i(this),r=o(n);return"number"!=typeof r||isFinite(r)?n.toISOString():null}})},function(t,n,r){var e=r(7)("toPrimitive"),i=Date.prototype;e in i||r(27)(i,e,r(204))},function(t,n,r){var e=Date.prototype,i="Invalid Date",o="toString",u=e[o],c=e.getTime;new Date(NaN)+""!=i&&r(28)(e,o,function(){var t=c.call(this);return t===t?u.call(this):i})},function(t,n,r){var e=r(1);e(e.P,"Function",{bind:r(163)})},function(t,n,r){"use strict";var e=r(6),i=r(32),o=r(7)("hasInstance"),u=Function.prototype;o in u||r(11).f(u,o,{value:function(t){if("function"!=typeof this||!e(t))return!1;if(!e(this.prototype))return t instanceof this;for(;t=i(t);)if(this.prototype===t)return!0;return!1}})},function(t,n,r){var e=r(11).f,i=r(66),o=r(24),u=Function.prototype,c="name",f=Object.isExtensible||function(){return!0};c in u||r(10)&&e(u,c,{configurable:!0,get:function(){try{var t=this,n=(""+t).match(/^\s*function ([^ (]*)/)[1];return o(t,c)||!f(t)||e(t,c,i(5,n)),n}catch(t){return""}}})},function(t,n,r){var e=r(1),i=r(171),o=Math.sqrt,u=Math.acosh;e(e.S+e.F*!(u&&710==Math.floor(u(Number.MAX_VALUE))&&u(1/0)==1/0),"Math",{acosh:function(t){return(t=+t)<1?NaN:t>94906265.62425156?Math.log(t)+Math.LN2:i(t-1+o(t-1)*o(t+1))}})},function(t,n,r){function e(t){return isFinite(t=+t)&&0!=t?t<0?-e(-t):Math.log(t+Math.sqrt(t*t+1)):t}var i=r(1),o=Math.asinh;i(i.S+i.F*!(o&&1/o(0)>0),"Math",{asinh:e})},function(t,n,r){var e=r(1),i=Math.atanh;e(e.S+e.F*!(i&&1/i(-0)<0),"Math",{atanh:function(t){return 0==(t=+t)?t:Math.log((1+t)/(1-t))/2}})},function(t,n,r){var e=r(1),i=r(142);e(e.S,"Math",{cbrt:function(t){return i(t=+t)*Math.pow(Math.abs(t),1/3)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{clz32:function(t){return(t>>>=0)?31-Math.floor(Math.log(t+.5)*Math.LOG2E):32}})},function(t,n,r){var e=r(1),i=Math.exp;e(e.S,"Math",{cosh:function(t){return(i(t=+t)+i(-t))/2}})},function(t,n,r){var e=r(1),i=r(141);e(e.S+e.F*(i!=Math.expm1),"Math",{expm1:i})},function(t,n,r){var e=r(1),i=r(142),o=Math.pow,u=o(2,-52),c=o(2,-23),f=o(2,127)*(2-c),a=o(2,-126),s=function(t){return t+1/u-1/u};e(e.S,"Math",{fround:function(t){var n,r,e=Math.abs(t),o=i(t);return e<a?o*s(e/a/c)*a*c:(n=(1+c/u)*e,r=n-(n-e),r>f||r!=r?o*(1/0):o*r)}})},function(t,n,r){var e=r(1),i=Math.abs;e(e.S,"Math",{hypot:function(t,n){for(var r,e,o=0,u=0,c=arguments.length,f=0;u<c;)r=i(arguments[u++]),f<r?(e=f/r,o=o*e*e+1,f=r):r>0?(e=r/f,o+=e*e):o+=r;return f===1/0?1/0:f*Math.sqrt(o)}})},function(t,n,r){var e=r(1),i=Math.imul;e(e.S+e.F*r(4)(function(){return-5!=i(4294967295,5)||2!=i.length}),"Math",{imul:function(t,n){var r=65535,e=+t,i=+n,o=r&e,u=r&i;return 0|o*u+((r&e>>>16)*u+o*(r&i>>>16)<<16>>>0)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log10:function(t){return Math.log(t)/Math.LN10}})},function(t,n,r){var e=r(1);e(e.S,"Math",{log1p:r(171)})},function(t,n,r){var e=r(1);e(e.S,"Math",{log2:function(t){return Math.log(t)/Math.LN2}})},function(t,n,r){var e=r(1);e(e.S,"Math",{sign:r(142)})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S+e.F*r(4)(function(){return-2e-17!=!Math.sinh(-2e-17)}),"Math",{sinh:function(t){return Math.abs(t=+t)<1?(i(t)-i(-t))/2:(o(t-1)-o(-t-1))*(Math.E/2)}})},function(t,n,r){var e=r(1),i=r(141),o=Math.exp;e(e.S,"Math",{tanh:function(t){var n=i(t=+t),r=i(-t);return n==1/0?1:r==1/0?-1:(n-r)/(o(t)+o(-t))}})},function(t,n,r){var e=r(1);e(e.S,"Math",{trunc:function(t){return(t>0?Math.floor:Math.ceil)(t)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(45),u=r(136),c=r(50),f=r(4),a=r(71).f,s=r(31).f,l=r(11).f,h=r(82).trim,v="Number",p=e[v],d=p,y=p.prototype,g=o(r(70)(y))==v,b="trim"in String.prototype,m=function(t){var n=c(t,!1);if("string"==typeof n&&n.length>2){n=b?n.trim():h(n,3);var r,e,i,o=n.charCodeAt(0);if(43===o||45===o){if(88===(r=n.charCodeAt(2))||120===r)return NaN}else if(48===o){switch(n.charCodeAt(1)){case 66:case 98:e=2,i=49;break;case 79:case 111:e=8,i=55;break;default:return+n}for(var u,f=n.slice(2),a=0,s=f.length;a<s;a++)if((u=f.charCodeAt(a))<48||u>i)return NaN;return parseInt(f,e)}}return+n};if(!p(" 0o1")||!p("0b1")||p("+0x1")){p=function(t){var n=arguments.length<1?0:t,r=this;return r instanceof p&&(g?f(function(){y.valueOf.call(r)}):o(r)!=v)?u(new d(m(n)),r,p):m(n)};for(var x,w=r(10)?a(d):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),S=0;w.length>S;S++)i(d,x=w[S])&&!i(p,x)&&l(p,x,s(d,x));p.prototype=y,y.constructor=p,r(28)(e,v,p)}},function(t,n,r){var e=r(1);e(e.S,"Number",{EPSILON:Math.pow(2,-52)})},function(t,n,r){var e=r(1),i=r(3).isFinite;e(e.S,"Number",{isFinite:function(t){return"number"==typeof t&&i(t)}})},function(t,n,r){var e=r(1);e(e.S,"Number",{isInteger:r(168)})},function(t,n,r){var e=r(1);e(e.S,"Number",{isNaN:function(t){return t!=t}})},function(t,n,r){var e=r(1),i=r(168),o=Math.abs;e(e.S,"Number",{isSafeInteger:function(t){return i(t)&&o(t)<=9007199254740991}})},function(t,n,r){var e=r(1);e(e.S,"Number",{MAX_SAFE_INTEGER:9007199254740991})},function(t,n,r){var e=r(1);e(e.S,"Number",{MIN_SAFE_INTEGER:-9007199254740991})},function(t,n,r){var e=r(1),i=r(178);e(e.S+e.F*(Number.parseFloat!=i),"Number",{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.S+e.F*(Number.parseInt!=i),"Number",{parseInt:i})},function(t,n,r){"use strict";var e=r(1),i=r(67),o=r(159),u=r(149),c=1..toFixed,f=Math.floor,a=[0,0,0,0,0,0],s="Number.toFixed: incorrect invocation!",l="0",h=function(t,n){for(var r=-1,e=n;++r<6;)e+=t*a[r],a[r]=e%1e7,e=f(e/1e7)},v=function(t){for(var n=6,r=0;--n>=0;)r+=a[n],a[n]=f(r/t),r=r%t*1e7},p=function(){for(var t=6,n="";--t>=0;)if(""!==n||0===t||0!==a[t]){var r=String(a[t]);n=""===n?r:n+u.call(l,7-r.length)+r}return n},d=function(t,n,r){return 0===n?r:n%2==1?d(t,n-1,r*t):d(t*t,n/2,r)},y=function(t){for(var n=0,r=t;r>=4096;)n+=12,r/=4096;for(;r>=2;)n+=1,r/=2;return n};e(e.P+e.F*(!!c&&("0.000"!==8e-5.toFixed(3)||"1"!==.9.toFixed(0)||"1.25"!==1.255.toFixed(2)||"1000000000000000128"!==(0xde0b6b3a7640080).toFixed(0))||!r(4)(function(){c.call({})})),"Number",{toFixed:function(t){var n,r,e,c,f=o(this,s),a=i(t),g="",b=l;if(a<0||a>20)throw RangeError(s);if(f!=f)return"NaN";if(f<=-1e21||f>=1e21)return String(f);if(f<0&&(g="-",f=-f),f>1e-21)if(n=y(f*d(2,69,1))-69,r=n<0?f*d(2,-n,1):f/d(2,n,1),r*=4503599627370496,(n=52-n)>0){for(h(0,r),e=a;e>=7;)h(1e7,0),e-=7;for(h(d(10,e,1),0),e=n-1;e>=23;)v(1<<23),e-=23;v(1<<e),h(1,1),v(2),b=p()}else h(0,r),h(1<<-n,0),b=p()+u.call(l,a);return a>0?(c=b.length,b=g+(c<=a?"0."+u.call(l,a-c)+b:b.slice(0,c-a)+"."+b.slice(c-a))):b=g+b,b}})},function(t,n,r){"use strict";var e=r(1),i=r(4),o=r(159),u=1..toPrecision;e(e.P+e.F*(i(function(){return"1"!==u.call(1,void 0)})||!i(function(){u.call({})})),"Number",{toPrecision:function(t){var n=o(this,"Number#toPrecision: incorrect invocation!");return void 0===t?u.call(n):u.call(n,t)}})},function(t,n,r){var e=r(1);e(e.S+e.F,"Object",{assign:r(172)})},function(t,n,r){var e=r(1);e(e.S,"Object",{create:r(70)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperties:r(173)})},function(t,n,r){var e=r(1);e(e.S+e.F*!r(10),"Object",{defineProperty:r(11).f})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("freeze",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(30),i=r(31).f;r(49)("getOwnPropertyDescriptor",function(){return function(t,n){return i(e(t),n)}})},function(t,n,r){r(49)("getOwnPropertyNames",function(){return r(174).f})},function(t,n,r){var e=r(17),i=r(32);r(49)("getPrototypeOf",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6);r(49)("isExtensible",function(t){return function(n){return!!e(n)&&(!t||t(n))}})},function(t,n,r){var e=r(6);r(49)("isFrozen",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(6);r(49)("isSealed",function(t){return function(n){return!e(n)||!!t&&t(n)}})},function(t,n,r){var e=r(1);e(e.S,"Object",{is:r(180)})},function(t,n,r){var e=r(17),i=r(72);r(49)("keys",function(){return function(t){return i(e(t))}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("preventExtensions",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(6),i=r(65).onFreeze;r(49)("seal",function(t){return function(n){return t&&e(n)?t(i(n)):n}})},function(t,n,r){var e=r(1);e(e.S,"Object",{setPrototypeOf:r(144).set})},function(t,n,r){"use strict";var e=r(114),i={};i[r(7)("toStringTag")]="z",i+""!="[object z]"&&r(28)(Object.prototype,"toString",function(){return"[object "+e(this)+"]"},!0)},function(t,n,r){var e=r(1),i=r(178);e(e.G+e.F*(parseFloat!=i),{parseFloat:i})},function(t,n,r){var e=r(1),i=r(179);e(e.G+e.F*(parseInt!=i),{parseInt:i})},function(t,n,r){"use strict";var e,i,o,u=r(69),c=r(3),f=r(53),a=r(114),s=r(1),l=r(6),h=r(26),v=r(68),p=r(79),d=r(146),y=r(151).set,g=r(143)(),b="Promise",m=c.TypeError,x=c.process,w=c[b],x=c.process,S="process"==a(x),_=function(){},O=!!function(){try{var t=w.resolve(1),n=(t.constructor={})[r(7)("species")]=function(t){t(_,_)};return(S||"function"==typeof PromiseRejectionEvent)&&t.then(_)instanceof n}catch(t){}}(),E=function(t,n){return t===n||t===w&&n===o},P=function(t){var n;return!(!l(t)||"function"!=typeof(n=t.then))&&n},j=function(t){return E(w,t)?new F(t):new i(t)},F=i=function(t){var n,r;this.promise=new t(function(t,e){if(void 0!==n||void 0!==r)throw m("Bad Promise constructor");n=t,r=e}),this.resolve=h(n),this.reject=h(r)},M=function(t){try{t()}catch(t){return{error:t}}},A=function(t,n){if(!t._n){t._n=!0;var r=t._c;g(function(){for(var e=t._v,i=1==t._s,o=0;r.length>o;)!function(n){var r,o,u=i?n.ok:n.fail,c=n.resolve,f=n.reject,a=n.domain;try{u?(i||(2==t._h&&I(t),t._h=1),!0===u?r=e:(a&&a.enter(),r=u(e),a&&a.exit()),r===n.promise?f(m("Promise-chain cycle")):(o=P(r))?o.call(r,c,f):c(r)):f(e)}catch(t){f(t)}}(r[o++]);t._c=[],t._n=!1,n&&!t._h&&N(t)})}},N=function(t){y.call(c,function(){var n,r,e,i=t._v;if(T(t)&&(n=M(function(){S?x.emit("unhandledRejection",i,t):(r=c.onunhandledrejection)?r({promise:t,reason:i}):(e=c.console)&&e.error&&e.error("Unhandled promise rejection",i)}),t._h=S||T(t)?2:1),t._a=void 0,n)throw n.error})},T=function(t){if(1==t._h)return!1;for(var n,r=t._a||t._c,e=0;r.length>e;)if(n=r[e++],n.fail||!T(n.promise))return!1;return!0},I=function(t){y.call(c,function(){var n;S?x.emit("rejectionHandled",t):(n=c.onrejectionhandled)&&n({promise:t,reason:t._v})})},k=function(t){var n=this;n._d||(n._d=!0,n=n._w||n,n._v=t,n._s=2,n._a||(n._a=n._c.slice()),A(n,!0))},L=function(t){var n,r=this;if(!r._d){r._d=!0,r=r._w||r;try{if(r===t)throw m("Promise can't be resolved itself");(n=P(t))?g(function(){var e={_w:r,_d:!1};try{n.call(t,f(L,e,1),f(k,e,1))}catch(t){k.call(e,t)}}):(r._v=t,r._s=1,A(r,!1))}catch(t){k.call({_w:r,_d:!1},t)}}};O||(w=function(t){v(this,w,b,"_h"),h(t),e.call(this);try{t(f(L,this,1),f(k,this,1))}catch(t){k.call(this,t)}},e=function(t){this._c=[],this._a=void 0,this._s=0,this._d=!1,this._v=void 0,this._h=0,this._n=!1},e.prototype=r(73)(w.prototype,{then:function(t,n){var r=j(d(this,w));return r.ok="function"!=typeof t||t,r.fail="function"==typeof n&&n,r.domain=S?x.domain:void 0,this._c.push(r),this._a&&this._a.push(r),this._s&&A(this,!1),r.promise},catch:function(t){return this.then(void 0,t)}}),F=function(){var t=new e;this.promise=t,this.resolve=f(L,t,1),this.reject=f(k,t,1)}),s(s.G+s.W+s.F*!O,{Promise:w}),r(81)(w,b),r(74)(b),o=r(52)[b],s(s.S+s.F*!O,b,{reject:function(t){var n=j(this);return(0,n.reject)(t),n.promise}}),s(s.S+s.F*(u||!O),b,{resolve:function(t){if(t instanceof w&&E(t.constructor,this))return t;var n=j(this);return(0,n.resolve)(t),n.promise}}),s(s.S+s.F*!(O&&r(123)(function(t){w.all(t).catch(_)})),b,{all:function(t){var n=this,r=j(n),e=r.resolve,i=r.reject,o=M(function(){var r=[],o=0,u=1;p(t,!1,function(t){var c=o++,f=!1;r.push(void 0),u++,n.resolve(t).then(function(t){f||(f=!0,r[c]=t,--u||e(r))},i)}),--u||e(r)});return o&&i(o.error),r.promise},race:function(t){var n=this,r=j(n),e=r.reject,i=M(function(){p(t,!1,function(t){n.resolve(t).then(r.resolve,e)})});return i&&e(i.error),r.promise}})},function(t,n,r){var e=r(1),i=r(26),o=r(2),u=(r(3).Reflect||{}).apply,c=Function.apply;e(e.S+e.F*!r(4)(function(){u(function(){})}),"Reflect",{apply:function(t,n,r){var e=i(t),f=o(r);return u?u(e,n,f):c.call(e,n,f)}})},function(t,n,r){var e=r(1),i=r(70),o=r(26),u=r(2),c=r(6),f=r(4),a=r(163),s=(r(3).Reflect||{}).construct,l=f(function(){function t(){}return!(s(function(){},[],t)instanceof t)}),h=!f(function(){s(function(){})});e(e.S+e.F*(l||h),"Reflect",{construct:function(t,n){o(t),u(n);var r=arguments.length<3?t:o(arguments[2]);if(h&&!l)return s(t,n,r);if(t==r){switch(n.length){case 0:return new t;case 1:return new t(n[0]);case 2:return new t(n[0],n[1]);case 3:return new t(n[0],n[1],n[2]);case 4:return new t(n[0],n[1],n[2],n[3])}var e=[null];return e.push.apply(e,n),new(a.apply(t,e))}var f=r.prototype,v=i(c(f)?f:Object.prototype),p=Function.apply.call(t,v,n);return c(p)?p:v}})},function(t,n,r){var e=r(11),i=r(1),o=r(2),u=r(50);i(i.S+i.F*r(4)(function(){Reflect.defineProperty(e.f({},1,{value:1}),1,{value:2})}),"Reflect",{defineProperty:function(t,n,r){o(t),n=u(n,!0),o(r);try{return e.f(t,n,r),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(31).f,o=r(2);e(e.S,"Reflect",{deleteProperty:function(t,n){var r=i(o(t),n);return!(r&&!r.configurable)&&delete t[n]}})},function(t,n,r){"use strict";var e=r(1),i=r(2),o=function(t){this._t=i(t),this._i=0;var n,r=this._k=[];for(n in t)r.push(n)};r(139)(o,"Object",function(){var t,n=this,r=n._k;do{if(n._i>=r.length)return{value:void 0,done:!0}}while(!((t=r[n._i++])in n._t));return{value:t,done:!1}}),e(e.S,"Reflect",{enumerate:function(t){return new o(t)}})},function(t,n,r){var e=r(31),i=r(1),o=r(2);i(i.S,"Reflect",{getOwnPropertyDescriptor:function(t,n){return e.f(o(t),n)}})},function(t,n,r){var e=r(1),i=r(32),o=r(2);e(e.S,"Reflect",{getPrototypeOf:function(t){return i(o(t))}})},function(t,n,r){function e(t,n){var r,c,s=arguments.length<3?t:arguments[2];return a(t)===s?t[n]:(r=i.f(t,n))?u(r,"value")?r.value:void 0!==r.get?r.get.call(s):void 0:f(c=o(t))?e(c,n,s):void 0}var i=r(31),o=r(32),u=r(24),c=r(1),f=r(6),a=r(2);c(c.S,"Reflect",{get:e})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{has:function(t,n){return n in t}})},function(t,n,r){var e=r(1),i=r(2),o=Object.isExtensible;e(e.S,"Reflect",{isExtensible:function(t){return i(t),!o||o(t)}})},function(t,n,r){var e=r(1);e(e.S,"Reflect",{ownKeys:r(177)})},function(t,n,r){var e=r(1),i=r(2),o=Object.preventExtensions;e(e.S,"Reflect",{preventExtensions:function(t){i(t);try{return o&&o(t),!0}catch(t){return!1}}})},function(t,n,r){var e=r(1),i=r(144);i&&e(e.S,"Reflect",{setPrototypeOf:function(t,n){i.check(t,n);try{return i.set(t,n),!0}catch(t){return!1}}})},function(t,n,r){function e(t,n,r){var f,h,v=arguments.length<4?t:arguments[3],p=o.f(s(t),n);if(!p){if(l(h=u(t)))return e(h,n,r,v);p=a(0)}return c(p,"value")?!(!1===p.writable||!l(v)||(f=o.f(v,n)||a(0),f.value=r,i.f(v,n,f),0)):void 0!==p.set&&(p.set.call(v,r),!0)}var i=r(11),o=r(31),u=r(32),c=r(24),f=r(1),a=r(66),s=r(2),l=r(6);f(f.S,"Reflect",{set:e})},function(t,n,r){var e=r(3),i=r(136),o=r(11).f,u=r(71).f,c=r(122),f=r(120),a=e.RegExp,s=a,l=a.prototype,h=/a/g,v=/a/g,p=new a(h)!==h;if(r(10)&&(!p||r(4)(function(){return v[r(7)("match")]=!1,a(h)!=h||a(v)==v||"/a/i"!=a(h,"i")}))){a=function(t,n){var r=this instanceof a,e=c(t),o=void 0===n;return!r&&e&&t.constructor===a&&o?t:i(p?new s(e&&!o?t.source:t,n):s((e=t instanceof a)?t.source:t,e&&o?f.call(t):n),r?this:l,a)};for(var d=u(s),y=0;d.length>y;)!function(t){t in a||o(a,t,{configurable:!0,get:function(){return s[t]},set:function(n){s[t]=n}})}(d[y++]);l.constructor=a,a.prototype=l,r(28)(e,"RegExp",a)}r(74)("RegExp")},function(t,n,r){r(119)("match",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("replace",2,function(t,n,r){return[function(e,i){"use strict";var o=t(this),u=void 0==e?void 0:e[n];return void 0!==u?u.call(e,o,i):r.call(String(o),e,i)},r]})},function(t,n,r){r(119)("search",1,function(t,n,r){return[function(r){"use strict";var e=t(this),i=void 0==r?void 0:r[n];return void 0!==i?i.call(r,e):new RegExp(r)[n](String(e))},r]})},function(t,n,r){r(119)("split",2,function(t,n,e){"use strict";var i=r(122),o=e,u=[].push,c="split",f="length",a="lastIndex";if("c"=="abbc"[c](/(b)*/)[1]||4!="test"[c](/(?:)/,-1)[f]||2!="ab"[c](/(?:ab)*/)[f]||4!="."[c](/(.?)(.?)/)[f]||"."[c](/()()/)[f]>1||""[c](/.?/)[f]){var s=void 0===/()??/.exec("")[1];e=function(t,n){var r=String(this);if(void 0===t&&0===n)return[];if(!i(t))return o.call(r,t,n);var e,c,l,h,v,p=[],d=(t.ignoreCase?"i":"")+(t.multiline?"m":"")+(t.unicode?"u":"")+(t.sticky?"y":""),y=0,g=void 0===n?4294967295:n>>>0,b=new RegExp(t.source,d+"g");for(s||(e=new RegExp("^"+b.source+"$(?!\\s)",d));(c=b.exec(r))&&!((l=c.index+c[0][f])>y&&(p.push(r.slice(y,c.index)),!s&&c[f]>1&&c[0].replace(e,function(){for(v=1;v<arguments[f]-2;v++)void 0===arguments[v]&&(c[v]=void 0)}),c[f]>1&&c.index<r[f]&&u.apply(p,c.slice(1)),h=c[0][f],y=l,p[f]>=g));)b[a]===c.index&&b[a]++;return y===r[f]?!h&&b.test("")||p.push(""):p.push(r.slice(y)),p[f]>g?p.slice(0,g):p}}else"0"[c](void 0,0)[f]&&(e=function(t,n){return void 0===t&&0===n?[]:o.call(this,t,n)});return[function(r,i){var o=t(this),u=void 0==r?void 0:r[n];return void 0!==u?u.call(r,o,i):e.call(String(o),r,i)},e]})},function(t,n,r){"use strict";r(184);var e=r(2),i=r(120),o=r(10),u="toString",c=/./[u],f=function(t){r(28)(RegExp.prototype,u,t,!0)};r(4)(function(){return"/a/b"!=c.call({source:"a",flags:"b"})})?f(function(){var t=e(this);return"/".concat(t.source,"/","flags"in t?t.flags:!o&&t instanceof RegExp?i.call(t):void 0)}):c.name!=u&&f(function(){return c.call(this)})},function(t,n,r){"use strict";r(29)("anchor",function(t){return function(n){return t(this,"a","name",n)}})},function(t,n,r){"use strict";r(29)("big",function(t){return function(){return t(this,"big","","")}})},function(t,n,r){"use strict";r(29)("blink",function(t){return function(){return t(this,"blink","","")}})},function(t,n,r){"use strict";r(29)("bold",function(t){return function(){return t(this,"b","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!1);e(e.P,"String",{codePointAt:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="endsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{endsWith:function(t){var n=o(this,t,u),r=arguments.length>1?arguments[1]:void 0,e=i(n.length),f=void 0===r?e:Math.min(i(r),e),a=String(t);return c?c.call(n,a,f):n.slice(f-a.length,f)===a}})},function(t,n,r){"use strict";r(29)("fixed",function(t){return function(){return t(this,"tt","","")}})},function(t,n,r){"use strict";r(29)("fontcolor",function(t){return function(n){return t(this,"font","color",n)}})},function(t,n,r){"use strict";r(29)("fontsize",function(t){return function(n){return t(this,"font","size",n)}})},function(t,n,r){var e=r(1),i=r(75),o=String.fromCharCode,u=String.fromCodePoint;e(e.S+e.F*(!!u&&1!=u.length),"String",{fromCodePoint:function(t){for(var n,r=[],e=arguments.length,u=0;e>u;){if(n=+arguments[u++],i(n,1114111)!==n)throw RangeError(n+" is not a valid code point");r.push(n<65536?o(n):o(55296+((n-=65536)>>10),n%1024+56320))}return r.join("")}})},function(t,n,r){"use strict";var e=r(1),i=r(148),o="includes";e(e.P+e.F*r(134)(o),"String",{includes:function(t){return!!~i(this,t,o).indexOf(t,arguments.length>1?arguments[1]:void 0)}})},function(t,n,r){"use strict";r(29)("italics",function(t){return function(){return t(this,"i","","")}})},function(t,n,r){"use strict";var e=r(147)(!0);r(140)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,r=this._i;return r>=n.length?{value:void 0,done:!0}:(t=e(n,r),this._i+=t.length,{value:t,done:!1})})},function(t,n,r){"use strict";r(29)("link",function(t){return function(n){return t(this,"a","href",n)}})},function(t,n,r){var e=r(1),i=r(30),o=r(16);e(e.S,"String",{raw:function(t){for(var n=i(t.raw),r=o(n.length),e=arguments.length,u=[],c=0;r>c;)u.push(String(n[c++])),c<e&&u.push(String(arguments[c]));return u.join("")}})},function(t,n,r){var e=r(1);e(e.P,"String",{repeat:r(149)})},function(t,n,r){"use strict";r(29)("small",function(t){return function(){return t(this,"small","","")}})},function(t,n,r){"use strict";var e=r(1),i=r(16),o=r(148),u="startsWith",c=""[u];e(e.P+e.F*r(134)(u),"String",{startsWith:function(t){var n=o(this,t,u),r=i(Math.min(arguments.length>1?arguments[1]:void 0,n.length)),e=String(t);return c?c.call(n,e,r):n.slice(r,r+e.length)===e}})},function(t,n,r){"use strict";r(29)("strike",function(t){return function(){return t(this,"strike","","")}})},function(t,n,r){"use strict";r(29)("sub",function(t){return function(){return t(this,"sub","","")}})},function(t,n,r){"use strict";r(29)("sup",function(t){return function(){return t(this,"sup","","")}})},function(t,n,r){"use strict";r(82)("trim",function(t){return function(){return t(this,3)}})},function(t,n,r){"use strict";var e=r(3),i=r(24),o=r(10),u=r(1),c=r(28),f=r(65).KEY,a=r(4),s=r(126),l=r(81),h=r(76),v=r(7),p=r(182),d=r(153),y=r(206),g=r(205),b=r(138),m=r(2),x=r(30),w=r(50),S=r(66),_=r(70),O=r(174),E=r(31),P=r(11),j=r(72),F=E.f,M=P.f,A=O.f,N=e.Symbol,T=e.JSON,I=T&&T.stringify,k="prototype",L=v("_hidden"),R=v("toPrimitive"),C={}.propertyIsEnumerable,D=s("symbol-registry"),U=s("symbols"),W=s("op-symbols"),G=Object[k],B="function"==typeof N,V=e.QObject,z=!V||!V[k]||!V[k].findChild,q=o&&a(function(){return 7!=_(M({},"a",{get:function(){return M(this,"a",{value:7}).a}})).a})?function(t,n,r){var e=F(G,n);e&&delete G[n],M(t,n,r),e&&t!==G&&M(G,n,e)}:M,K=function(t){var n=U[t]=_(N[k]);return n._k=t,n},J=B&&"symbol"==typeof N.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof N},Y=function(t,n,r){return t===G&&Y(W,n,r),m(t),n=w(n,!0),m(r),i(U,n)?(r.enumerable?(i(t,L)&&t[L][n]&&(t[L][n]=!1),r=_(r,{enumerable:S(0,!1)})):(i(t,L)||M(t,L,S(1,{})),t[L][n]=!0),q(t,n,r)):M(t,n,r)},H=function(t,n){m(t);for(var r,e=g(n=x(n)),i=0,o=e.length;o>i;)Y(t,r=e[i++],n[r]);return t},$=function(t,n){return void 0===n?_(t):H(_(t),n)},X=function(t){var n=C.call(this,t=w(t,!0));return!(this===G&&i(U,t)&&!i(W,t))&&(!(n||!i(this,t)||!i(U,t)||i(this,L)&&this[L][t])||n)},Q=function(t,n){if(t=x(t),n=w(n,!0),t!==G||!i(U,n)||i(W,n)){var r=F(t,n);return!r||!i(U,n)||i(t,L)&&t[L][n]||(r.enumerable=!0),r}},Z=function(t){for(var n,r=A(x(t)),e=[],o=0;r.length>o;)i(U,n=r[o++])||n==L||n==f||e.push(n);return e},tt=function(t){for(var n,r=t===G,e=A(r?W:x(t)),o=[],u=0;e.length>u;)!i(U,n=e[u++])||r&&!i(G,n)||o.push(U[n]);return o};B||(N=function(){if(this instanceof N)throw TypeError("Symbol is not a constructor!");var t=h(arguments.length>0?arguments[0]:void 0),n=function(r){this===G&&n.call(W,r),i(this,L)&&i(this[L],t)&&(this[L][t]=!1),q(this,t,S(1,r))};return o&&z&&q(G,t,{configurable:!0,set:n}),K(t)},c(N[k],"toString",function(){return this._k}),E.f=Q,P.f=Y,r(71).f=O.f=Z,r(116).f=X,r(125).f=tt,o&&!r(69)&&c(G,"propertyIsEnumerable",X,!0),p.f=function(t){return K(v(t))}),u(u.G+u.W+u.F*!B,{Symbol:N});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),rt=0;nt.length>rt;)v(nt[rt++]);for(var nt=j(v.store),rt=0;nt.length>rt;)d(nt[rt++]);u(u.S+u.F*!B,"Symbol",{for:function(t){return i(D,t+="")?D[t]:D[t]=N(t)},keyFor:function(t){if(J(t))return y(D,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){z=!0},useSimple:function(){z=!1}}),u(u.S+u.F*!B,"Object",{create:$,defineProperty:Y,defineProperties:H,getOwnPropertyDescriptor:Q,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),T&&u(u.S+u.F*(!B||a(function(){var t=N();return"[null]"!=I([t])||"{}"!=I({a:t})||"{}"!=I(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!J(t)){for(var n,r,e=[t],i=1;arguments.length>i;)e.push(arguments[i++]);return n=e[1],"function"==typeof n&&(r=n),!r&&b(n)||(n=function(t,n){if(r&&(n=r.call(this,t,n)),!J(n))return n}),e[1]=n,I.apply(T,e)}}}),N[k][R]||r(27)(N[k],R,N[k].valueOf),l(N,"Symbol"),l(Math,"Math",!0),l(e.JSON,"JSON",!0)},function(t,n,r){"use strict";var e=r(1),i=r(127),o=r(152),u=r(2),c=r(75),f=r(16),a=r(6),s=r(3).ArrayBuffer,l=r(146),h=o.ArrayBuffer,v=o.DataView,p=i.ABV&&s.isView,d=h.prototype.slice,y=i.VIEW,g="ArrayBuffer";e(e.G+e.W+e.F*(s!==h),{ArrayBuffer:h}),e(e.S+e.F*!i.CONSTR,g,{isView:function(t){return p&&p(t)||a(t)&&y in t}}),e(e.P+e.U+e.F*r(4)(function(){return!new h(2).slice(1,void 0).byteLength}),g,{slice:function(t,n){if(void 0!==d&&void 0===n)return d.call(u(this),t);for(var r=u(this).byteLength,e=c(t,r),i=c(void 0===n?r:n,r),o=new(l(this,h))(f(i-e)),a=new v(this),s=new v(o),p=0;e<i;)s.setUint8(p++,a.getUint8(e++));return o}}),r(74)(g)},function(t,n,r){var e=r(1);e(e.G+e.W+e.F*!r(127).ABV,{DataView:r(152).DataView})},function(t,n,r){r(55)("Float32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Float64",8,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Int8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint16",2,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint32",4,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}})},function(t,n,r){r(55)("Uint8",1,function(t){return function(n,r,e){return t(this,n,r,e)}},!0)},function(t,n,r){"use strict";var e=r(166);r(118)("WeakSet",function(t){return function(){return t(this,arguments.length>0?arguments[0]:void 0)}},{add:function(t){return e.def(this,t,!0)}},e,!1,!0)},function(t,n,r){"use strict";var e=r(1),i=r(117)(!0);e(e.P,"Array",{includes:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0)}}),r(78)("includes")},function(t,n,r){var e=r(1),i=r(143)(),o=r(3).process,u="process"==r(45)(o);e(e.G,{asap:function(t){var n=u&&o.domain;i(n?n.bind(t):t)}})},function(t,n,r){var e=r(1),i=r(45);e(e.S,"Error",{isError:function(t){return"Error"===i(t)}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Map",{toJSON:r(165)("Map")})},function(t,n,r){var e=r(1);e(e.S,"Math",{iaddh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o+(e>>>0)+((i&u|(i|u)&~(i+u>>>0))>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{imulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>16,f=i>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>16)+((o*f>>>0)+(a&r)>>16)}})},function(t,n,r){var e=r(1);e(e.S,"Math",{isubh:function(t,n,r,e){var i=t>>>0,o=n>>>0,u=r>>>0;return o-(e>>>0)-((~i&u|~(i^u)&i-u>>>0)>>>31)|0}})},function(t,n,r){var e=r(1);e(e.S,"Math",{umulh:function(t,n){var r=65535,e=+t,i=+n,o=e&r,u=i&r,c=e>>>16,f=i>>>16,a=(c*u>>>0)+(o*u>>>16);return c*f+(a>>>16)+((o*f>>>0)+(a&r)>>>16)}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineGetter__:function(t,n){u.f(i(this),t,{get:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(26),u=r(11);r(10)&&e(e.P+r(124),"Object",{__defineSetter__:function(t,n){u.f(i(this),t,{set:o(n),enumerable:!0,configurable:!0})}})},function(t,n,r){var e=r(1),i=r(176)(!0);e(e.S,"Object",{entries:function(t){return i(t)}})},function(t,n,r){var e=r(1),i=r(177),o=r(30),u=r(31),c=r(131);e(e.S,"Object",{getOwnPropertyDescriptors:function(t){for(var n,r=o(t),e=u.f,f=i(r),a={},s=0;f.length>s;)c(a,n=f[s++],e(r,n));return a}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupGetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.get}while(r=u(r))}})},function(t,n,r){"use strict";var e=r(1),i=r(17),o=r(50),u=r(32),c=r(31).f;r(10)&&e(e.P+r(124),"Object",{__lookupSetter__:function(t){var n,r=i(this),e=o(t,!0);do{if(n=c(r,e))return n.set}while(r=u(r))}})},function(t,n,r){var e=r(1),i=r(176)(!1);e(e.S,"Object",{values:function(t){return i(t)}})},function(t,n,r){"use strict";var e=r(1),i=r(3),o=r(52),u=r(143)(),c=r(7)("observable"),f=r(26),a=r(2),s=r(68),l=r(73),h=r(27),v=r(79),p=v.RETURN,d=function(t){return null==t?void 0:f(t)},y=function(t){var n=t._c;n&&(t._c=void 0,n())},g=function(t){return void 0===t._o},b=function(t){g(t)||(t._o=void 0,y(t))},m=function(t,n){a(t),this._c=void 0,this._o=t,t=new x(this);try{var r=n(t),e=r;null!=r&&("function"==typeof r.unsubscribe?r=function(){e.unsubscribe()}:f(r),this._c=r)}catch(n){return void t.error(n)}g(this)&&y(this)};m.prototype=l({},{unsubscribe:function(){b(this)}});var x=function(t){this._s=t};x.prototype=l({},{next:function(t){var n=this._s;if(!g(n)){var r=n._o;try{var e=d(r.next);if(e)return e.call(r,t)}catch(t){try{b(n)}finally{throw t}}}},error:function(t){var n=this._s;if(g(n))throw t;var r=n._o;n._o=void 0;try{var e=d(r.error);if(!e)throw t;t=e.call(r,t)}catch(t){try{y(n)}finally{throw t}}return y(n),t},complete:function(t){var n=this._s;if(!g(n)){var r=n._o;n._o=void 0;try{var e=d(r.complete);t=e?e.call(r,t):void 0}catch(t){try{y(n)}finally{throw t}}return y(n),t}}});var w=function(t){s(this,w,"Observable","_f")._f=f(t)};l(w.prototype,{subscribe:function(t){return new m(t,this._f)},forEach:function(t){var n=this;return new(o.Promise||i.Promise)(function(r,e){f(t);var i=n.subscribe({next:function(n){try{return t(n)}catch(t){e(t),i.unsubscribe()}},error:e,complete:r})})}}),l(w,{from:function(t){var n="function"==typeof this?this:w,r=d(a(t)[c]);if(r){var e=a(r.call(t));return e.constructor===n?e:new n(function(t){return e.subscribe(t)})}return new n(function(n){var r=!1;return u(function(){if(!r){try{if(v(t,!1,function(t){if(n.next(t),r)return p})===p)return}catch(t){if(r)throw t;return void n.error(t)}n.complete()}}),function(){r=!0}})},of:function(){for(var t=0,n=arguments.length,r=Array(n);t<n;)r[t]=arguments[t++];return new("function"==typeof this?this:w)(function(t){var n=!1;return u(function(){if(!n){for(var e=0;e<r.length;++e)if(t.next(r[e]),n)return;t.complete()}}),function(){n=!0}})}}),h(w.prototype,c,function(){return this}),e(e.G,{Observable:w}),r(74)("Observable")},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.set;e.exp({defineMetadata:function(t,n,r,e){u(t,n,i(r),o(e))}})},function(t,n,r){var e=r(54),i=r(2),o=e.key,u=e.map,c=e.store;e.exp({deleteMetadata:function(t,n){var r=arguments.length<3?void 0:o(arguments[2]),e=u(i(n),r,!1);if(void 0===e||!e.delete(t))return!1;if(e.size)return!0;var f=c.get(n);return f.delete(r),!!f.size||c.delete(n)}})},function(t,n,r){var e=r(185),i=r(161),o=r(54),u=r(2),c=r(32),f=o.keys,a=o.key,s=function(t,n){var r=f(t,n),o=c(t);if(null===o)return r;var u=s(o,n);return u.length?r.length?i(new e(r.concat(u))):u:r};o.exp({getMetadataKeys:function(t){return s(u(t),arguments.length<2?void 0:a(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.get,f=e.key,a=function(t,n,r){if(u(t,n,r))return c(t,n,r);var e=o(n);return null!==e?a(t,e,r):void 0};e.exp({getMetadata:function(t,n){return a(t,i(n),arguments.length<3?void 0:f(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.keys,u=e.key;e.exp({getOwnMetadataKeys:function(t){
return o(i(t),arguments.length<2?void 0:u(arguments[1]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.get,u=e.key;e.exp({getOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(32),u=e.has,c=e.key,f=function(t,n,r){if(u(t,n,r))return!0;var e=o(n);return null!==e&&f(t,e,r)};e.exp({hasMetadata:function(t,n){return f(t,i(n),arguments.length<3?void 0:c(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=e.has,u=e.key;e.exp({hasOwnMetadata:function(t,n){return o(t,i(n),arguments.length<3?void 0:u(arguments[2]))}})},function(t,n,r){var e=r(54),i=r(2),o=r(26),u=e.key,c=e.set;e.exp({metadata:function(t,n){return function(r,e){c(t,n,(void 0!==e?i:o)(r),u(e))}}})},function(t,n,r){var e=r(1);e(e.P+e.R,"Set",{toJSON:r(165)("Set")})},function(t,n,r){"use strict";var e=r(1),i=r(147)(!0);e(e.P,"String",{at:function(t){return i(this,t)}})},function(t,n,r){"use strict";var e=r(1),i=r(46),o=r(16),u=r(122),c=r(120),f=RegExp.prototype,a=function(t,n){this._r=t,this._s=n};r(139)(a,"RegExp String",function(){var t=this._r.exec(this._s);return{value:t,done:null===t}}),e(e.P,"String",{matchAll:function(t){if(i(this),!u(t))throw TypeError(t+" is not a regexp!");var n=String(this),r="flags"in f?String(t.flags):c.call(t),e=new RegExp(t.source,~r.indexOf("g")?r:"g"+r);return e.lastIndex=o(t.lastIndex),new a(e,n)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padEnd:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!1)}})},function(t,n,r){"use strict";var e=r(1),i=r(181);e(e.P,"String",{padStart:function(t){return i(this,t,arguments.length>1?arguments[1]:void 0,!0)}})},function(t,n,r){"use strict";r(82)("trimLeft",function(t){return function(){return t(this,1)}},"trimStart")},function(t,n,r){"use strict";r(82)("trimRight",function(t){return function(){return t(this,2)}},"trimEnd")},function(t,n,r){r(153)("asyncIterator")},function(t,n,r){r(153)("observable")},function(t,n,r){var e=r(1);e(e.S,"System",{global:r(3)})},function(t,n,r){for(var e=r(155),i=r(28),o=r(3),u=r(27),c=r(80),f=r(7),a=f("iterator"),s=f("toStringTag"),l=c.Array,h=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],v=0;v<5;v++){var p,d=h[v],y=o[d],g=y&&y.prototype;if(g){g[a]||u(g,a,l),g[s]||u(g,s,d),c[d]=l;for(p in e)g[p]||i(g,p,e[p],!0)}}},function(t,n,r){var e=r(1),i=r(151);e(e.G+e.B,{setImmediate:i.set,clearImmediate:i.clear})},function(t,n,r){var e=r(3),i=r(1),o=r(121),u=r(207),c=e.navigator,f=!!c&&/MSIE .\./.test(c.userAgent),a=function(t){return f?function(n,r){return t(o(u,[].slice.call(arguments,2),"function"==typeof n?n:Function(n)),r)}:t};i(i.G+i.B+i.F*f,{setTimeout:a(e.setTimeout),setInterval:a(e.setInterval)})},function(t,n,r){r(330),r(269),r(271),r(270),r(273),r(275),r(280),r(274),r(272),r(282),r(281),r(277),r(278),r(276),r(268),r(279),r(283),r(284),r(236),r(238),r(237),r(286),r(285),r(256),r(266),r(267),r(257),r(258),r(259),r(260),r(261),r(262),r(263),r(264),r(265),r(239),r(240),r(241),r(242),r(243),r(244),r(245),r(246),r(247),r(248),r(249),r(250),r(251),r(252),r(253),r(254),r(255),r(317),r(322),r(329),r(320),r(312),r(313),r(318),r(323),r(325),r(308),r(309),r(310),r(311),r(314),r(315),r(316),r(319),r(321),r(324),r(326),r(327),r(328),r(231),r(233),r(232),r(235),r(234),r(220),r(218),r(224),r(221),r(227),r(229),r(217),r(223),r(214),r(228),r(212),r(226),r(225),r(219),r(222),r(211),r(213),r(216),r(215),r(230),r(155),r(302),r(307),r(184),r(303),r(304),r(305),r(306),r(287),r(183),r(185),r(186),r(342),r(331),r(332),r(337),r(340),r(341),r(335),r(338),r(336),r(339),r(333),r(334),r(288),r(289),r(290),r(291),r(292),r(295),r(293),r(294),r(296),r(297),r(298),r(299),r(301),r(300),r(343),r(369),r(372),r(371),r(373),r(374),r(370),r(375),r(376),r(354),r(357),r(353),r(351),r(352),r(355),r(356),r(346),r(368),r(377),r(345),r(347),r(349),r(348),r(350),r(359),r(360),r(362),r(361),r(364),r(363),r(365),r(366),r(367),r(344),r(358),r(380),r(379),r(378),t.exports=r(52)},function(t,n){function r(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var r=t.nextSibling;return r?t.parentNode.insertBefore(n,r):t.parentNode.appendChild(n)}t.exports=r},,,,,,,,,function(t,n,r){(function(n,r){!function(n){"use strict";function e(t,n,r,e){var i=n&&n.prototype instanceof o?n:o,u=Object.create(i.prototype),c=new p(e||[]);return u._invoke=s(t,r,c),u}function i(t,n,r){try{return{type:"normal",arg:t.call(n,r)}}catch(t){return{type:"throw",arg:t}}}function o(){}function u(){}function c(){}function f(t){["next","throw","return"].forEach(function(n){t[n]=function(t){return this._invoke(n,t)}})}function a(t){function n(r,e,o,u){var c=i(t[r],t,e);if("throw"!==c.type){var f=c.arg,a=f.value;return a&&"object"==typeof a&&m.call(a,"__await")?Promise.resolve(a.__await).then(function(t){n("next",t,o,u)},function(t){n("throw",t,o,u)}):Promise.resolve(a).then(function(t){f.value=t,o(f)},u)}u(c.arg)}function e(t,r){function e(){return new Promise(function(e,i){n(t,r,e,i)})}return o=o?o.then(e,e):e()}"object"==typeof r&&r.domain&&(n=r.domain.bind(n));var o;this._invoke=e}function s(t,n,r){var e=P;return function(o,u){if(e===F)throw new Error("Generator is already running");if(e===M){if("throw"===o)throw u;return y()}for(r.method=o,r.arg=u;;){var c=r.delegate;if(c){var f=l(c,r);if(f){if(f===A)continue;return f}}if("next"===r.method)r.sent=r._sent=r.arg;else if("throw"===r.method){if(e===P)throw e=M,r.arg;r.dispatchException(r.arg)}else"return"===r.method&&r.abrupt("return",r.arg);e=F;var a=i(t,n,r);if("normal"===a.type){if(e=r.done?M:j,a.arg===A)continue;return{value:a.arg,done:r.done}}"throw"===a.type&&(e=M,r.method="throw",r.arg=a.arg)}}}function l(t,n){var r=t.iterator[n.method];if(r===g){if(n.delegate=null,"throw"===n.method){if(t.iterator.return&&(n.method="return",n.arg=g,l(t,n),"throw"===n.method))return A;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return A}var e=i(r,t.iterator,n.arg);if("throw"===e.type)return n.method="throw",n.arg=e.arg,n.delegate=null,A;var o=e.arg;return o?o.done?(n[t.resultName]=o.value,n.next=t.nextLoc,"return"!==n.method&&(n.method="next",n.arg=g),n.delegate=null,A):o:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,A)}function h(t){var n={tryLoc:t[0]};1 in t&&(n.catchLoc=t[1]),2 in t&&(n.finallyLoc=t[2],n.afterLoc=t[3]),this.tryEntries.push(n)}function v(t){var n=t.completion||{};n.type="normal",delete n.arg,t.completion=n}function p(t){this.tryEntries=[{tryLoc:"root"}],t.forEach(h,this),this.reset(!0)}function d(t){if(t){var n=t[w];if(n)return n.call(t);if("function"==typeof t.next)return t;if(!isNaN(t.length)){var r=-1,e=function n(){for(;++r<t.length;)if(m.call(t,r))return n.value=t[r],n.done=!1,n;return n.value=g,n.done=!0,n};return e.next=e}}return{next:y}}function y(){return{value:g,done:!0}}var g,b=Object.prototype,m=b.hasOwnProperty,x="function"==typeof Symbol?Symbol:{},w=x.iterator||"@@iterator",S=x.asyncIterator||"@@asyncIterator",_=x.toStringTag||"@@toStringTag",O="object"==typeof t,E=n.regeneratorRuntime;if(E)return void(O&&(t.exports=E));E=n.regeneratorRuntime=O?t.exports:{},E.wrap=e;var P="suspendedStart",j="suspendedYield",F="executing",M="completed",A={},N={};N[w]=function(){return this};var T=Object.getPrototypeOf,I=T&&T(T(d([])));I&&I!==b&&m.call(I,w)&&(N=I);var k=c.prototype=o.prototype=Object.create(N);u.prototype=k.constructor=c,c.constructor=u,c[_]=u.displayName="GeneratorFunction",E.isGeneratorFunction=function(t){var n="function"==typeof t&&t.constructor;return!!n&&(n===u||"GeneratorFunction"===(n.displayName||n.name))},E.mark=function(t){return Object.setPrototypeOf?Object.setPrototypeOf(t,c):(t.__proto__=c,_ in t||(t[_]="GeneratorFunction")),t.prototype=Object.create(k),t},E.awrap=function(t){return{__await:t}},f(a.prototype),a.prototype[S]=function(){return this},E.AsyncIterator=a,E.async=function(t,n,r,i){var o=new a(e(t,n,r,i));return E.isGeneratorFunction(n)?o:o.next().then(function(t){return t.done?t.value:o.next()})},f(k),k[_]="Generator",k.toString=function(){return"[object Generator]"},E.keys=function(t){var n=[];for(var r in t)n.push(r);return n.reverse(),function r(){for(;n.length;){var e=n.pop();if(e in t)return r.value=e,r.done=!1,r}return r.done=!0,r}},E.values=d,p.prototype={constructor:p,reset:function(t){if(this.prev=0,this.next=0,this.sent=this._sent=g,this.done=!1,this.delegate=null,this.method="next",this.arg=g,this.tryEntries.forEach(v),!t)for(var n in this)"t"===n.charAt(0)&&m.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=g)},stop:function(){this.done=!0;var t=this.tryEntries[0],n=t.completion;if("throw"===n.type)throw n.arg;return this.rval},dispatchException:function(t){function n(n,e){return o.type="throw",o.arg=t,r.next=n,e&&(r.method="next",r.arg=g),!!e}if(this.done)throw t;for(var r=this,e=this.tryEntries.length-1;e>=0;--e){var i=this.tryEntries[e],o=i.completion;if("root"===i.tryLoc)return n("end");if(i.tryLoc<=this.prev){var u=m.call(i,"catchLoc"),c=m.call(i,"finallyLoc");if(u&&c){if(this.prev<i.catchLoc)return n(i.catchLoc,!0);if(this.prev<i.finallyLoc)return n(i.finallyLoc)}else if(u){if(this.prev<i.catchLoc)return n(i.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<i.finallyLoc)return n(i.finallyLoc)}}}},abrupt:function(t,n){for(var r=this.tryEntries.length-1;r>=0;--r){var e=this.tryEntries[r];if(e.tryLoc<=this.prev&&m.call(e,"finallyLoc")&&this.prev<e.finallyLoc){var i=e;break}}i&&("break"===t||"continue"===t)&&i.tryLoc<=n&&n<=i.finallyLoc&&(i=null);var o=i?i.completion:{};return o.type=t,o.arg=n,i?(this.method="next",this.next=i.finallyLoc,A):this.complete(o)},complete:function(t,n){if("throw"===t.type)throw t.arg;return"break"===t.type||"continue"===t.type?this.next=t.arg:"return"===t.type?(this.rval=this.arg=t.arg,this.method="return",this.next="end"):"normal"===t.type&&n&&(this.next=n),A},finish:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.finallyLoc===t)return this.complete(r.completion,r.afterLoc),v(r),A}},catch:function(t){for(var n=this.tryEntries.length-1;n>=0;--n){var r=this.tryEntries[n];if(r.tryLoc===t){var e=r.completion;if("throw"===e.type){var i=e.arg;v(r)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(t,n,r){return this.delegate={iterator:d(t),resultName:n,nextLoc:r},"next"===this.method&&(this.arg=g),A}}}("object"==typeof n?n:"object"==typeof window?window:"object"==typeof self?self:this)}).call(n,function(){return this}(),r(158))}])</script><script src="/elasticsearch_learning/./main.0cf68a.js"></script><script>!function(){!function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)}("/elasticsearch_learning/slider.e37972.js")}()</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">_cat/nodes接口</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">ES, Semgent Merge</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Flink、Slot分配、SubTask申请slot, SubTask部署</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Linux, close</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">NIO, write, read</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">NIO</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">LockSupport</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Lucene、ByteBlockPool</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Lucene、词典、tim、tip、doc、pos、fst、倒排索引</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Lucene、词典、FST</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">Lucene、StoredField</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">Lucene、BKW树、Point</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">Lucene、DocValue</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">PoolArena</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Cycler</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">NioEventLoop</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">ReentrantLock</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">netty4, ServerBootstrap, Initiale</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">随笔</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">gdb</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">git, rebase, cherry-pick, reset, checkout</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">perftools、jcmd、pmap</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">maven, 插件, 打包</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、请确保node版本大于6.2<br/>2、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            3、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: false
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接1</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">往事随风&lt;br&gt;&lt;br&gt;当才华满足不了你的梦想的时候，你就应该好好学习</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>